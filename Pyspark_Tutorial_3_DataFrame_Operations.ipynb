{"cells":[{"cell_type":"markdown","source":["##### DATAFRAME\n* DISPLAY() And SHOW() we will use for DataFrame and it will  display in table formation. if we use Collect() it will display row format\n* We will use Spark Session or SQLContext for DF Creation\n* Datafram will contain two Dimension Structured Data( Like Columns & Rows)\n##### RDD\n* COLLECT() we will in RRD to get the resultset. DISPLAY() and SHOW() actions are not avialble in RDD.\n* We will use SparkContext for RDD Creation\n* RDD can store RAW Format Data like individual items..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c211ee1f-6851-4de9-8100-92d41b6f9255"}}},{"cell_type":"markdown","source":["### What is Pyspark DataFrame?\n* PySpark DataFrames are tables that consist of rows and columns of data. It has a two-dimensional structure wherein every column consists of values of a particular variable while each row consists of a single set of values from each column"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27a1de5c-39f4-4d80-bef7-9a5d3099c079"}}},{"cell_type":"code","source":["#Creating Pyspark DataFram using Spark Session...\n#my_schema=StructType(List(StructField(name,StringType,true),StructField(age,LongType,true)))\nmy_df = spark.createDataFrame([('Reshwanth',9),('Vikranth',5)],['name','age'])\ntype(my_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f66012b5-37c1-4a2d-9739-67e6453fefb0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#collect(), show(), display  we will use mainly for development time and unit testing not in production code"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34506aef-7db8-4306-892f-c069ecdfc2bb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(my_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf1c302f-2dc8-4f00-b7e1-b777c0bb2c4f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["emp_sql_df = spark.sql('select * from sample_db.emp')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54984eb5-cdee-4d78-80db-78f438eca2ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(emp_sql_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c0e8388-f259-45e1-a2e8-bbcb3b4252d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Creating Dataframe using SQLContext..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fccd8171-d3b1-47fa-926c-677af2f7b526"}}},{"cell_type":"code","source":["sc-sparkcontext\nspark-sparksession\nsqlContext-SQLContext"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e488d062-abe7-4a07-b64a-cde09cc4ab6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#import pyspark.sql.functions as f\n\nsql_df = sqlContext.createDataFrame([('Reshwanth',9,'Bangalore'),('Vikranth',5,'Bangalore')],['name','age','Locaiton'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce0a38b4-3182-4051-9632-3743d2c9de0f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Metadata Comparision functions...\n* df.schema\n* df.printSchmea()\n* df.columns\n* df.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfa5e6b8-9155-4e4f-956a-919aadba104f"}}},{"cell_type":"code","source":["# \nprint(sql_df.schema) # schema comparision we will use this function to get completed dataframe schema....\nprint(sql_df.printSchema())\nprint(sql_df.columns)\nprint(sql_df.dtypes)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2bb5ced-02c0-4226-93e1-b94c5ad831ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark alias function\n* Returns a new DataFrame with an alias set.\n\n###### Parameters\n* alias – string, an alias name to be set for the DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9fda732-1273-4b2a-b434-452908d2b514"}}},{"cell_type":"code","source":["import pyspark.sql.functions as f\nx_df = sqlContext.createDataFrame([(\"Prasad\",\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\nx_df.select(f.col(\"x_df.from\"),f.col(\"x_df.to\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fda63a8-35fd-425b-af49-d50e815b2ed4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# alias\nimport pyspark.sql.functions as f\nx = sqlContext.createDataFrame([(\"Prasad\",\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.alias('mydf')\n#x.show()\n#y.show()\n#y.select(\"mydf.to\",\"mydf.amt\").show()\nprint(type(x))\nprint(type(y))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b1ccecf-ee47-47c3-a76d-fab5c6f0510a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from sales as s "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"062ede1a-b15b-4cbc-8d91-15a07b3e4268"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark collect function \n##### Returns all the records as a list of Row."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35d039f0-0505-4c31-b131-c5cc8ebbdce1"}}},{"cell_type":"code","source":["# collect\nx = sqlContext.createDataFrame([(\"Prasad\",\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.collect()\nx.show()\nprint(y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78a04604-1c08-4c20-a29e-c5088004571d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark columns function \n##### Returns all column names as a list."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a86d81f0-dd78-4428-adaa-faa8786fa96c"}}},{"cell_type":"code","source":["# columns\nx = sqlContext.createDataFrame([(\"Prasad\",\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.columns \nx.show()\nprint('Available Columns in X DF: ',y)\nprint(type(y))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2329745c-3bea-4c1c-a80a-4c5302a27116"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["x.schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14c1a585-8db1-4b6a-88bf-a90c9614b576"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark count function\n##### Returns the number of rows in this DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa5c8f73-42f0-444f-812c-c5961cfde9c2"}}},{"cell_type":"code","source":["# count\nx = sqlContext.createDataFrame([(\"Prasad\",\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\nx.show()\nprint(x.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96cbbcda-61ee-4f55-ac2d-33ea8e3c4d33"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark crosstab function\n* Computes a pair-wise frequency table of the given columns. Also known as a contingency table. The number of distinct values for each column should be less than 1e4. At most 1e6 non-zero pair frequencies will be returned. The first column of each row will be the distinct values of col1 and the column names will be the distinct values of col2. The name of the first column will be $col1_$col2. Pairs that have no occurrences will have zero as their counts. DataFrame.crosstab() and DataFrameStatFunctions.crosstab() are aliases."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84b2cd54-568c-4dfb-a268-895d49882710"}}},{"cell_type":"code","source":["# crosstab\nx = sqlContext.createDataFrame([(\"Prasad\",\"Raj\",0.1),(\"Ravi\",\"Raj\",0.5),(\"Raj\",\"Sridhar\",0.2),(\"Prasad\",\"Raj\",0.6),(\"Raj\",\"Prasad\",0.2),(\"Sridhar\",\"Ravi\",0.3),(\"Sridhar\",\"Ravi\",0.4)], ['from','to','amt'])\ny = x.crosstab(col1='from',col2='to')\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acd9b661-65da-400e-9129-66a17d1e992d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark describe function\n* Computes basic statistics for numeric and string columns.\n\n* This include count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7131b3cc-d949-4f86-9c40-00a63060a98c"}}},{"cell_type":"code","source":["# describe\nx = sqlContext.createDataFrame([(\"Prasad\",\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\nx.show()\nx.describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a5fba41-c62d-4243-b79b-0206ffe4fca3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark distinct function\n* Returns a new DataFrame containing the distinct rows in this DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b888e314-2915-437c-b1ad-9765174ab5f1"}}},{"cell_type":"code","source":["# distinct\nx = sqlContext.createDataFrame([(\"Prasad\",\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3),(\"Raj\",\"Sridhar\",0.2)], ['from','to','amt'])\ny = x.distinct()\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f697d46-ebd8-4a94-9f36-1abdb84599f4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark drop function\n* Returns a new DataFrame that drops the specified column. This is a no-op if schema doesn’t contain the given column name(s).\n\n###### Parameters\n* cols – a string name of the column to drop, or a Column to drop, or a list of string name of the columns to drop."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58a3de26-e446-4039-a7f8-5b1777dfe846"}}},{"cell_type":"code","source":["# drop\nx = sqlContext.createDataFrame([(\"Prasad\",\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.drop('from')\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"576c81cc-d21a-4378-adc2-0b07ece06136"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark dropDuplicates function\n##### Return a new DataFrame with duplicate rows removed, optionally only considering certain columns.\n\n* For a static batch DataFrame, it just drops duplicate rows. For a streaming DataFrame, it will keep all data across triggers as intermediate state to drop duplicates rows. You can use withWatermark() to limit how late the duplicate data can be and system will accordingly limit the state. In addition, too late data older than watermark will be dropped to avoid any possibility of duplicates.\n\n* `drop_duplicates()` is an alias for `dropDuplicates()`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e1c8765-47e3-4e90-903a-2e1a980d2616"}}},{"cell_type":"code","source":["# dropDuplicates\nx = sqlContext.createDataFrame([(\"Prasad\",\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Raj\",\"Sridhar\",0.3),(\"Raj\",\"Sridhar\",0.2)], ['from','to','amt'])\ny = x.dropDuplicates([\"from\",\"to\"])\nx.show()\n\ny.show()\nx.drop_duplicates().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af805ac5-4530-462e-a213-a1dc721257e2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["\nx.drop_duplicates().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c6d0115-36a8-48c2-b342-c7d892b2940f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark dropna function\n##### Returns a new DataFrame omitting rows with null values. DataFrame.dropna() and DataFrameNaFunctions.drop() are aliases of each other.\n\n##### Parameters\n* how – ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n\n* thresh – int, default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n\n* subset – optional list of column names to consider."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"466fd90b-0cf9-4277-a6c0-d92730b4f7f3"}}},{"cell_type":"code","source":["# dropna\nx = sqlContext.createDataFrame([(None,\"Raj\",0.1),(\"Raj\",\"Sridhar\",None),(\"Sridhar\",None,0.3),(\"Raj\",\"Sridhar\",0.2),(None,None,None)], ['from','to','amt'])\ny = x.dropna(how='all',subset=['from','to'])\nx.show()\ny.show()\ny.dropna().show()\ny.subtract(x)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f588bbc9-9331-499a-b3b9-43669f8192c0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#to get metadata we can use below list of functions\nprint('Get Columns : ',x.columns)\nprint('get schema : ',x.schema)\nprint('get printschema :',x.printSchema())\nprint('get datatypes : ',x.dtypes)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"667f46cf-ddd4-4310-8bcc-85686adb76bb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark dtypes function\n##### Returns all column names and their data types as a list."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ed71ce3-5e98-4fa8-8cb1-e90024d12657"}}},{"cell_type":"code","source":["# dtypes\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.dtypes\nx.show()\nprint(y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b3b3857-1cef-4975-965d-7b9ccf8ccdf6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark fillna function\n##### Replace null values, alias for na.fill(). DataFrame.fillna() and DataFrameNaFunctions.fill() are aliases of each other.\n\n##### Parameters\n* value – int, long, float, string, bool or dict. Value to replace null values with. If the value is a dict, then subset is ignored and value must be a mapping from column name (string) to replacement value. The replacement value must be an int, long, float, boolean, or string.\n\n* subset – optional list of column names to consider. Columns specified in subset that do not have matching data type are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e77733d2-f9dc-4e59-a3da-ed8185f63dba"}}},{"cell_type":"code","source":["emp_df = spark.sql('select * from sample_db.emp')\n## in sql we have NVL Function nvl(col,0) , nvl(col,'n/a')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02435222-ef0c-42c0-b7f1-f87bdfda2564"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["x = sqlContext.createDataFrame([(None,\"Raj\",0.1),(\"Raj\",\"Sridhar\",None),(\"Sridhar\",None,0.3)], ['from','to','amt'])\nprint(x.show())\nprint(x.fillna(0).fillna('N/A').show())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7aa8b665-7e43-4ce9-b698-d89451d437a0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# fillna\nx = sqlContext.createDataFrame([(None,\"Raj\",0.1),(\"Raj\",\"Sridhar\",None),(\"Sridhar\",None,0.3)], ['from','to','amt'])\ny = x.fillna(value='unknown',subset=['from','to'])\nx.show()\nx.fillna(0).fillna('No-Value').show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e277a537-939b-4b3e-b4e0-1638bd3b26b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark filter function\n##### Filters rows using the given condition.\n\n* where() is an alias for filter().\n\n###### Parameters\n* condition – a Column of types.BooleanType or a string of SQL expression."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aeaf22c7-7c5b-46a0-8437-4750248cd878"}}},{"cell_type":"code","source":["%sql\nselect * from sample_db.emp where deptno in(20,10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"482fbd65-5f0c-4c7c-a918-3a1284fcd62e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# filter\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.filter(\"amt =0.1\")\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73b4d685-0b15-49e8-8f49-160c61ec0652"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+---+\n|   from|     to|amt|\n+-------+-------+---+\n| Prasad|    Raj|0.1|\n|    Raj|Sridhar|0.2|\n|Sridhar|   Ravi|0.3|\n+-------+-------+---+\n\n+------+---+---+\n|  from| to|amt|\n+------+---+---+\n|Prasad|Raj|0.1|\n+------+---+---+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+---+\n   from|     to|amt|\n+-------+-------+---+\n Prasad|    Raj|0.1|\n    Raj|Sridhar|0.2|\nSridhar|   Ravi|0.3|\n+-------+-------+---+\n\n+------+---+---+\n  from| to|amt|\n+------+---+---+\nPrasad|Raj|0.1|\n+------+---+---+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark first function\n##### Returns the first row as a Row."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33bb2f5b-d094-41a7-b3d6-0488443e3cb9"}}},{"cell_type":"code","source":["# first\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a147bb1f-4560-47b3-872e-4e8888163471"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(x.first())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a4f7cc0-404e-4a64-95e5-29bf48a45a61"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Row(from=&#39;Prasad&#39;, to=&#39;Raj&#39;, amt=0.1)\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Row(from=&#39;Prasad&#39;, to=&#39;Raj&#39;, amt=0.1)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Python File operations\n* Open a file\n* Read or write (perform operation)\n* Close the file\n\n\n* f = open(\"test.txt\")      # equivalent to 'r'  \n* f = open(\"test.txt\",'w')  # write in text mode\n* f = open(\"img.bmp\",'r+b') # read and write in binary mode"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d1cce21-6a7f-46e2-a1de-ddd71dead0fb"}}},{"cell_type":"code","source":["dbutils.fs.put('/tmp/dbfsutils_put.txt','this is sample text file created using dbutils.fs.put method....')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92190c4d-756f-4f37-9291-a0230cbdc1bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Wrote 64 bytes.\nOut[13]: True</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote 64 bytes.\nOut[13]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs ls /tmp/dbfsutils_put.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba9d31dd-2af4-4c70-84eb-4ddfdce9f276"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/tmp/dbfsutils_put.txt","dbfsutils_put.txt",64]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/tmp/dbfsutils_put.txt</td><td>dbfsutils_put.txt</td><td>64</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.head('/tmp/dbfsutils_put.txt')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8bd95876-4b33-4b61-8e13-53f98181b14b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["\n#file_object  = open(\"filename\", \"mode\") \n\n#write a file to DBFS using Python I/O APIs\n#File modes - A- Append , w- overwrite , r - read\nwith open(\"/tmp/test_dbfs.txt\", 'w') as f_write:\n  f_write.write(\"Apache Spark is awesome!\\n\")\n  f_write.write(\"End of example!\")\n  f_write.close()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82e2ca9f-319a-43bf-a440-7cda26eb2c8c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# read the file\nwith open(\"/tmp/test_dbfs.txt\", \"r\") as f_read:\n  for line in f_read:\n    print(line)\n    #print(f_read.read())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01bfcd1a-b237-4217-a784-bb60782186a9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Apache Spark is awesome!\n\nEnd of example!\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Apache Spark is awesome!\n\nEnd of example!\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls('file:/tmp/test_dbfs.txt')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e651a21-df6c-47c7-b81a-33bc33ab6176"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[14]: [FileInfo(path=&#39;file:/tmp/test_dbfs.txt&#39;, name=&#39;test_dbfs.txt&#39;, size=40)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: [FileInfo(path=&#39;file:/tmp/test_dbfs.txt&#39;, name=&#39;test_dbfs.txt&#39;, size=40)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["Normal file system\nBig Data file : DBFS (Databricks File system)\nhdfs -> dbfs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a6c3f27-8ee4-45d7-8d83-eeb6985daefe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark foreach function\n##### Applies the f function to all Row of this DataFrame.\n\n* This is a shorthand for df.rdd.foreach()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30835a08-52e4-418e-ac9f-31cd6946eb0d"}}},{"cell_type":"code","source":["def f(x):\n  return x*x\n\nmy_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\nfor i in my_rdd.collect():\n  print(f(i))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1af5853-971f-469b-91c3-363a6a86f6a8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">1\n4\n9\n16\n25\n36\n49\n64\n81\n100\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">1\n4\n9\n16\n25\n36\n49\n64\n81\n100\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["type(foreach_Rdd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cce3b351-d7a9-443c-b83a-8d37d09b90d2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformations or Actions it will return RDD , DF or variable (result set)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ead5f72-0136-4142-8b28-752d4ce1dc5a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# foreach\n#from __future__ import print_function\n\"\"\"  this is sample comments using\ntriple single quotes or double quotes...\"\"\"\n\nfn = 'foreachExampleDataFrames.txt' \nopen(fn, 'w').close()\n\n\n#function for opening a file and appending data using a+ -append method.\ndef writeFile(el,f):\n    '''appends each record  into file using this method '''\n    print(el,file=open(f, 'a+'))\n\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\n# original dataframe\nx.show() \n# writes into foreachExampleDataFrames.txt\nx.foreach(lambda x: writeFile(x,fn)) \n\n#Reading data using python read mode.\nwith open(fn, \"r\") as foreachExample:\n    print (foreachExample.read())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d55ae216-726e-4a67-81e5-d8b8fffefb7a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+---+\n|   from|     to|amt|\n+-------+-------+---+\n| Prasad|    Raj|0.1|\n|    Raj|Sridhar|0.2|\n|Sridhar|   Ravi|0.3|\n+-------+-------+---+\n\nRow(from=&#39;Prasad&#39;, to=&#39;Raj&#39;, amt=0.1)\nRow(from=&#39;Raj&#39;, to=&#39;Sridhar&#39;, amt=0.2)\nRow(from=&#39;Sridhar&#39;, to=&#39;Ravi&#39;, amt=0.3)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+---+\n   from|     to|amt|\n+-------+-------+---+\n Prasad|    Raj|0.1|\n    Raj|Sridhar|0.2|\nSridhar|   Ravi|0.3|\n+-------+-------+---+\n\nRow(from=&#39;Prasad&#39;, to=&#39;Raj&#39;, amt=0.1)\nRow(from=&#39;Raj&#39;, to=&#39;Sridhar&#39;, amt=0.2)\nRow(from=&#39;Sridhar&#39;, to=&#39;Ravi&#39;, amt=0.3)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs ls file:foreachExampleDataFrames.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"506cbf80-43cd-46a3-8441-fd5ab035c3a6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\">\tat org.apache.hadoop.fs.Path.initialize(Path.java:205)\n\tat org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:171)\n\tat org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:245)\n\tat org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:411)\n\tat org.apache.spark.api.python.PythonSecurityUtils$.resolvePaths(PythonSecurityUtils.scala:263)\n\tat org.apache.spark.api.python.PythonSecurityUtils$.checkPathStringFileSystemSafety(PythonSecurityUtils.scala:277)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:80)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:85)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:34)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:1)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:43)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:45)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:47)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw.&lt;init&gt;(command-3789555545015489:49)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw.&lt;init&gt;(command-3789555545015489:51)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read.&lt;init&gt;(command-3789555545015489:53)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$.&lt;init&gt;(command-3789555545015489:57)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$.&lt;clinit&gt;(command-3789555545015489)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$eval$.$print(&lt;notebook&gt;:6)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:233)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:773)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:726)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.URISyntaxException: Relative path in absolute URI: file:foreachExampleDataFrames.txt\n\tat java.net.URI.checkPath(URI.java:1849)\n\tat java.net.URI.&lt;init&gt;(URI.java:745)\n\tat org.apache.hadoop.fs.Path.initialize(Path.java:202)\n\tat org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:171)\n\tat org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:245)\n\tat org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:411)\n\tat org.apache.spark.api.python.PythonSecurityUtils$.resolvePaths(PythonSecurityUtils.scala:263)\n\tat org.apache.spark.api.python.PythonSecurityUtils$.checkPathStringFileSystemSafety(PythonSecurityUtils.scala:277)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:80)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:85)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:34)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:1)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:43)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:45)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:47)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw.&lt;init&gt;(command-3789555545015489:49)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw.&lt;init&gt;(command-3789555545015489:51)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read.&lt;init&gt;(command-3789555545015489:53)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$.&lt;init&gt;(command-3789555545015489:57)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$.&lt;clinit&gt;(command-3789555545015489)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$eval$.$print(&lt;notebook&gt;:6)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:233)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:773)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:726)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)\n\tat java.lang.Thread.run(Thread.java:748)</div>","errorSummary":"IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:foreachExampleDataFrames.txt\nCaused by: URISyntaxException: Relative path in absolute URI: file:foreachExampleDataFrames.txt","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\tat org.apache.hadoop.fs.Path.initialize(Path.java:205)\n\tat org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:171)\n\tat org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:245)\n\tat org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:411)\n\tat org.apache.spark.api.python.PythonSecurityUtils$.resolvePaths(PythonSecurityUtils.scala:263)\n\tat org.apache.spark.api.python.PythonSecurityUtils$.checkPathStringFileSystemSafety(PythonSecurityUtils.scala:277)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:80)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:85)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:34)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:1)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:43)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:45)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:47)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw.&lt;init&gt;(command-3789555545015489:49)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw.&lt;init&gt;(command-3789555545015489:51)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read.&lt;init&gt;(command-3789555545015489:53)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$.&lt;init&gt;(command-3789555545015489:57)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$.&lt;clinit&gt;(command-3789555545015489)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$eval$.$print(&lt;notebook&gt;:6)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:233)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:773)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:726)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.URISyntaxException: Relative path in absolute URI: file:foreachExampleDataFrames.txt\n\tat java.net.URI.checkPath(URI.java:1849)\n\tat java.net.URI.&lt;init&gt;(URI.java:745)\n\tat org.apache.hadoop.fs.Path.initialize(Path.java:202)\n\tat org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:171)\n\tat org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:245)\n\tat org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:411)\n\tat org.apache.spark.api.python.PythonSecurityUtils$.resolvePaths(PythonSecurityUtils.scala:263)\n\tat org.apache.spark.api.python.PythonSecurityUtils$.checkPathStringFileSystemSafety(PythonSecurityUtils.scala:277)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:80)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:85)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:34)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:1)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:43)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:45)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw$$iw.&lt;init&gt;(command-3789555545015489:47)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw$$iw.&lt;init&gt;(command-3789555545015489:49)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$$iw.&lt;init&gt;(command-3789555545015489:51)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read.&lt;init&gt;(command-3789555545015489:53)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$.&lt;init&gt;(command-3789555545015489:57)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$read$.&lt;clinit&gt;(command-3789555545015489)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$eval$.$print(&lt;notebook&gt;:6)\n\tat line2cae9708d6d34e959a6f0b045005994c33.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:233)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:773)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:726)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)\n\tat java.lang.Thread.run(Thread.java:748)</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark freqItems function\n* Finding frequent items for columns, possibly with false positives. \n* Using the frequent element count algorithm described in “https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou”. \n##### cols\t\n* A vector column names to search frequent items in.\n\n##### support\t\n* (Optional) The minimum frequency for an item to be considered frequent. Should be greater than 1e-4. Default support = 0.01."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84be9295-8c22-4000-8500-24f7bf81f996"}}},{"cell_type":"code","source":["# freqItems\nx = sqlContext.createDataFrame([(\"Raj\",\"Sridhar\",0.1), (\"Prasad\",\"Ravi\",0.1), (\"Prasad\",\"Raj\",0.1), (\"Prasad\",\"Raj\",0.5), (\"Sridhar\",\"Raj\",0.1)], ['from','to','amt'])\ny = x.freqItems(cols=['from','amt'],support=0.9)\n\nx.show()\ny.show()\nx.freqItems(['to']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b1980d1-459a-4d78-a654-717c6fa6582d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+---+\n|   from|     to|amt|\n+-------+-------+---+\n|    Raj|Sridhar|0.1|\n| Prasad|   Ravi|0.1|\n| Prasad|    Raj|0.1|\n| Prasad|    Raj|0.5|\n|Sridhar|    Raj|0.1|\n+-------+-------+---+\n\n+--------------+-------------+\n|from_freqItems|amt_freqItems|\n+--------------+-------------+\n|      [Prasad]|        [0.1]|\n+--------------+-------------+\n\n+--------------------+\n|        to_freqItems|\n+--------------------+\n|[Ravi, Sridhar, Raj]|\n+--------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+---+\n   from|     to|amt|\n+-------+-------+---+\n    Raj|Sridhar|0.1|\n Prasad|   Ravi|0.1|\n Prasad|    Raj|0.1|\n Prasad|    Raj|0.5|\nSridhar|    Raj|0.1|\n+-------+-------+---+\n\n+--------------+-------------+\nfrom_freqItems|amt_freqItems|\n+--------------+-------------+\n      [Prasad]|        [0.1]|\n+--------------+-------------+\n\n+--------------------+\n        to_freqItems|\n+--------------------+\n[Ravi, Sridhar, Raj]|\n+--------------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect min(sal),deptno from sample_db.emp group by deptno"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a53d1a93-8056-4d38-8fa1-b25a18a5a832"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark groupBy function\n##### Groups the DataFrame using the specified columns, so we can run aggregation on them. See GroupedData for all the available aggregate functions.\n\n* groupby() is an alias for groupBy().\n\n###### Parameters\n* cols – list of columns to group by. Each element should be a column name (string) or an expression (Column)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"386cf6ea-bc16-4739-a667-4ab6ad4e0f6a"}}},{"cell_type":"code","source":["# groupBy(col1).avg(col2)\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Prasad\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.groupBy('from').min('amt')\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1831306-7fd2-400f-aba1-22cb4c2da755"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+---+\n|   from|     to|amt|\n+-------+-------+---+\n| Prasad|    Raj|0.1|\n| Prasad|Sridhar|0.2|\n|Sridhar|   Ravi|0.3|\n+-------+-------+---+\n\n+-------+--------+\n|   from|min(amt)|\n+-------+--------+\n| Prasad|     0.1|\n|Sridhar|     0.3|\n+-------+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+---+\n   from|     to|amt|\n+-------+-------+---+\n Prasad|    Raj|0.1|\n Prasad|Sridhar|0.2|\nSridhar|   Ravi|0.3|\n+-------+-------+---+\n\n+-------+--------+\n   from|min(amt)|\n+-------+--------+\n Prasad|     0.1|\nSridhar|     0.3|\n+-------+--------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark head function\n#### Returns the first n rows.\n\n* Note This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.\n##### Parameters\n* n – int, default 1. Number of rows to return.\n\n##### Returns\n* If n is greater than 1, return a list of Row. If n is 1, return a single Row."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be57d14c-aa9c-405f-ae87-0e2b63e602be"}}},{"cell_type":"code","source":["# head\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.head(1)\nx.show()\nprint(y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8c8b38a-d383-4513-a475-cafa917ea49b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+---+\n|   from|     to|amt|\n+-------+-------+---+\n| Prasad|    Raj|0.1|\n|    Raj|Sridhar|0.2|\n|Sridhar|   Ravi|0.3|\n+-------+-------+---+\n\n[Row(from=&#39;Prasad&#39;, to=&#39;Raj&#39;, amt=0.1)]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+---+\n   from|     to|amt|\n+-------+-------+---+\n Prasad|    Raj|0.1|\n    Raj|Sridhar|0.2|\nSridhar|   Ravi|0.3|\n+-------+-------+---+\n\n[Row(from=&#39;Prasad&#39;, to=&#39;Raj&#39;, amt=0.1)]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark intersect function\n#### Return a new DataFrame containing rows only in both this DataFrame and another DataFrame.\n\n* This is equivalent to INTERSECT in SQL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30b846bc-b70b-45a5-b170-87bec22e9a71"}}},{"cell_type":"code","source":["# intersect\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Prasad\",0.2),(\"Sridhar\",\"Ravi\",0.1)], ['from','to','amt'])\nz = x.union(y)\nx.show()\ny.show()\nz.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"393ad51c-9119-4fe5-a1c0-d227ab9a1756"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+---+\n|   from|     to|amt|\n+-------+-------+---+\n| Prasad|    Raj|0.1|\n|    Raj|Sridhar|0.2|\n|Sridhar|   Ravi|0.3|\n+-------+-------+---+\n\n+-------+------+---+\n|   from|    to|amt|\n+-------+------+---+\n| Prasad|   Raj|0.1|\n|    Raj|Prasad|0.2|\n|Sridhar|  Ravi|0.1|\n+-------+------+---+\n\n+-------+-------+---+\n|   from|     to|amt|\n+-------+-------+---+\n| Prasad|    Raj|0.1|\n|    Raj|Sridhar|0.2|\n|Sridhar|   Ravi|0.3|\n| Prasad|    Raj|0.1|\n|    Raj| Prasad|0.2|\n|Sridhar|   Ravi|0.1|\n+-------+-------+---+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+---+\n   from|     to|amt|\n+-------+-------+---+\n Prasad|    Raj|0.1|\n    Raj|Sridhar|0.2|\nSridhar|   Ravi|0.3|\n+-------+-------+---+\n\n+-------+------+---+\n   from|    to|amt|\n+-------+------+---+\n Prasad|   Raj|0.1|\n    Raj|Prasad|0.2|\nSridhar|  Ravi|0.1|\n+-------+------+---+\n\n+-------+-------+---+\n   from|     to|amt|\n+-------+-------+---+\n Prasad|    Raj|0.1|\n    Raj|Sridhar|0.2|\nSridhar|   Ravi|0.3|\n Prasad|    Raj|0.1|\n    Raj| Prasad|0.2|\nSridhar|   Ravi|0.1|\n+-------+-------+---+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark join function\n\n#### Joins with another DataFrame, using the given join expression.\n\n##### Parameters\n* other – Right side of the join\n\n* on – a string for the join column name, a list of column names, a join expression (Column), or a list of Columns. If on is a string or a list of strings indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join.\n\n* how – str, default inner. Must be one of: inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer, semi, leftsemi, left_semi, anti, leftanti and left_anti."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02ecf908-d0fe-4279-9de4-78fce0d251e5"}}},{"cell_type":"code","source":["z = x.join(y,x.to == y.name,'full')\nz.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20b4e4bf-a8fb-438e-8f4e-bb0947493283"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# join\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = sqlContext.createDataFrame([('Prasad',20),(\"Raj\",40),(\"Ravi\",80)], ['name','age'])\nz = x.join(y,x.to == y.name,'anti').select('*')\nx.show()\ny.show()\nz.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3329ffa-a2be-40c1-8d1f-f84b752d0924"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+---+\n|   from|     to|amt|\n+-------+-------+---+\n| Prasad|    Raj|0.1|\n|    Raj|Sridhar|0.2|\n|Sridhar|   Ravi|0.3|\n+-------+-------+---+\n\n+------+---+\n|  name|age|\n+------+---+\n|Prasad| 20|\n|   Raj| 40|\n|  Ravi| 80|\n+------+---+\n\n+----+-------+---+\n|from|     to|amt|\n+----+-------+---+\n| Raj|Sridhar|0.2|\n+----+-------+---+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+---+\n   from|     to|amt|\n+-------+-------+---+\n Prasad|    Raj|0.1|\n    Raj|Sridhar|0.2|\nSridhar|   Ravi|0.3|\n+-------+-------+---+\n\n+------+---+\n  name|age|\n+------+---+\nPrasad| 20|\n   Raj| 40|\n  Ravi| 80|\n+------+---+\n\n+----+-------+---+\nfrom|     to|amt|\n+----+-------+---+\n Raj|Sridhar|0.2|\n+----+-------+---+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark limit function\n##### Limits the result count to the number specified."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bde9a6ca-cc67-467d-a37f-c415da8c5e1f"}}},{"cell_type":"code","source":["# limit\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.limit(2)\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4bc95833-df52-4821-81e0-99273121ae5b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark orderBy function\n#### Returns a new DataFrame sorted by the specified column(s).\n\n##### Parameters\n* cols – list of Column or column names to sort by.\n\n* ascending – boolean or list of boolean (default True). Sort ascending vs. descending. Specify list for multiple sort orders. If a list is specified, length of the list must equal length of the cols."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2325975b-88ba-40ca-ac9a-974cffaaca7d"}}},{"cell_type":"code","source":["# orderBy\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.orderBy(['from'],ascending=[False])\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2dae6af7-d644-4f68-8975-f0dd5762c07b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark printSchema function\n##### Prints out the schema in the tree format."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0345ba91-d21a-4272-afa0-a04147d5b6c7"}}},{"cell_type":"code","source":["# printSchema\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\nx.show()\nx.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e5e3f64-b968-4b99-bb0e-33d009f1ccb0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark randomSplit function\n#### Randomly splits this DataFrame with the provided weights.\n\n##### Parameters\n* weights – list of doubles as weights with which to split the DataFrame. Weights will be normalized if they don’t sum up to 1.0.\n\n* seed – The seed for sampling."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2975f8ee-8656-4f58-9216-9af7b95431cb"}}},{"cell_type":"code","source":["# randomSplit\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3),(\"Vamsi\",\"Vinod\",0.4)], ['from','to','amt'])\ny = x.randomSplit([0.6,0.4])\nx.show()\ny[0].show()\ny[1].show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6f9f2cc-f0d5-4e8c-9670-9dff2fef45ea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark rdd function\n##### Returns the content as an pyspark.RDD of Row."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d5cf87e-6f2e-4824-ae13-603aeece3069"}}},{"cell_type":"code","source":["# rdd\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.rdd\nx.show()\nprint(y.collect())\nprint('X type is : ',type(x))\nprint('Y Type is : ',type(y))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14ef7324-1d12-48af-8682-51d228044f96"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark registerTempTable function\n##### Registers this DataFrame as a temporary table using the given name.\n\n* The lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f917589-05a2-4db4-b5cf-db82d34d15ba"}}},{"cell_type":"code","source":["# registerTempTable\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\nx.registerTempTable(name=\"TRANSACTIONS\")\ny = sqlContext.sql('SELECT * FROM TRANSACTIONS WHERE amt > 0.1')\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7a3ec42-8f1e-4129-86d8-addab166b7c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from TRANSACTIONS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76548248-3b2e-46cf-bd2e-bef37dda4fd4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["x.createOrReplaceTempView(\"x_df\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eaf5c6e6-44af-435a-a843-50e068455b9c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#data frame we can use in pyspark\n#if we want to use in sql we have to convert Dataframe to SQL View...\n#We can create sql view on top of DataFrame..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"731a14bd-4b3a-4254-a4a2-9be80fd7dea8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["emp_csv = spark.read.csv(\"/FileStore/tables/emp.csv\",header=True,inferSchema=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bbfc491-ab0b-4579-acdb-58d92c99bcea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(emp_csv)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e33c2907-505b-46a2-a572-9b2ede3fdd55"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["emp_csv.createGlobalTempView(\"global_emp_view\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07aaa30b-8e94-4d91-a590-c17bc1973933"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from global_temp.global_emp_view"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49946f2a-72de-4d49-8cfd-e1fa58045db6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from x_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b898ea60-5f0c-429a-91a7-b1d0af1e1182"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from TRANSACTIONS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee6c63c8-6a2b-44f7-8565-aae195889050"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark replace function\n* Returns a new DataFrame replacing a value with another value. DataFrame.replace() and DataFrameNaFunctions.replace() are aliases of each other. Values to_replace and value must have the same type and can only be numerics, booleans, or strings. Value can have None. When replacing, the new value will be cast to the type of the existing column. For numeric replacements all values to be replaced should have unique floating point representation. In case of conflicts (for example with {42: -1, 42.0: 1}) and arbitrary replacement will be used.\n\n##### Parameters\n* to_replace – bool, int, long, float, string, list or dict. Value to be replaced. If the value is a dict, then value is ignored or can be omitted, and to_replace must be a mapping between a value and a replacement.\n\n* value – bool, int, long, float, string, list or None. The replacement value must be a bool, int, long, float, string or None. If value is a list, value should be of the same length and type as to_replace. If value is a scalar and to_replace is a sequence, then value is used as a replacement for each item in to_replace.\n\n* subset – optional list of column names to consider. Columns specified in subset that do not have matching data type are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e437e15-775b-4f7e-b280-afd5f59d19f9"}}},{"cell_type":"code","source":["# replace\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.replace('Raj','Ram',['from'])\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20500743-3e11-4266-bc83-813ba6106ea9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark select function\n##### Projects a set of expressions and returns a new DataFrame.\n\n##### Parameters\n* cols – list of column names (string) or expressions (Column). If one of the column names is ‘*’, that column is expanded to include all columns in the current DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5b39ec3-3819-471a-824c-9882612cd7cd"}}},{"cell_type":"code","source":["# select\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.select(['to','amt'])\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c71f91ef-3dd6-4f64-b334-a6bd5762e3eb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["emp_csv =spark.sql('select * from sample_db.emp')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83063e90-e378-4db2-b209-a61f9fc4367d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark selectExpr function\n##### Projects a set of SQL expressions and returns a new DataFrame.\n\n* This is a variant of select() that accepts SQL expressions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d12f8e75-afd2-43cc-918b-94fe6b88b29c"}}},{"cell_type":"code","source":["# selectExpr\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.selectExpr(['from','to','amt+121'])\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ea3480c-99c2-4a77-9166-f3e56755a187"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# show\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\nx.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"660f206d-5c62-46a8-8410-bb3bf3c2b090"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark sort function\n##### Returns a new DataFrame sorted by the specified column(s).\n\n###### Parameters\n* cols – list of Column or column names to sort by.\n\n* ascending – boolean or list of boolean (default True). Sort ascending vs. descending. Specify list for multiple sort orders. If a list is specified, length of the list must equal length of the cols."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29e9dc77-0c08-4fca-8e4e-2e3ac76b00c2"}}},{"cell_type":"code","source":["# sort\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Prasad\",0.3)], ['from','to','amt'])\ny = x.sort(['from'])\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e6ea2c7-5142-416d-af03-23430802dc94"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark subtract function\n##### Return a new DataFrame containing rows in this DataFrame but not in another DataFrame.\n\n* This is equivalent to EXCEPT DISTINCT in SQL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d6a0818-b885-4715-9e82-03ba1beff1cd"}}},{"cell_type":"code","source":["# subtract\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.1)], ['from','to','amt'])\nz = x.subtract(y)\nx.show()\ny.show()\nz.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5c4f7a9-dd4e-44b8-9141-69bd1546bad4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark unionAll function\n##### Return a new DataFrame containing union of rows in this and another DataFrame.\n\n* This is equivalent to UNION ALL in SQL. To do a SQL-style set union (that does deduplication of elements), use this function followed by distinct().\n\n* Also as standard in SQL, this function resolves columns by position (not by name)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0edaa9a-67ae-49b1-8182-f8382b4dcd20"}}},{"cell_type":"code","source":["\n# unionAll\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2)], ['from','to','amt'])\ny = sqlContext.createDataFrame([(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.1)], ['from','to','amt'])\nz = x.unionAll(y)\nx.show()\ny.show()\nz.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc29f79d-626e-43cd-88f5-9981ac4ded38"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark take function\n###### Returns the first num rows as a list of Row."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3e88570-47c7-448e-a988-7215e2d95aa5"}}},{"cell_type":"code","source":["# take\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.take(2)\nx.show()\nprint(y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73fb63f3-1a90-4f05-98f7-bf0a7d8ee291"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark toDF function\n##### Returns a new DataFrame that with new specified column names\n\n###### Parameters\n* cols – list of new column names (string)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"181afaf1-4eca-4225-98b9-d30eb45294b2"}}},{"cell_type":"code","source":["# toDF\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.toDF(\"seller\",\"buyer\",\"amount\")\nx.show()\ny.show()\nprint(type(y))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92674259-35d1-4968-8c4e-f656525142bc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark toJSON function\n##### Converts a DataFrame into a RDD of string.\n\n* Each row is turned into a JSON document as one element in the returned RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b31d0f4a-60e7-47fe-9449-7ea134dd2fa8"}}},{"cell_type":"code","source":["# toJSON\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Prasad\",0.3)], ['from','to','amt'])\ny = x.toJSON()\nx.show()\nprint(y.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23a34ec1-2202-4696-a291-93b6f19e813f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark toPandas function\n##### Returns the contents of this DataFrame as Pandas pandas.DataFrame.\n\n* This is only available if Pandas is installed and available.\n\n* Note This method should only be used if the resulting Pandas’s DataFrame is expected to be small, as all the data is loaded into the driver’s memory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0477c110-231f-422c-a729-c3b982a7bfbd"}}},{"cell_type":"code","source":["# toPandas\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.toPandas()\nx.show()\nprint(type(y))\ny"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56dacdea-28ca-4b34-83ba-df116d7c1f65"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark where function\n##### where() is an alias for filter()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"187c3b86-fafe-4a99-8531-ba32e9c9653e"}}},{"cell_type":"code","source":["# where (filter)\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.where(\"amt == 0.1\")\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a65ee73-9edd-45a1-9703-2ec6285c695a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["sal,comm => total_sal (sal+comm)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0d4022b-54a8-4c44-afcc-45035885c672"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark withColumn function\n##### Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n\n* The column expression must be an expression over this DataFrame; attempting to add a column from some other DataFrame will raise an error.\n\n##### Parameters\n* colName – string, name of the new column.\n\n* col – a Column expression for the new column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f801e255-4b61-4d7c-9a1c-6e89f600c961"}}},{"cell_type":"code","source":["# withColumn\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",None),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.withColumn('conf',x.amt.isNotNull())\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd5addbf-05f4-4991-9638-0dd44d0f5617"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark withColumnRenamed function\n##### Returns a new DataFrame by renaming an existing column. This is a no-op if schema doesn’t contain the given column name.\n\n##### Parameters\n* existing – string, name of the existing column to rename.\n\n* new – string, new name of the column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50588e7a-da90-40b9-8017-8c4354c0560e"}}},{"cell_type":"code","source":["# withColumnRenamed\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.withColumnRenamed('amt','amount')\nx.show()\ny.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2932606-cd19-4770-899c-d6fc3703e725"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### PySpark write function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26c41a1e-7f97-4309-9716-d2218859b6c5"}}},{"cell_type":"code","source":["# write\nimport json\nx = sqlContext.createDataFrame([('Prasad',\"Raj\",0.1),(\"Raj\",\"Sridhar\",0.2),(\"Sridhar\",\"Ravi\",0.3)], ['from','to','amt'])\ny = x.write.mode('overwrite').json('/tmp/dataframeWriteExample.json')\nx.show()\n# read the dataframe back in from file\nsqlContext.read.text('/tmp/dataframeWriteExample.json').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c32d8631-5935-4f97-91eb-3c502572e270"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Tutorial_3_DataFrame_Operations","dashboards":[],"language":"python","widgets":{},"notebookOrigID":627595547634624}},"nbformat":4,"nbformat_minor":0}
