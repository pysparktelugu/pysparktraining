{"cells":[{"cell_type":"code","source":["#spark context variable\n#Note: in databricks sparkSession (spark) ,sparkContext(sc),sqlcontext and hivecontext are available default.\nsc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff94d519-e956-4e91-a0c9-283eaaf531e9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: </div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.228.93:46225\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.228.93:46225\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["#install pyspark module\npip install pyspark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65cc37d4-5826-43f3-8d0e-11df1ce3efb7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>Requirement already satisfied: pyspark in c:\\users\\raveendra\\anaconda3\\lib\\site-packages (3.0.1)\nRequirement already satisfied: py4j==0.10.9 in c:\\users\\raveendra\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9)\nNote: you may need to restart the kernel to use updated packages.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>Requirement already satisfied: pyspark in c:\\users\\raveendra\\anaconda3\\lib\\site-packages (3.0.1)\nRequirement already satisfied: py4j==0.10.9 in c:\\users\\raveendra\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9)\nNote: you may need to restart the kernel to use updated packages.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#install findspark for spark intialization \npip install findspark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53f4d476-a7dd-428e-8ab5-68f3c10d9677"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>Requirement already satisfied: findspark in c:\\users\\raveendra\\anaconda3\\lib\\site-packages (1.4.2)\nNote: you may need to restart the kernel to use updated packages.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>Requirement already satisfied: findspark in c:\\users\\raveendra\\anaconda3\\lib\\site-packages (1.4.2)\nNote: you may need to restart the kernel to use updated packages.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#### Importing findspark and intializing findspark..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f9ea609-2b2f-4797-aeae-a602b01f6b8c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#intialize finspark using init() method\nimport findspark\nfindspark.init()\n#findspark.init(SPARK_HOME)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33552110-6395-4c09-b185-63a96a486b3d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#### Creating Spark Session"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ecf5316-1dfe-48e0-a911-30bc3a51730f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder\\\n.master(\"local\")\\\n.appName(\"Localspark\")\\\n.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7f68318-6916-43d4-8048-703f320e97b1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#spark session variabl\nspark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4253903e-0f05-47d6-b7da-3f7582130c0b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://VICKY:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Localspark</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://VICKY:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Localspark</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n#spark = SparkSession.builder\\\n#.master(\"local\".format(2))\\\n#.appName(\"Localspark\")\\\n#.getOrCreate()\n\nn_cpu = 2\nspark = SparkSession.builder.appName('localSpark')\\\n.master('local[{}]'.format(n_cpu))\\\n.config(\"spark.driver.memory\", \"1g\")\\\n.config('spark.executor.memory', '2g')\\\n.config('spark.executor.cores', '3')\\\n.config('spark.cores.max', '3')\\\n.getOrCreate()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7ef7483-204e-475d-9059-e78d00e9ed18"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d80716a8-8073-4170-ae85-c944c66d6906"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://VICKY:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[2]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>localSpark</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://VICKY:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[2]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>localSpark</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark import SparkContext\n\nsc = SparkContext.getOrCreate()\n\nsc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"048cc0bd-01bd-40fe-a6d9-761924f56ca6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://VICKY:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[2]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>localSpark</code></dd>\n            </dl>\n        </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://VICKY:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[2]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>localSpark</code></dd>\n            </dl>\n        </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["#stop spark session\nspark.stop()\n# stop SparkContext\nsc.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbc058e5-eeed-4df9-b43c-300c0f42a1ea"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8522b21-bdc0-4c8a-ba73-bbce571d4dc4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://VICKY:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[2]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>localSpark</code></dd>\n            </dl>\n        </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://VICKY:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[2]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>localSpark</code></dd>\n            </dl>\n        </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["#Setting spark Configuration....\nconf = spark.sparkContext._conf.setAll([('spark.executor.memory', '4g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','2g')])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96056beb-2e60-4650-a7cb-ba9e0441721e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Get all configuration \nsc.getConf().getAll()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c22d5ae-8154-47a9-b316-2b570dc9eef8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[5]: [(&#39;spark.files.useFetchCache&#39;, &#39;false&#39;),\n (&#39;spark.databricks.preemption.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.abfs.impl&#39;,\n  &#39;shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&#39;),\n (&#39;spark.driver.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.hadoop.fs.adl.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.max&#39;, &#39;10&#39;),\n (&#39;spark.hadoop.fs.s3a.connection.maximum&#39;, &#39;200&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload.active.blocks&#39;, &#39;32&#39;),\n (&#39;spark.shuffle.reduceLocality.enabled&#39;, &#39;false&#39;),\n (&#39;spark.sql.streaming.checkpointFileManagerClass&#39;,\n  &#39;com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager&#39;),\n (&#39;spark.databricks.service.dbutils.repl.backend&#39;,\n  &#39;com.databricks.dbconnect.ReplDBUtils&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverNodeType&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.hadoop.spark.sql.sources.outputCommitterClass&#39;,\n  &#39;com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter&#39;),\n (&#39;spark.hadoop.fs.AbstractFileSystem.gs.impl&#39;,\n  &#39;shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceBootstrapType&#39;, &#39;ssh&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.allowBatching&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterSource&#39;, &#39;UI&#39;),\n (&#39;spark.hadoop.hive.server2.transport.mode&#39;, &#39;http&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.driverNodeTypeId&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.sql.parquet.compression.codec&#39;, &#39;snappy&#39;),\n (&#39;spark.databricks.clusterUsageTags.hailEnabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.containerType&#39;, &#39;LXC&#39;),\n (&#39;spark.eventLog.enabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.isIMv2Enabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories&#39;,\n  &#39;false&#39;),\n (&#39;spark.hadoop.fs.wasb.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstancePrivateIp&#39;,\n  &#39;10.172.253.76&#39;),\n (&#39;spark.executor.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerOrgId&#39;, &#39;1513371373162116&#39;),\n (&#39;spark.databricks.workerNodeTypeId&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.hadoop.mapred.output.committer.class&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DirectOutputCommitter&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.port&#39;, &#39;10000&#39;),\n (&#39;spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version&#39;, &#39;2&#39;),\n (&#39;spark.sql.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeSize&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.spark.driverproxy.customHeadersToProperties&#39;,\n  &#39;X-Databricks-SqlGateway-SessionId:X-Databricks-SqlGateway-SessionId,X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name&#39;),\n (&#39;spark.home&#39;, &#39;/databricks/spark&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterTargetWorkers&#39;, &#39;0&#39;),\n (&#39;spark.sql.warehouse.dir&#39;, &#39;/user/hive/warehouse&#39;),\n (&#39;spark.hadoop.hive.server2.idle.operation.timeout&#39;, &#39;7200000&#39;),\n (&#39;spark.task.reaper.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.passthrough.s3a.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider&#39;),\n (&#39;spark.storage.memoryFraction&#39;, &#39;0.5&#39;),\n (&#39;spark.databricks.session.share&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAllTags&#39;,\n  &#39;[{&#34;key&#34;:&#34;Name&#34;,&#34;value&#34;:&#34;ce4-worker&#34;}]&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerPrivateIp&#39;,\n  &#39;10.172.228.93&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterResourceClass&#39;, &#39;default&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterFirstOnDemand&#39;, &#39;0&#39;),\n (&#39;spark.driver.maxResultSize&#39;, &#39;4g&#39;),\n (&#39;spark.databricks.delta.multiClusterWrites.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSku&#39;, &#39;STANDARD_SKU&#39;),\n (&#39;spark.worker.cleanup.enabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.gs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.executor.extraJavaOptions&#39;,\n  &#39;-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1&#39;),\n (&#39;spark.databricks.workspace.matplotlibInline.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableCredentialPassthrough&#39;, &#39;false&#39;),\n (&#39;spark.executor.cores&#39;, &#39;4&#39;),\n (&#39;spark.cores.max&#39;, &#39;4&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType&#39;,\n  &#39;ebs_volume_type: GENERAL_PURPOSE_SSD\\n&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJdbcAutoStart&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstanceId&#39;, &#39;i-0418ea1baca5ca6a7&#39;),\n (&#39;spark.hadoop.fs.wasb.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeType&#39;,\n  &#39;GENERAL_PURPOSE_SSD&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDestination&#39;, &#39;&#39;),\n (&#39;spark.cleaner.referenceTracking.blocking&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.parquet.page.size.check.estimate&#39;, &#39;false&#39;),\n (&#39;spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory&#39;),\n (&#39;spark.databricks.delta.preview.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.isSingleUserCluster&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverPublicDns&#39;,\n  &#39;ec2-54-189-89-160.us-west-2.compute.amazonaws.com&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterState&#39;, &#39;Pending&#39;),\n (&#39;spark.databricks.tahoe.logStore.azure.class&#39;,\n  &#39;com.databricks.tahoe.store.AzureLogStore&#39;),\n (&#39;spark.hadoop.fs.azure.skip.metrics&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.spark.thriftserver.sqlGatewayCloseSessionHeaderName&#39;,\n  &#39;X-Databricks-SqlGateway-CloseSession&#39;),\n (&#39;spark.hadoop.fs.s3.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.master&#39;, &#39;local[8]&#39;),\n (&#39;spark.scheduler.mode&#39;, &#39;FAIR&#39;),\n (&#39;spark.repl.class.uri&#39;, &#39;spark://10.172.228.93:43252/classes&#39;),\n (&#39;spark.databricks.delta.logStore.crossCloud.fatal&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterWorkers&#39;, &#39;0&#39;),\n (&#39;spark.files.fetchFailure.unRegisterOutputOnHost&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableSqlAclsOnly&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeCount&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNumSshKeys&#39;, &#39;0&#39;),\n (&#39;spark.databricks.tahoe.logStore.aws.class&#39;,\n  &#39;com.databricks.tahoe.store.S3LockBasedLogStore&#39;),\n (&#39;spark.driver.memory&#39;, &#39;2g&#39;),\n (&#39;spark.speculation.quantile&#39;, &#39;0.9&#39;),\n (&#39;spark.shuffle.manager&#39;, &#39;SORT&#39;),\n (&#39;spark.files.overwrite&#39;, &#39;true&#39;),\n (&#39;spark.sql.hive.metastore.sharedPrefixes&#39;,\n  &#39;org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks&#39;),\n (&#39;spark.databricks.io.directoryCommit.enableLogicalDelete&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.spark.thriftserver.sqlGatewaySessionIdHeaderName&#39;,\n  &#39;X-Databricks-SqlGateway-SessionId&#39;),\n (&#39;spark.task.reaper.killTimeout&#39;, &#39;60s&#39;),\n (&#39;spark.r.numRBackendThreads&#39;, &#39;1&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.min&#39;, &#39;10&#39;),\n (&#39;spark.hadoop.hive.server2.use.SSL&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.workspace.multipleResults.enabled&#39;, &#39;true&#39;),\n (&#39;spark.sql.hive.metastore.version&#39;, &#39;0.13.0&#39;),\n (&#39;spark.shuffle.service.port&#39;, &#39;4048&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType&#39;, &#39;default&#39;),\n (&#39;spark.databricks.acl.client&#39;,\n  &#39;com.databricks.spark.sql.acl.client.SparkSqlAclClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAvailability&#39;, &#39;ON_DEMAND&#39;),\n (&#39;spark.hadoop.hive.warehouse.subdir.inherit.perms&#39;, &#39;false&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.closeFileAfterWrite&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb&#39;, &#39;0&#39;),\n (&#39;spark.driver.host&#39;, &#39;10.172.228.93&#39;),\n (&#39;spark.databricks.sparkContextId&#39;, &#39;3301741754963124117&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.path&#39;,\n  &#39;/databricks/keys/jetty-ssl-driver-keystore.jks&#39;),\n (&#39;spark.databricks.credential.redactor&#39;,\n  &#39;com.databricks.logging.secrets.CredentialRedactorProxyImpl&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPinned&#39;, &#39;false&#39;),\n (&#39;spark.databricks.acl.provider&#39;,\n  &#39;com.databricks.sql.acl.ReflectionBackedAclProvider&#39;),\n (&#39;spark.hadoop.fs.s3n.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.extraListeners&#39;,\n  &#39;com.databricks.backend.daemon.driver.DBCEventLoggingListener&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableElasticDisk&#39;, &#39;false&#39;),\n (&#39;spark.executor.extraClassPath&#39;,\n  &#39;/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/api-base--api-base_java-spark_3.0_2.12_deploy.jar:/databricks/jars/api-base--api-base-spark_3.0_2.12_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_3.0_2.12_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_3.0_2.12_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_3.0_2.12_deploy.jar:/databricks/jars/common--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/common--common-spark_3.0_2.12_deploy.jar:/databricks/jars/common--credentials--credentials-spark_3.0_2.12_deploy.jar:/databricks/jars/common--crypto-providers--amazon-corretto-crypto-provider--libamazon-corretto-crypto-provider.jar:/databricks/jars/common--hadoop--hadoop-spark_3.0_2.12_deploy.jar:/databricks/jars/common--java-flight-recorder--java-flight-recorder-spark_3.0_2.12_deploy.jar:/databricks/jars/common--jetty--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/common--lazy--lazy-spark_3.0_2.12_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--path--path-spark_3.0_2.12_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_3.0_2.12_deploy.jar:/databricks/jars/common--reflection--reflection-spark_3.0_2.12_deploy.jar:/databricks/jars/common--storage--storage-spark_3.0_2.12_deploy.jar:/databricks/jars/common--tracing--tracing-spark_3.0_2.12_deploy.jar:/databricks/jars/common--util--locks-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_3.0_2.12_deploy.jar:/databricks/jars/dbfs--exceptions--exceptions-spark_3.0_2.12_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_3.0_2.12_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_3.0_2.12_deploy.jar:/databricks/jars/extern--extern-spark_3.0_2.12_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/jsonutil--jsonutil-spark_3.0_2.12_deploy.jar:/databricks/jars/libraries--api--managedLibraries--managedLibraries-spark_3.0_2.12_deploy.jar:/databricks/jars/libraries--api--typemappers--typemappers-spark_3.0_2.12_deploy.jar:/databricks/jars/libraries--libraries-spark_3.0_2.12_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_3.0_2.12_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_3.0_2.12_deploy.jar:/databricks/jars/maven-trees--amazon-corretto-crypto-provider--software.amazon.cryptools--AmazonCorrettoCryptoProvider-linux-x86_64--software.amazon.cryptools__AmazonCorrettoCryptoProvider-linux-x86_64__1.4.0.jar:/databricks/jars/s3commit--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/s3commit--common--common-spark_3.0_2.12_deploy.jar:/databricks/jars/s3--s3-spark_3.0_2.12_deploy.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/secret-manager--api--api-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--command--api--api-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--command--command-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--common--spark-common-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--common-utils--utils-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--display--display-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--driver-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--events-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--sql-extension--sql-extension-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--versions--3.0--shim_2.12_deploy.jar:/databricks/jars/spark--versions--3.0--spark_2.12_deploy.jar:/databricks/jars/sqlgateway--common--endpoint_id-spark_3.0_2.12_deploy.jar:/databricks/jars/sqlgateway--history--api--api-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-annotations_com.fasterxml.jackson.core__jackson-annotations__2.11.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-core_com.fasterxml.jackson.core__jackson-core__2.11.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-databind_com.fasterxml.jackson.core__jackson-databind__2.11.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.android_annotations_com.google.android__annotations__4.1.1.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.api.grpc_proto-google-common-protos_com.google.api.grpc__proto-google-common-protos__1.17.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.findbugs_jsr305_com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.gson_gson_com.google.code.gson__gson__2.8.6_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.errorprone_error_prone_annotations_com.google.errorprone__error_prone_annotations__2.3.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_failureaccess_com.google.guava__failureaccess__1.0.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_guava_com.google.guava__guava__29.0-android_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_listenablefuture_com.google.guava__listenablefuture__9999.0-empty-to-avoid-conflict-with-guava_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.j2objc_j2objc-annotations_com.google.j2objc__j2objc-annotations__1.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java_com.google.protobuf__protobuf-java__3.12.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java-util_com.google.protobuf__protobuf-java-util__3.12.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-brave_com.linecorp.armeria__armeria-brave__1.0.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria_com.linecorp.armeria__armeria__1.0.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc_com.linecorp.armeria__armeria-grpc__1.0.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc-protocol_com.linecorp.armeria__armeria-grpc-protocol__1.0.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-api_io.grpc__grpc-api__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-context_io.grpc__grpc-context__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-core_io.grpc__grpc-core__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf_io.grpc__grpc-protobuf__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf-lite_io.grpc__grpc-protobuf-lite__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-services_io.grpc__grpc-services__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-stub_io.grpc__grpc-stub__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.micrometer_micrometer-core_io.micrometer__micrometer-core__1.5.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-buffer_io.netty__netty-buffer__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-dns_io.netty__netty-codec-dns__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-haproxy_io.netty__netty-codec-haproxy__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http2_io.netty__netty-codec-http2__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http_io.netty__netty-codec-http__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec_io.netty__netty-codec__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-socks_io.netty__netty-codec-socks__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-common_io.netty__netty-common__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler_io.netty__netty-handler__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler-proxy_io.netty__netty-handler-proxy__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver-dns_io.netty__netty-resolver-dns__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver_io.netty__netty-resolver__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-tcnative-boringssl-static_io.netty__netty-tcnative-boringssl-static__2.0.31.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport_io.netty__netty-transport__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-epoll-linux-x86_64_io.netty__netty-transport-native-epoll-linux-x86_64__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common_io.netty__netty-transport-native-unix-common__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common-linux-x86_64_io.netty__netty-transport-native-unix-common-linux-x86_64__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.perfmark_perfmark-api_io.perfmark__perfmark-api__0.19.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.brave_brave-instrumentation-http_io.zipkin.brave__brave-instrumentation-http__5.12.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.brave_brave_io.zipkin.brave__brave__5.12.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.reporter2_zipkin-reporter-brave_io.zipkin.reporter2__zipkin-reporter-brave__2.15.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.reporter2_zipkin-reporter_io.zipkin.reporter2__zipkin-reporter__2.15.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.zipkin2_zipkin_io.zipkin.zipkin2__zipkin__2.21.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_jakarta.annotation_jakarta.annotation-api_jakarta.annotation__jakarta.annotation-api__1.3.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_liball_deps_2.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_net.bytebuddy_byte-buddy_net.bytebuddy__byte-buddy__1.10.9_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.checkerframework_checker-compat-qual_org.checkerframework__checker-compat-qual__2.5.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.codehaus.mojo_animal-sniffer-annotations_org.codehaus.mojo__animal-sniffer-annotations__1.18_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.curioswitch.curiostack_protobuf-jackson_org.curioswitch.curiostack__protobuf-jackson__1.1.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.hdrhistogram_HdrHistogram_org.hdrhistogram__HdrHistogram__2.1.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.joda_joda-convert_org.joda__joda-convert__2.2.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.latencyutils_LatencyUtils_org.latencyutils__LatencyUtils__2.0.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.reactivestreams_reactive-streams_org.reactivestreams__reactive-streams__1.0.3_shaded.jar:/databricks/jars/third_party--armeria--service_discovery-resources.jar:/databricks/jars/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-client-runtime__1.7.8_container_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-storage__8.6.4_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.rest__client-runtime__1.7.8_container_shaded.jar:/databricks/jars/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:/databricks/jars/third_party--datalake--datalake-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--dropwizard-metrics-log4j-v3.2.6--metrics-log4j-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--animal-sniffer-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--annotations_shaded.jar:/databricks/jars/third_party--gcs-private--api-common_shaded.jar:/databricks/jars/third_party--gcs-private--auto-value-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--checker-compat-qual_shaded.jar:/databricks/jars/third_party--gcs-private--checker-qual_shaded.jar:/databricks/jars/third_party--gcs-private--commons-codec_shaded.jar:/databricks/jars/third_party--gcs-private--commons-lang3_shaded.jar:/databricks/jars/third_party--gcs-private--commons-logging_shaded.jar:/databricks/jars/third_party--gcs-private--conscrypt-openjdk-uber_shaded.jar:/databricks/jars/third_party--gcs-private--error_prone_annotations_shaded.jar:/databricks/jars/third_party--gcs-private--failureaccess_shaded.jar:/databricks/jars/third_party--gcs-private--flogger_shaded.jar:/databricks/jars/third_party--gcs-private--flogger-slf4j-backend_shaded.jar:/databricks/jars/third_party--gcs-private--flogger-system-backend_shaded.jar:/databricks/jars/third_party--gcs-private--gcs-connector_shaded.jar:/databricks/jars/third_party--gcs-private--gcsio_shaded.jar:/databricks/jars/third_party--gcs-private--gcs-shaded-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--google-api-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client-java6_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-services-iamcredentials_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-services-storage_shaded.jar:/databricks/jars/third_party--gcs-private--google-auth-library-credentials_shaded.jar:/databricks/jars/third_party--gcs-private--google-auth-library-oauth2-http_shaded.jar:/databricks/jars/third_party--gcs-private--google-extensions_shaded.jar:/databricks/jars/third_party--gcs-private--google-http-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs-private--google-http-client_shaded.jar:/databricks/jars/third_party--gcs-private--google-oauth-client-java6_shaded.jar:/databricks/jars/third_party--gcs-private--google-oauth-client_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-alts_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-api_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-auth_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-context_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-core_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-grpclb_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-netty-shaded_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-protobuf-lite_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-protobuf_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-stub_shaded.jar:/databricks/jars/third_party--gcs-private--gson_shaded.jar:/databricks/jars/third_party--gcs-private--guava_shaded.jar:/databricks/jars/third_party--gcs-private--httpclient_shaded.jar:/databricks/jars/third_party--gcs-private--httpcore_shaded.jar:/databricks/jars/third_party--gcs-private--j2objc-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--jackson-core_shaded.jar:/databricks/jars/third_party--gcs-private--javax.annotation-api_shaded.jar:/databricks/jars/third_party--gcs-private--jsr305_shaded.jar:/databricks/jars/third_party--gcs-private--listenablefuture_shaded.jar:/databricks/jars/third_party--gcs-private--opencensus-api_shaded.jar:/databricks/jars/third_party--gcs-private--opencensus-contrib-http-util_shaded.jar:/databricks/jars/third_party--gcs-private--perfmark-api_shaded.jar:/databricks/jars/third_party--gcs-private--protobuf-java_shaded.jar:/databricks/jars/third_party--gcs-private--protobuf-java-util_shaded.jar:/databricks/jars/third_party--gcs-private--proto-google-common-protos_shaded.jar:/databricks/jars/third_party--gcs-private--proto-google-iam-v1_shaded.jar:/databricks/jars/third_party--gcs-private--util-hadoop_shaded.jar:/databricks/jars/third_party--gcs-private--util_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__8.6.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_3.0_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_3.0_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop_azure_abfs--hadoop-tools--hadoop-azure--lib-spark_3.0_2.12_deploy.jar_shaded.jar:/databricks/jars/third_party--hadoop--hadoop-tools--hadoop-aws--lib-spark_3.0_2.12_deploy_shaded.jar:/databricks/jars/third_party--jackson--guava_only_shaded.jar:/databricks/jars/third_party--jackson--jackson-module-scala-shaded_2.12_deploy.jar:/databricks/jars/third_party--jackson--jsr305_only_shaded.jar:/databricks/jars/third_party--jackson--paranamer_only_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--jetty-util_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-client_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-http_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-util_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:/databricks/jars/third_party--prometheus-client--jmx_collector-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient_common-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient_dropwizard-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient_servlet-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient-spark_3.0_2.12_deploy.jar:/databricks/jars/utils--process_utils-spark_3.0_2.12_deploy.jar:/databricks/jars/workflow--workflow-spark_3.0_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--kvstore--kvstore-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--network-common--network-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--network-shuffle--network-shuffle-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--sketch--sketch-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--tags--tags-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--unsafe--unsafe-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_0--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_0--graphx--graphx-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--launcher--launcher-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackso\n*** WARNING: skipped 47910 bytes of output ***\n\n (&#39;spark.sql.parquet.cacheMetadata&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPythonVersion&#39;, &#39;2&#39;),\n (&#39;spark.hadoop.fs.adl.impl&#39;, &#39;com.databricks.adl.AdlFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNodeType&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableLocalDiskEncryption&#39;, &#39;false&#39;),\n (&#39;spark.databricks.tahoe.logStore.class&#39;,\n  &#39;com.databricks.tahoe.store.DelegatingLogStore&#39;),\n (&#39;spark.executor.memory&#39;, &#39;4g&#39;),\n (&#39;spark.databricks.cloudProvider&#39;, &#39;AWS&#39;),\n (&#39;spark.sql.hive.convertMetastoreParquet&#39;, &#39;true&#39;),\n (&#39;spark.executor.id&#39;, &#39;driver&#39;),\n (&#39;spark.databricks.passthrough.adls.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider&#39;),\n (&#39;spark.databricks.service.dbutils.server.backend&#39;,\n  &#39;com.databricks.dbconnect.SparkServerDBUtils&#39;),\n (&#39;spark.driver.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.workerEnvironmentId&#39;,\n  &#39;default-worker-env&#39;),\n (&#39;spark.rdd.compress&#39;, &#39;true&#39;),\n (&#39;spark.databricks.repl.enableClassFileCleanup&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerId&#39;,\n  &#39;26802bf79ade47f89b45b427f9b842d2&#39;),\n (&#39;spark.databricks.eventLog.dir&#39;, &#39;eventlogs&#39;),\n (&#39;spark.repl.class.outputDir&#39;,\n  &#39;/local_disk0/tmp/repl/spark-3301741754963124117-d41e921f-4cce-4460-9be3-9f685b5b05c0&#39;),\n (&#39;spark.app.name&#39;, &#39;Spark Updated Conf&#39;),\n (&#39;spark.databricks.driverNfs.pathSuffix&#39;, &#39;.ephemeral_nfs&#39;),\n (&#39;spark.sql.catalogImplementation&#39;, &#39;hive&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterCreator&#39;, &#39;Webapp&#39;),\n (&#39;spark.speculation&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.size&#39;, &#39;10485760&#39;),\n (&#39;spark.databricks.clusterUsageTags.cloudProvider&#39;, &#39;AWS&#39;),\n (&#39;spark.hadoop.databricks.dbfs.client.version&#39;, &#39;v1&#39;),\n (&#39;spark.hadoop.hive.server2.session.check.interval&#39;, &#39;60000&#39;),\n (&#39;spark.sql.hive.convertCTAS&#39;, &#39;true&#39;),\n (&#39;spark.metrics.conf&#39;, &#39;/databricks/spark/conf/metrics.properties&#39;),\n (&#39;spark.hadoop.spark.sql.parquet.output.committer.class&#39;,\n  &#39;org.apache.spark.sql.parquet.DirectParquetOutputCommitter&#39;),\n (&#39;spark.ui.port&#39;, &#39;46225&#39;),\n (&#39;spark.hadoop.fs.gs.impl&#39;,\n  &#39;shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload.default&#39;, &#39;true&#39;),\n (&#39;spark.akka.frameSize&#39;, &#39;256&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterGeneration&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.abfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.sql.streaming.stopTimeout&#39;, &#39;15s&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.password&#39;, &#39;[REDACTED]&#39;),\n (&#39;spark.speculation.multiplier&#39;, &#39;3&#39;),\n (&#39;spark.storage.blockManagerTimeoutIntervalMs&#39;, &#39;300000&#39;),\n (&#39;spark.databricks.overrideDefaultCommitProtocol&#39;,\n  &#39;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNoDriverDaemon&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvId&#39;,\n  &#39;default-worker-env&#39;),\n (&#39;spark.driver.port&#39;, &#39;43252&#39;),\n (&#39;spark.hadoop.parquet.memory.pool.ratio&#39;, &#39;0.5&#39;),\n (&#39;spark.sparkr.use.daemon&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterId&#39;, &#39;1106-024443-tangs32&#39;),\n (&#39;spark.scheduler.listenerbus.eventqueue.capacity&#39;, &#39;20000&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterScalingType&#39;, &#39;fixed_size&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterStateMessage&#39;, &#39;Starting Spark&#39;),\n (&#39;spark.sql.hive.metastore.jars&#39;, &#39;/databricks/hive/*&#39;),\n (&#39;spark.databricks.passthrough.adls.gen2.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider&#39;),\n (&#39;spark.hadoop.parquet.page.write-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.databricks.s3commit.client.sslTrustAll&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.s3a.threads.max&#39;, &#39;136&#39;),\n (&#39;spark.app.id&#39;, &#39;local-1604630829022&#39;),\n (&#39;spark.r.backendConnectionTimeout&#39;, &#39;604800&#39;),\n (&#39;spark.databricks.tahoe.logStore.gcp.class&#39;,\n  &#39;com.databricks.tahoe.store.GCPLogStore&#39;),\n (&#39;spark.serializer.objectStreamReset&#39;, &#39;100&#39;),\n (&#39;spark.sql.sources.commitProtocolClass&#39;,\n  &#39;com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol&#39;),\n (&#39;spark.hadoop.hive.server2.idle.session.timeout&#39;, &#39;900000&#39;),\n (&#39;spark.databricks.redactor&#39;,\n  &#39;com.databricks.spark.util.DatabricksSparkLogRedactorProxy&#39;),\n (&#39;spark.databricks.clusterUsageTags.autoTerminationMinutes&#39;, &#39;120&#39;),\n (&#39;spark.hadoop.fs.s3a.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableDfAcls&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.abfss.impl&#39;,\n  &#39;shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem&#39;),\n (&#39;spark.r.sql.derby.temp.dir&#39;, &#39;/tmp/RtmphY2lB4&#39;),\n (&#39;spark.shuffle.service.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkVersion&#39;, &#39;7.4.x-scala2.12&#39;),\n (&#39;spark.hadoop.parquet.page.verify-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.threshold&#39;, &#39;104857600&#39;),\n (&#39;spark.databricks.clusterUsageTags.dataPlaneRegion&#39;, &#39;us-west-2&#39;),\n (&#39;spark.rpc.message.maxSize&#39;, &#39;256&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerUserId&#39;, &#39;6947996924610561&#39;),\n (&#39;spark.logConf&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJobsAutostart&#39;, &#39;true&#39;),\n (&#39;spark.databricks.driverNfs.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMetastoreAccessType&#39;,\n  &#39;RDS_DIRECT&#39;),\n (&#39;spark.hadoop.hive.server2.enable.doAs&#39;, &#39;false&#39;),\n (&#39;eventLog.rolloverIntervalSeconds&#39;, &#39;3600&#39;),\n (&#39;spark.shuffle.memoryFraction&#39;, &#39;0.2&#39;),\n (&#39;spark.databricks.clusterUsageTags.containerZoneId&#39;, &#39;us-west-2c&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterName&#39;, &#39;datacluster&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSpotBidPricePercent&#39;, &#39;100&#39;),\n (&#39;spark.databricks.acl.scim.client&#39;,\n  &#39;com.databricks.spark.sql.acl.client.DriverToWebappScimClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.region&#39;, &#39;us-west-2&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: [(&#39;spark.files.useFetchCache&#39;, &#39;false&#39;),\n (&#39;spark.databricks.preemption.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.abfs.impl&#39;,\n  &#39;shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&#39;),\n (&#39;spark.driver.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.hadoop.fs.adl.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.max&#39;, &#39;10&#39;),\n (&#39;spark.hadoop.fs.s3a.connection.maximum&#39;, &#39;200&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload.active.blocks&#39;, &#39;32&#39;),\n (&#39;spark.shuffle.reduceLocality.enabled&#39;, &#39;false&#39;),\n (&#39;spark.sql.streaming.checkpointFileManagerClass&#39;,\n  &#39;com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager&#39;),\n (&#39;spark.databricks.service.dbutils.repl.backend&#39;,\n  &#39;com.databricks.dbconnect.ReplDBUtils&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverNodeType&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.hadoop.spark.sql.sources.outputCommitterClass&#39;,\n  &#39;com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter&#39;),\n (&#39;spark.hadoop.fs.AbstractFileSystem.gs.impl&#39;,\n  &#39;shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceBootstrapType&#39;, &#39;ssh&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.allowBatching&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterSource&#39;, &#39;UI&#39;),\n (&#39;spark.hadoop.hive.server2.transport.mode&#39;, &#39;http&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.driverNodeTypeId&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.sql.parquet.compression.codec&#39;, &#39;snappy&#39;),\n (&#39;spark.databricks.clusterUsageTags.hailEnabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.containerType&#39;, &#39;LXC&#39;),\n (&#39;spark.eventLog.enabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.isIMv2Enabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories&#39;,\n  &#39;false&#39;),\n (&#39;spark.hadoop.fs.wasb.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstancePrivateIp&#39;,\n  &#39;10.172.253.76&#39;),\n (&#39;spark.executor.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerOrgId&#39;, &#39;1513371373162116&#39;),\n (&#39;spark.databricks.workerNodeTypeId&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.hadoop.mapred.output.committer.class&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DirectOutputCommitter&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.port&#39;, &#39;10000&#39;),\n (&#39;spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version&#39;, &#39;2&#39;),\n (&#39;spark.sql.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeSize&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.spark.driverproxy.customHeadersToProperties&#39;,\n  &#39;X-Databricks-SqlGateway-SessionId:X-Databricks-SqlGateway-SessionId,X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name&#39;),\n (&#39;spark.home&#39;, &#39;/databricks/spark&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterTargetWorkers&#39;, &#39;0&#39;),\n (&#39;spark.sql.warehouse.dir&#39;, &#39;/user/hive/warehouse&#39;),\n (&#39;spark.hadoop.hive.server2.idle.operation.timeout&#39;, &#39;7200000&#39;),\n (&#39;spark.task.reaper.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.passthrough.s3a.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider&#39;),\n (&#39;spark.storage.memoryFraction&#39;, &#39;0.5&#39;),\n (&#39;spark.databricks.session.share&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAllTags&#39;,\n  &#39;[{&#34;key&#34;:&#34;Name&#34;,&#34;value&#34;:&#34;ce4-worker&#34;}]&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerPrivateIp&#39;,\n  &#39;10.172.228.93&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterResourceClass&#39;, &#39;default&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterFirstOnDemand&#39;, &#39;0&#39;),\n (&#39;spark.driver.maxResultSize&#39;, &#39;4g&#39;),\n (&#39;spark.databricks.delta.multiClusterWrites.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSku&#39;, &#39;STANDARD_SKU&#39;),\n (&#39;spark.worker.cleanup.enabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.gs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.executor.extraJavaOptions&#39;,\n  &#39;-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1&#39;),\n (&#39;spark.databricks.workspace.matplotlibInline.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableCredentialPassthrough&#39;, &#39;false&#39;),\n (&#39;spark.executor.cores&#39;, &#39;4&#39;),\n (&#39;spark.cores.max&#39;, &#39;4&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType&#39;,\n  &#39;ebs_volume_type: GENERAL_PURPOSE_SSD\\n&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJdbcAutoStart&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstanceId&#39;, &#39;i-0418ea1baca5ca6a7&#39;),\n (&#39;spark.hadoop.fs.wasb.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeType&#39;,\n  &#39;GENERAL_PURPOSE_SSD&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDestination&#39;, &#39;&#39;),\n (&#39;spark.cleaner.referenceTracking.blocking&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.parquet.page.size.check.estimate&#39;, &#39;false&#39;),\n (&#39;spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory&#39;),\n (&#39;spark.databricks.delta.preview.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.isSingleUserCluster&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverPublicDns&#39;,\n  &#39;ec2-54-189-89-160.us-west-2.compute.amazonaws.com&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterState&#39;, &#39;Pending&#39;),\n (&#39;spark.databricks.tahoe.logStore.azure.class&#39;,\n  &#39;com.databricks.tahoe.store.AzureLogStore&#39;),\n (&#39;spark.hadoop.fs.azure.skip.metrics&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.spark.thriftserver.sqlGatewayCloseSessionHeaderName&#39;,\n  &#39;X-Databricks-SqlGateway-CloseSession&#39;),\n (&#39;spark.hadoop.fs.s3.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.master&#39;, &#39;local[8]&#39;),\n (&#39;spark.scheduler.mode&#39;, &#39;FAIR&#39;),\n (&#39;spark.repl.class.uri&#39;, &#39;spark://10.172.228.93:43252/classes&#39;),\n (&#39;spark.databricks.delta.logStore.crossCloud.fatal&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterWorkers&#39;, &#39;0&#39;),\n (&#39;spark.files.fetchFailure.unRegisterOutputOnHost&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableSqlAclsOnly&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeCount&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNumSshKeys&#39;, &#39;0&#39;),\n (&#39;spark.databricks.tahoe.logStore.aws.class&#39;,\n  &#39;com.databricks.tahoe.store.S3LockBasedLogStore&#39;),\n (&#39;spark.driver.memory&#39;, &#39;2g&#39;),\n (&#39;spark.speculation.quantile&#39;, &#39;0.9&#39;),\n (&#39;spark.shuffle.manager&#39;, &#39;SORT&#39;),\n (&#39;spark.files.overwrite&#39;, &#39;true&#39;),\n (&#39;spark.sql.hive.metastore.sharedPrefixes&#39;,\n  &#39;org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks&#39;),\n (&#39;spark.databricks.io.directoryCommit.enableLogicalDelete&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.spark.thriftserver.sqlGatewaySessionIdHeaderName&#39;,\n  &#39;X-Databricks-SqlGateway-SessionId&#39;),\n (&#39;spark.task.reaper.killTimeout&#39;, &#39;60s&#39;),\n (&#39;spark.r.numRBackendThreads&#39;, &#39;1&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.min&#39;, &#39;10&#39;),\n (&#39;spark.hadoop.hive.server2.use.SSL&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.workspace.multipleResults.enabled&#39;, &#39;true&#39;),\n (&#39;spark.sql.hive.metastore.version&#39;, &#39;0.13.0&#39;),\n (&#39;spark.shuffle.service.port&#39;, &#39;4048&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType&#39;, &#39;default&#39;),\n (&#39;spark.databricks.acl.client&#39;,\n  &#39;com.databricks.spark.sql.acl.client.SparkSqlAclClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAvailability&#39;, &#39;ON_DEMAND&#39;),\n (&#39;spark.hadoop.hive.warehouse.subdir.inherit.perms&#39;, &#39;false&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.closeFileAfterWrite&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb&#39;, &#39;0&#39;),\n (&#39;spark.driver.host&#39;, &#39;10.172.228.93&#39;),\n (&#39;spark.databricks.sparkContextId&#39;, &#39;3301741754963124117&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.path&#39;,\n  &#39;/databricks/keys/jetty-ssl-driver-keystore.jks&#39;),\n (&#39;spark.databricks.credential.redactor&#39;,\n  &#39;com.databricks.logging.secrets.CredentialRedactorProxyImpl&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPinned&#39;, &#39;false&#39;),\n (&#39;spark.databricks.acl.provider&#39;,\n  &#39;com.databricks.sql.acl.ReflectionBackedAclProvider&#39;),\n (&#39;spark.hadoop.fs.s3n.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.extraListeners&#39;,\n  &#39;com.databricks.backend.daemon.driver.DBCEventLoggingListener&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableElasticDisk&#39;, &#39;false&#39;),\n (&#39;spark.executor.extraClassPath&#39;,\n  &#39;/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/api-base--api-base_java-spark_3.0_2.12_deploy.jar:/databricks/jars/api-base--api-base-spark_3.0_2.12_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_3.0_2.12_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_3.0_2.12_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_3.0_2.12_deploy.jar:/databricks/jars/common--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/common--common-spark_3.0_2.12_deploy.jar:/databricks/jars/common--credentials--credentials-spark_3.0_2.12_deploy.jar:/databricks/jars/common--crypto-providers--amazon-corretto-crypto-provider--libamazon-corretto-crypto-provider.jar:/databricks/jars/common--hadoop--hadoop-spark_3.0_2.12_deploy.jar:/databricks/jars/common--java-flight-recorder--java-flight-recorder-spark_3.0_2.12_deploy.jar:/databricks/jars/common--jetty--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/common--lazy--lazy-spark_3.0_2.12_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--path--path-spark_3.0_2.12_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_3.0_2.12_deploy.jar:/databricks/jars/common--reflection--reflection-spark_3.0_2.12_deploy.jar:/databricks/jars/common--storage--storage-spark_3.0_2.12_deploy.jar:/databricks/jars/common--tracing--tracing-spark_3.0_2.12_deploy.jar:/databricks/jars/common--util--locks-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_3.0_2.12_deploy.jar:/databricks/jars/dbfs--exceptions--exceptions-spark_3.0_2.12_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_3.0_2.12_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_3.0_2.12_deploy.jar:/databricks/jars/extern--extern-spark_3.0_2.12_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/jsonutil--jsonutil-spark_3.0_2.12_deploy.jar:/databricks/jars/libraries--api--managedLibraries--managedLibraries-spark_3.0_2.12_deploy.jar:/databricks/jars/libraries--api--typemappers--typemappers-spark_3.0_2.12_deploy.jar:/databricks/jars/libraries--libraries-spark_3.0_2.12_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_3.0_2.12_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_3.0_2.12_deploy.jar:/databricks/jars/maven-trees--amazon-corretto-crypto-provider--software.amazon.cryptools--AmazonCorrettoCryptoProvider-linux-x86_64--software.amazon.cryptools__AmazonCorrettoCryptoProvider-linux-x86_64__1.4.0.jar:/databricks/jars/s3commit--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/s3commit--common--common-spark_3.0_2.12_deploy.jar:/databricks/jars/s3--s3-spark_3.0_2.12_deploy.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/secret-manager--api--api-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--command--api--api-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--command--command-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--common--spark-common-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--common-utils--utils-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--display--display-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--driver-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--events-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--sql-extension--sql-extension-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--versions--3.0--shim_2.12_deploy.jar:/databricks/jars/spark--versions--3.0--spark_2.12_deploy.jar:/databricks/jars/sqlgateway--common--endpoint_id-spark_3.0_2.12_deploy.jar:/databricks/jars/sqlgateway--history--api--api-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-annotations_com.fasterxml.jackson.core__jackson-annotations__2.11.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-core_com.fasterxml.jackson.core__jackson-core__2.11.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-databind_com.fasterxml.jackson.core__jackson-databind__2.11.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.android_annotations_com.google.android__annotations__4.1.1.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.api.grpc_proto-google-common-protos_com.google.api.grpc__proto-google-common-protos__1.17.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.findbugs_jsr305_com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.gson_gson_com.google.code.gson__gson__2.8.6_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.errorprone_error_prone_annotations_com.google.errorprone__error_prone_annotations__2.3.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_failureaccess_com.google.guava__failureaccess__1.0.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_guava_com.google.guava__guava__29.0-android_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_listenablefuture_com.google.guava__listenablefuture__9999.0-empty-to-avoid-conflict-with-guava_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.j2objc_j2objc-annotations_com.google.j2objc__j2objc-annotations__1.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java_com.google.protobuf__protobuf-java__3.12.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java-util_com.google.protobuf__protobuf-java-util__3.12.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-brave_com.linecorp.armeria__armeria-brave__1.0.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria_com.linecorp.armeria__armeria__1.0.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc_com.linecorp.armeria__armeria-grpc__1.0.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc-protocol_com.linecorp.armeria__armeria-grpc-protocol__1.0.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-api_io.grpc__grpc-api__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-context_io.grpc__grpc-context__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-core_io.grpc__grpc-core__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf_io.grpc__grpc-protobuf__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf-lite_io.grpc__grpc-protobuf-lite__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-services_io.grpc__grpc-services__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-stub_io.grpc__grpc-stub__1.31.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.micrometer_micrometer-core_io.micrometer__micrometer-core__1.5.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-buffer_io.netty__netty-buffer__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-dns_io.netty__netty-codec-dns__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-haproxy_io.netty__netty-codec-haproxy__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http2_io.netty__netty-codec-http2__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http_io.netty__netty-codec-http__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec_io.netty__netty-codec__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-socks_io.netty__netty-codec-socks__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-common_io.netty__netty-common__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler_io.netty__netty-handler__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler-proxy_io.netty__netty-handler-proxy__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver-dns_io.netty__netty-resolver-dns__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver_io.netty__netty-resolver__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-tcnative-boringssl-static_io.netty__netty-tcnative-boringssl-static__2.0.31.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport_io.netty__netty-transport__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-epoll-linux-x86_64_io.netty__netty-transport-native-epoll-linux-x86_64__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common_io.netty__netty-transport-native-unix-common__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common-linux-x86_64_io.netty__netty-transport-native-unix-common-linux-x86_64__4.1.51.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.perfmark_perfmark-api_io.perfmark__perfmark-api__0.19.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.brave_brave-instrumentation-http_io.zipkin.brave__brave-instrumentation-http__5.12.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.brave_brave_io.zipkin.brave__brave__5.12.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.reporter2_zipkin-reporter-brave_io.zipkin.reporter2__zipkin-reporter-brave__2.15.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.reporter2_zipkin-reporter_io.zipkin.reporter2__zipkin-reporter__2.15.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.zipkin2_zipkin_io.zipkin.zipkin2__zipkin__2.21.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_jakarta.annotation_jakarta.annotation-api_jakarta.annotation__jakarta.annotation-api__1.3.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_liball_deps_2.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_net.bytebuddy_byte-buddy_net.bytebuddy__byte-buddy__1.10.9_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.checkerframework_checker-compat-qual_org.checkerframework__checker-compat-qual__2.5.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.codehaus.mojo_animal-sniffer-annotations_org.codehaus.mojo__animal-sniffer-annotations__1.18_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.curioswitch.curiostack_protobuf-jackson_org.curioswitch.curiostack__protobuf-jackson__1.1.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.hdrhistogram_HdrHistogram_org.hdrhistogram__HdrHistogram__2.1.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.joda_joda-convert_org.joda__joda-convert__2.2.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.latencyutils_LatencyUtils_org.latencyutils__LatencyUtils__2.0.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.reactivestreams_reactive-streams_org.reactivestreams__reactive-streams__1.0.3_shaded.jar:/databricks/jars/third_party--armeria--service_discovery-resources.jar:/databricks/jars/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-client-runtime__1.7.8_container_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-storage__8.6.4_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.rest__client-runtime__1.7.8_container_shaded.jar:/databricks/jars/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:/databricks/jars/third_party--datalake--datalake-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--dropwizard-metrics-log4j-v3.2.6--metrics-log4j-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--animal-sniffer-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--annotations_shaded.jar:/databricks/jars/third_party--gcs-private--api-common_shaded.jar:/databricks/jars/third_party--gcs-private--auto-value-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--checker-compat-qual_shaded.jar:/databricks/jars/third_party--gcs-private--checker-qual_shaded.jar:/databricks/jars/third_party--gcs-private--commons-codec_shaded.jar:/databricks/jars/third_party--gcs-private--commons-lang3_shaded.jar:/databricks/jars/third_party--gcs-private--commons-logging_shaded.jar:/databricks/jars/third_party--gcs-private--conscrypt-openjdk-uber_shaded.jar:/databricks/jars/third_party--gcs-private--error_prone_annotations_shaded.jar:/databricks/jars/third_party--gcs-private--failureaccess_shaded.jar:/databricks/jars/third_party--gcs-private--flogger_shaded.jar:/databricks/jars/third_party--gcs-private--flogger-slf4j-backend_shaded.jar:/databricks/jars/third_party--gcs-private--flogger-system-backend_shaded.jar:/databricks/jars/third_party--gcs-private--gcs-connector_shaded.jar:/databricks/jars/third_party--gcs-private--gcsio_shaded.jar:/databricks/jars/third_party--gcs-private--gcs-shaded-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--google-api-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client-java6_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-services-iamcredentials_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-services-storage_shaded.jar:/databricks/jars/third_party--gcs-private--google-auth-library-credentials_shaded.jar:/databricks/jars/third_party--gcs-private--google-auth-library-oauth2-http_shaded.jar:/databricks/jars/third_party--gcs-private--google-extensions_shaded.jar:/databricks/jars/third_party--gcs-private--google-http-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs-private--google-http-client_shaded.jar:/databricks/jars/third_party--gcs-private--google-oauth-client-java6_shaded.jar:/databricks/jars/third_party--gcs-private--google-oauth-client_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-alts_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-api_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-auth_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-context_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-core_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-grpclb_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-netty-shaded_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-protobuf-lite_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-protobuf_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-stub_shaded.jar:/databricks/jars/third_party--gcs-private--gson_shaded.jar:/databricks/jars/third_party--gcs-private--guava_shaded.jar:/databricks/jars/third_party--gcs-private--httpclient_shaded.jar:/databricks/jars/third_party--gcs-private--httpcore_shaded.jar:/databricks/jars/third_party--gcs-private--j2objc-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--jackson-core_shaded.jar:/databricks/jars/third_party--gcs-private--javax.annotation-api_shaded.jar:/databricks/jars/third_party--gcs-private--jsr305_shaded.jar:/databricks/jars/third_party--gcs-private--listenablefuture_shaded.jar:/databricks/jars/third_party--gcs-private--opencensus-api_shaded.jar:/databricks/jars/third_party--gcs-private--opencensus-contrib-http-util_shaded.jar:/databricks/jars/third_party--gcs-private--perfmark-api_shaded.jar:/databricks/jars/third_party--gcs-private--protobuf-java_shaded.jar:/databricks/jars/third_party--gcs-private--protobuf-java-util_shaded.jar:/databricks/jars/third_party--gcs-private--proto-google-common-protos_shaded.jar:/databricks/jars/third_party--gcs-private--proto-google-iam-v1_shaded.jar:/databricks/jars/third_party--gcs-private--util-hadoop_shaded.jar:/databricks/jars/third_party--gcs-private--util_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__8.6.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_3.0_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_3.0_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop_azure_abfs--hadoop-tools--hadoop-azure--lib-spark_3.0_2.12_deploy.jar_shaded.jar:/databricks/jars/third_party--hadoop--hadoop-tools--hadoop-aws--lib-spark_3.0_2.12_deploy_shaded.jar:/databricks/jars/third_party--jackson--guava_only_shaded.jar:/databricks/jars/third_party--jackson--jackson-module-scala-shaded_2.12_deploy.jar:/databricks/jars/third_party--jackson--jsr305_only_shaded.jar:/databricks/jars/third_party--jackson--paranamer_only_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--jetty-util_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-client_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-http_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-util_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:/databricks/jars/third_party--prometheus-client--jmx_collector-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient_common-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient_dropwizard-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient_servlet-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient-spark_3.0_2.12_deploy.jar:/databricks/jars/utils--process_utils-spark_3.0_2.12_deploy.jar:/databricks/jars/workflow--workflow-spark_3.0_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--kvstore--kvstore-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--network-common--network-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--network-shuffle--network-shuffle-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--sketch--sketch-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--tags--tags-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--unsafe--unsafe-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_0--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_0--graphx--graphx-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--launcher--launcher-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackso\n*** WARNING: skipped 47910 bytes of output ***\n\n (&#39;spark.sql.parquet.cacheMetadata&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPythonVersion&#39;, &#39;2&#39;),\n (&#39;spark.hadoop.fs.adl.impl&#39;, &#39;com.databricks.adl.AdlFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNodeType&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableLocalDiskEncryption&#39;, &#39;false&#39;),\n (&#39;spark.databricks.tahoe.logStore.class&#39;,\n  &#39;com.databricks.tahoe.store.DelegatingLogStore&#39;),\n (&#39;spark.executor.memory&#39;, &#39;4g&#39;),\n (&#39;spark.databricks.cloudProvider&#39;, &#39;AWS&#39;),\n (&#39;spark.sql.hive.convertMetastoreParquet&#39;, &#39;true&#39;),\n (&#39;spark.executor.id&#39;, &#39;driver&#39;),\n (&#39;spark.databricks.passthrough.adls.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider&#39;),\n (&#39;spark.databricks.service.dbutils.server.backend&#39;,\n  &#39;com.databricks.dbconnect.SparkServerDBUtils&#39;),\n (&#39;spark.driver.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.workerEnvironmentId&#39;,\n  &#39;default-worker-env&#39;),\n (&#39;spark.rdd.compress&#39;, &#39;true&#39;),\n (&#39;spark.databricks.repl.enableClassFileCleanup&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerId&#39;,\n  &#39;26802bf79ade47f89b45b427f9b842d2&#39;),\n (&#39;spark.databricks.eventLog.dir&#39;, &#39;eventlogs&#39;),\n (&#39;spark.repl.class.outputDir&#39;,\n  &#39;/local_disk0/tmp/repl/spark-3301741754963124117-d41e921f-4cce-4460-9be3-9f685b5b05c0&#39;),\n (&#39;spark.app.name&#39;, &#39;Spark Updated Conf&#39;),\n (&#39;spark.databricks.driverNfs.pathSuffix&#39;, &#39;.ephemeral_nfs&#39;),\n (&#39;spark.sql.catalogImplementation&#39;, &#39;hive&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterCreator&#39;, &#39;Webapp&#39;),\n (&#39;spark.speculation&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.size&#39;, &#39;10485760&#39;),\n (&#39;spark.databricks.clusterUsageTags.cloudProvider&#39;, &#39;AWS&#39;),\n (&#39;spark.hadoop.databricks.dbfs.client.version&#39;, &#39;v1&#39;),\n (&#39;spark.hadoop.hive.server2.session.check.interval&#39;, &#39;60000&#39;),\n (&#39;spark.sql.hive.convertCTAS&#39;, &#39;true&#39;),\n (&#39;spark.metrics.conf&#39;, &#39;/databricks/spark/conf/metrics.properties&#39;),\n (&#39;spark.hadoop.spark.sql.parquet.output.committer.class&#39;,\n  &#39;org.apache.spark.sql.parquet.DirectParquetOutputCommitter&#39;),\n (&#39;spark.ui.port&#39;, &#39;46225&#39;),\n (&#39;spark.hadoop.fs.gs.impl&#39;,\n  &#39;shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload.default&#39;, &#39;true&#39;),\n (&#39;spark.akka.frameSize&#39;, &#39;256&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterGeneration&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.abfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.sql.streaming.stopTimeout&#39;, &#39;15s&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.password&#39;, &#39;[REDACTED]&#39;),\n (&#39;spark.speculation.multiplier&#39;, &#39;3&#39;),\n (&#39;spark.storage.blockManagerTimeoutIntervalMs&#39;, &#39;300000&#39;),\n (&#39;spark.databricks.overrideDefaultCommitProtocol&#39;,\n  &#39;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNoDriverDaemon&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvId&#39;,\n  &#39;default-worker-env&#39;),\n (&#39;spark.driver.port&#39;, &#39;43252&#39;),\n (&#39;spark.hadoop.parquet.memory.pool.ratio&#39;, &#39;0.5&#39;),\n (&#39;spark.sparkr.use.daemon&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterId&#39;, &#39;1106-024443-tangs32&#39;),\n (&#39;spark.scheduler.listenerbus.eventqueue.capacity&#39;, &#39;20000&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterScalingType&#39;, &#39;fixed_size&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterStateMessage&#39;, &#39;Starting Spark&#39;),\n (&#39;spark.sql.hive.metastore.jars&#39;, &#39;/databricks/hive/*&#39;),\n (&#39;spark.databricks.passthrough.adls.gen2.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider&#39;),\n (&#39;spark.hadoop.parquet.page.write-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.databricks.s3commit.client.sslTrustAll&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.s3a.threads.max&#39;, &#39;136&#39;),\n (&#39;spark.app.id&#39;, &#39;local-1604630829022&#39;),\n (&#39;spark.r.backendConnectionTimeout&#39;, &#39;604800&#39;),\n (&#39;spark.databricks.tahoe.logStore.gcp.class&#39;,\n  &#39;com.databricks.tahoe.store.GCPLogStore&#39;),\n (&#39;spark.serializer.objectStreamReset&#39;, &#39;100&#39;),\n (&#39;spark.sql.sources.commitProtocolClass&#39;,\n  &#39;com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol&#39;),\n (&#39;spark.hadoop.hive.server2.idle.session.timeout&#39;, &#39;900000&#39;),\n (&#39;spark.databricks.redactor&#39;,\n  &#39;com.databricks.spark.util.DatabricksSparkLogRedactorProxy&#39;),\n (&#39;spark.databricks.clusterUsageTags.autoTerminationMinutes&#39;, &#39;120&#39;),\n (&#39;spark.hadoop.fs.s3a.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableDfAcls&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.abfss.impl&#39;,\n  &#39;shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem&#39;),\n (&#39;spark.r.sql.derby.temp.dir&#39;, &#39;/tmp/RtmphY2lB4&#39;),\n (&#39;spark.shuffle.service.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkVersion&#39;, &#39;7.4.x-scala2.12&#39;),\n (&#39;spark.hadoop.parquet.page.verify-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.threshold&#39;, &#39;104857600&#39;),\n (&#39;spark.databricks.clusterUsageTags.dataPlaneRegion&#39;, &#39;us-west-2&#39;),\n (&#39;spark.rpc.message.maxSize&#39;, &#39;256&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerUserId&#39;, &#39;6947996924610561&#39;),\n (&#39;spark.logConf&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJobsAutostart&#39;, &#39;true&#39;),\n (&#39;spark.databricks.driverNfs.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMetastoreAccessType&#39;,\n  &#39;RDS_DIRECT&#39;),\n (&#39;spark.hadoop.hive.server2.enable.doAs&#39;, &#39;false&#39;),\n (&#39;eventLog.rolloverIntervalSeconds&#39;, &#39;3600&#39;),\n (&#39;spark.shuffle.memoryFraction&#39;, &#39;0.2&#39;),\n (&#39;spark.databricks.clusterUsageTags.containerZoneId&#39;, &#39;us-west-2c&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterName&#39;, &#39;datacluster&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSpotBidPricePercent&#39;, &#39;100&#39;),\n (&#39;spark.databricks.acl.scim.client&#39;,\n  &#39;com.databricks.spark.sql.acl.client.DriverToWebappScimClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.region&#39;, &#39;us-west-2&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Get all configuration \nspark.sparkContext._conf.getAll()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1afa1666-2913-4798-a582-f137fb58949f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs ls /databricks-datasets/\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d0cf917-c370-4606-817f-e0d4b0545b2b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/databricks-datasets/","databricks-datasets/",0],["dbfs:/databricks-datasets/COVID/","COVID/",0],["dbfs:/databricks-datasets/README.md","README.md",976],["dbfs:/databricks-datasets/Rdatasets/","Rdatasets/",0],["dbfs:/databricks-datasets/SPARK_README.md","SPARK_README.md",3359],["dbfs:/databricks-datasets/adult/","adult/",0],["dbfs:/databricks-datasets/airlines/","airlines/",0],["dbfs:/databricks-datasets/amazon/","amazon/",0],["dbfs:/databricks-datasets/asa/","asa/",0],["dbfs:/databricks-datasets/atlas_higgs/","atlas_higgs/",0],["dbfs:/databricks-datasets/bikeSharing/","bikeSharing/",0],["dbfs:/databricks-datasets/cctvVideos/","cctvVideos/",0],["dbfs:/databricks-datasets/credit-card-fraud/","credit-card-fraud/",0],["dbfs:/databricks-datasets/cs100/","cs100/",0],["dbfs:/databricks-datasets/cs110x/","cs110x/",0],["dbfs:/databricks-datasets/cs190/","cs190/",0],["dbfs:/databricks-datasets/data.gov/","data.gov/",0],["dbfs:/databricks-datasets/definitive-guide/","definitive-guide/",0],["dbfs:/databricks-datasets/flights/","flights/",0],["dbfs:/databricks-datasets/flower_photos/","flower_photos/",0],["dbfs:/databricks-datasets/flowers/","flowers/",0],["dbfs:/databricks-datasets/genomics/","genomics/",0],["dbfs:/databricks-datasets/hail/","hail/",0],["dbfs:/databricks-datasets/iot/","iot/",0],["dbfs:/databricks-datasets/iot-stream/","iot-stream/",0],["dbfs:/databricks-datasets/learning-spark/","learning-spark/",0],["dbfs:/databricks-datasets/learning-spark-v2/","learning-spark-v2/",0],["dbfs:/databricks-datasets/lending-club-loan-stats/","lending-club-loan-stats/",0],["dbfs:/databricks-datasets/med-images/","med-images/",0],["dbfs:/databricks-datasets/mnist-digits/","mnist-digits/",0],["dbfs:/databricks-datasets/news20.binary/","news20.binary/",0],["dbfs:/databricks-datasets/nyctaxi/","nyctaxi/",0],["dbfs:/databricks-datasets/online_retail/","online_retail/",0],["dbfs:/databricks-datasets/overlap-join/","overlap-join/",0],["dbfs:/databricks-datasets/power-plant/","power-plant/",0],["dbfs:/databricks-datasets/retail-org/","retail-org/",0],["dbfs:/databricks-datasets/rwe/","rwe/",0],["dbfs:/databricks-datasets/sai-summit-2019-sf/","sai-summit-2019-sf/",0],["dbfs:/databricks-datasets/sample_logs/","sample_logs/",0],["dbfs:/databricks-datasets/samples/","samples/",0],["dbfs:/databricks-datasets/sfo_customer_survey/","sfo_customer_survey/",0],["dbfs:/databricks-datasets/sms_spam_collection/","sms_spam_collection/",0],["dbfs:/databricks-datasets/songs/","songs/",0],["dbfs:/databricks-datasets/structured-streaming/","structured-streaming/",0],["dbfs:/databricks-datasets/timeseries/","timeseries/",0],["dbfs:/databricks-datasets/tpch/","tpch/",0],["dbfs:/databricks-datasets/weather/","weather/",0],["dbfs:/databricks-datasets/wiki/","wiki/",0],["dbfs:/databricks-datasets/wikipedia-datasets/","wikipedia-datasets/",0],["dbfs:/databricks-datasets/wine-quality/","wine-quality/",0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/</td><td>databricks-datasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/retail-org/</td><td>retail-org/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd_file = sc.textFile(\"dbfs:/databricks-datasets/SPARK_README.md\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f69f50e2-a25e-4c81-b6f2-8b52ce8f53d5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd_file.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cff39fc6-2e55-4b87-b7e5-6d22a350d10c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[10]: [&#39;# Apache Spark&#39;,\n &#39;&#39;,\n &#39;Spark is a fast and general cluster computing system for Big Data. It provides&#39;,\n &#39;high-level APIs in Scala, Java, Python, and R, and an optimized engine that&#39;,\n &#39;supports general computation graphs for data analysis. It also supports a&#39;,\n &#39;rich set of higher-level tools including Spark SQL for SQL and DataFrames,&#39;,\n &#39;MLlib for machine learning, GraphX for graph processing,&#39;,\n &#39;and Spark Streaming for stream processing.&#39;,\n &#39;&#39;,\n &#39;&lt;http://spark.apache.org/&gt;&#39;,\n &#39;&#39;,\n &#39;&#39;,\n &#39;## Online Documentation&#39;,\n &#39;&#39;,\n &#39;You can find the latest Spark documentation, including a programming&#39;,\n &#39;guide, on the [project web page](http://spark.apache.org/documentation.html)&#39;,\n &#39;and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).&#39;,\n &#39;This README file only contains basic setup instructions.&#39;,\n &#39;&#39;,\n &#39;## Building Spark&#39;,\n &#39;&#39;,\n &#39;Spark is built using [Apache Maven](http://maven.apache.org/).&#39;,\n &#39;To build Spark and its example programs, run:&#39;,\n &#39;&#39;,\n &#39;    build/mvn -DskipTests clean package&#39;,\n &#39;&#39;,\n &#39;(You do not need to do this if you downloaded a pre-built package.)&#39;,\n &#39;More detailed documentation is available from the project site, at&#39;,\n &#39;[&#34;Building Spark&#34;](http://spark.apache.org/docs/latest/building-spark.html).&#39;,\n &#39;&#39;,\n &#39;## Interactive Scala Shell&#39;,\n &#39;&#39;,\n &#39;The easiest way to start using Spark is through the Scala shell:&#39;,\n &#39;&#39;,\n &#39;    ./bin/spark-shell&#39;,\n &#39;&#39;,\n &#39;Try the following command, which should return 1000:&#39;,\n &#39;&#39;,\n &#39;    scala&gt; sc.parallelize(1 to 1000).count()&#39;,\n &#39;&#39;,\n &#39;## Interactive Python Shell&#39;,\n &#39;&#39;,\n &#39;Alternatively, if you prefer Python, you can use the Python shell:&#39;,\n &#39;&#39;,\n &#39;    ./bin/pyspark&#39;,\n &#39;&#39;,\n &#39;And run the following command, which should also return 1000:&#39;,\n &#39;&#39;,\n &#39;    &gt;&gt;&gt; sc.parallelize(range(1000)).count()&#39;,\n &#39;&#39;,\n &#39;## Example Programs&#39;,\n &#39;&#39;,\n &#39;Spark also comes with several sample programs in the `examples` directory.&#39;,\n &#39;To run one of them, use `./bin/run-example &lt;class&gt; [params]`. For example:&#39;,\n &#39;&#39;,\n &#39;    ./bin/run-example SparkPi&#39;,\n &#39;&#39;,\n &#39;will run the Pi example locally.&#39;,\n &#39;&#39;,\n &#39;You can set the MASTER environment variable when running examples to submit&#39;,\n &#39;examples to a cluster. This can be a mesos:// or spark:// URL,&#39;,\n &#39;&#34;yarn&#34; to run on YARN, and &#34;local&#34; to run&#39;,\n &#39;locally with one thread, or &#34;local[N]&#34; to run locally with N threads. You&#39;,\n &#39;can also use an abbreviated class name if the class is in the `examples`&#39;,\n &#39;package. For instance:&#39;,\n &#39;&#39;,\n &#39;    MASTER=spark://host:7077 ./bin/run-example SparkPi&#39;,\n &#39;&#39;,\n &#39;Many of the example programs print usage help if no params are given.&#39;,\n &#39;&#39;,\n &#39;## Running Tests&#39;,\n &#39;&#39;,\n &#39;Testing first requires [building Spark](#building-spark). Once Spark is built, tests&#39;,\n &#39;can be run using:&#39;,\n &#39;&#39;,\n &#39;    ./dev/run-tests&#39;,\n &#39;&#39;,\n &#39;Please see the guidance on how to&#39;,\n &#39;[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).&#39;,\n &#39;&#39;,\n &#39;## A Note About Hadoop Versions&#39;,\n &#39;&#39;,\n &#39;Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported&#39;,\n &#39;storage systems. Because the protocols have changed in different versions of&#39;,\n &#39;Hadoop, you must build Spark against the same version that your cluster runs.&#39;,\n &#39;&#39;,\n &#39;Please refer to the build documentation at&#39;,\n &#39;[&#34;Specifying the Hadoop Version&#34;](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)&#39;,\n &#39;for detailed guidance on building for a particular distribution of Hadoop, including&#39;,\n &#39;building for particular Hive and Hive Thriftserver distributions.&#39;,\n &#39;&#39;,\n &#39;## Configuration&#39;,\n &#39;&#39;,\n &#39;Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)&#39;,\n &#39;in the online documentation for an overview on how to configure Spark.&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: [&#39;# Apache Spark&#39;,\n &#39;&#39;,\n &#39;Spark is a fast and general cluster computing system for Big Data. It provides&#39;,\n &#39;high-level APIs in Scala, Java, Python, and R, and an optimized engine that&#39;,\n &#39;supports general computation graphs for data analysis. It also supports a&#39;,\n &#39;rich set of higher-level tools including Spark SQL for SQL and DataFrames,&#39;,\n &#39;MLlib for machine learning, GraphX for graph processing,&#39;,\n &#39;and Spark Streaming for stream processing.&#39;,\n &#39;&#39;,\n &#39;&lt;http://spark.apache.org/&gt;&#39;,\n &#39;&#39;,\n &#39;&#39;,\n &#39;## Online Documentation&#39;,\n &#39;&#39;,\n &#39;You can find the latest Spark documentation, including a programming&#39;,\n &#39;guide, on the [project web page](http://spark.apache.org/documentation.html)&#39;,\n &#39;and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).&#39;,\n &#39;This README file only contains basic setup instructions.&#39;,\n &#39;&#39;,\n &#39;## Building Spark&#39;,\n &#39;&#39;,\n &#39;Spark is built using [Apache Maven](http://maven.apache.org/).&#39;,\n &#39;To build Spark and its example programs, run:&#39;,\n &#39;&#39;,\n &#39;    build/mvn -DskipTests clean package&#39;,\n &#39;&#39;,\n &#39;(You do not need to do this if you downloaded a pre-built package.)&#39;,\n &#39;More detailed documentation is available from the project site, at&#39;,\n &#39;[&#34;Building Spark&#34;](http://spark.apache.org/docs/latest/building-spark.html).&#39;,\n &#39;&#39;,\n &#39;## Interactive Scala Shell&#39;,\n &#39;&#39;,\n &#39;The easiest way to start using Spark is through the Scala shell:&#39;,\n &#39;&#39;,\n &#39;    ./bin/spark-shell&#39;,\n &#39;&#39;,\n &#39;Try the following command, which should return 1000:&#39;,\n &#39;&#39;,\n &#39;    scala&gt; sc.parallelize(1 to 1000).count()&#39;,\n &#39;&#39;,\n &#39;## Interactive Python Shell&#39;,\n &#39;&#39;,\n &#39;Alternatively, if you prefer Python, you can use the Python shell:&#39;,\n &#39;&#39;,\n &#39;    ./bin/pyspark&#39;,\n &#39;&#39;,\n &#39;And run the following command, which should also return 1000:&#39;,\n &#39;&#39;,\n &#39;    &gt;&gt;&gt; sc.parallelize(range(1000)).count()&#39;,\n &#39;&#39;,\n &#39;## Example Programs&#39;,\n &#39;&#39;,\n &#39;Spark also comes with several sample programs in the `examples` directory.&#39;,\n &#39;To run one of them, use `./bin/run-example &lt;class&gt; [params]`. For example:&#39;,\n &#39;&#39;,\n &#39;    ./bin/run-example SparkPi&#39;,\n &#39;&#39;,\n &#39;will run the Pi example locally.&#39;,\n &#39;&#39;,\n &#39;You can set the MASTER environment variable when running examples to submit&#39;,\n &#39;examples to a cluster. This can be a mesos:// or spark:// URL,&#39;,\n &#39;&#34;yarn&#34; to run on YARN, and &#34;local&#34; to run&#39;,\n &#39;locally with one thread, or &#34;local[N]&#34; to run locally with N threads. You&#39;,\n &#39;can also use an abbreviated class name if the class is in the `examples`&#39;,\n &#39;package. For instance:&#39;,\n &#39;&#39;,\n &#39;    MASTER=spark://host:7077 ./bin/run-example SparkPi&#39;,\n &#39;&#39;,\n &#39;Many of the example programs print usage help if no params are given.&#39;,\n &#39;&#39;,\n &#39;## Running Tests&#39;,\n &#39;&#39;,\n &#39;Testing first requires [building Spark](#building-spark). Once Spark is built, tests&#39;,\n &#39;can be run using:&#39;,\n &#39;&#39;,\n &#39;    ./dev/run-tests&#39;,\n &#39;&#39;,\n &#39;Please see the guidance on how to&#39;,\n &#39;[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).&#39;,\n &#39;&#39;,\n &#39;## A Note About Hadoop Versions&#39;,\n &#39;&#39;,\n &#39;Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported&#39;,\n &#39;storage systems. Because the protocols have changed in different versions of&#39;,\n &#39;Hadoop, you must build Spark against the same version that your cluster runs.&#39;,\n &#39;&#39;,\n &#39;Please refer to the build documentation at&#39;,\n &#39;[&#34;Specifying the Hadoop Version&#34;](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)&#39;,\n &#39;for detailed guidance on building for a particular distribution of Hadoop, including&#39;,\n &#39;building for particular Hive and Hive Thriftserver distributions.&#39;,\n &#39;&#39;,\n &#39;## Configuration&#39;,\n &#39;&#39;,\n &#39;Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)&#39;,\n &#39;in the online documentation for an overview on how to configure Spark.&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Creating RDD using sc variable using parallelize method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa61b303-395d-4184-995e-bec0144794e6"}}},{"cell_type":"code","source":["range_rdd=sc.parallelize([1,2,3,4,5,6,7,8,6,5,4,5,7,9,22,3333],4)\nrange_rdd.glom().collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e18a3b60-bea2-4077-a7ff-de0c13f951cf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[18]: [[1, 2, 3, 4], [5, 6, 7, 8], [6, 5, 4, 5], [7, 9, 22, 3333]]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: [[1, 2, 3, 4], [5, 6, 7, 8], [6, 5, 4, 5], [7, 9, 22, 3333]]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### displaying data using `collect()` action"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b503f95-5067-4570-bbfa-86d9a5908b13"}}},{"cell_type":"code","source":["range_rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8808ab4a-3e69-47f6-9eca-adefcbac681b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[31]: [1, 2, 3, 4, 5, 6, 7, 8, 6, 5, 4, 5, 7, 9, 22, 3333]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[31]: [1, 2, 3, 4, 5, 6, 7, 8, 6, 5, 4, 5, 7, 9, 22, 3333]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(sc)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d054580-f28a-49a9-9a82-06b70c541722"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on RemoteContext in module PythonShellImpl object:\n\nclass RemoteContext(pyspark.context.SparkContext)\n |  RemoteContext(master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=&lt;class &#39;pyspark.profiler.BasicProfiler&#39;&gt;)\n |  \n |  Main entry point for Spark functionality. A SparkContext represents the\n |  connection to a Spark cluster, and can be used to create :class:`RDD` and\n |  broadcast variables on that cluster.\n |  \n |  .. note:: Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n |      the active :class:`SparkContext` before creating a new one.\n |  \n |  .. note:: :class:`SparkContext` instance is not supported to share across multiple\n |      processes out of the box, and PySpark does not guarantee multi-processing execution.\n |      Use threads instead for concurrent processing purpose.\n |  \n |  Method resolution order:\n |      RemoteContext\n |      pyspark.context.SparkContext\n |      builtins.object\n |  \n |  Methods inherited from pyspark.context.SparkContext:\n |  \n |  __enter__(self)\n |      Enable &#39;with SparkContext(...) as sc: app(sc)&#39; syntax.\n |  \n |  __exit__(self, type, value, trace)\n |      Enable &#39;with SparkContext(...) as sc: app&#39; syntax.\n |      \n |      Specifically stop the context on exit of the with block.\n |  \n |  __getnewargs__(self)\n |  \n |  __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=&lt;class &#39;pyspark.profiler.BasicProfiler&#39;&gt;)\n |      Create a new SparkContext. At least the master and app name should be set,\n |      either through the named parameters here or through `conf`.\n |      \n |      :param master: Cluster URL to connect to\n |             (e.g. mesos://host:port, spark://host:port, local[4]).\n |      :param appName: A name for your job, to display on the cluster web UI.\n |      :param sparkHome: Location where Spark is installed on cluster nodes.\n |      :param pyFiles: Collection of .zip or .py files to send to the cluster\n |             and add to PYTHONPATH.  These can be paths on the local file\n |             system or HDFS, HTTP, HTTPS, or FTP URLs.\n |      :param environment: A dictionary of environment variables to set on\n |             worker nodes.\n |      :param batchSize: The number of Python objects represented as a single\n |             Java object. Set 1 to disable batching, 0 to automatically choose\n |             the batch size based on object sizes, or -1 to use an unlimited\n |             batch size\n |      :param serializer: The serializer for RDDs.\n |      :param conf: A :class:`SparkConf` object setting Spark properties.\n |      :param gateway: Use an existing gateway and JVM, otherwise a new JVM\n |             will be instantiated.\n |      :param jsc: The JavaSparkContext instance (optional).\n |      :param profiler_cls: A class of custom Profiler used to do profiling\n |             (default is pyspark.profiler.BasicProfiler).\n |      \n |      \n |      &gt;&gt;&gt; from pyspark.context import SparkContext\n |      &gt;&gt;&gt; sc = SparkContext(&#39;local&#39;, &#39;test&#39;)\n |      \n |      &gt;&gt;&gt; sc2 = SparkContext(&#39;local&#39;, &#39;test2&#39;) # doctest: +IGNORE_EXCEPTION_DETAIL\n |      Traceback (most recent call last):\n |          ...\n |      ValueError:...\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  accumulator(self, value, accum_param=None)\n |      Create an :class:`Accumulator` with the given initial value, using a given\n |      :class:`AccumulatorParam` helper object to define how to add values of the\n |      data type if provided. Default AccumulatorParams are used for integers\n |      and floating-point numbers if you do not provide one. For other types,\n |      a custom AccumulatorParam can be used.\n |  \n |  addClusterWideLibraryToPath(self, libname)\n |      # add cluster wide library to the system path. This is called by libraries installed\n |      # through addPyFile, library installation UI or command line (databricks libraries install)\n |  \n |  addFile(self, path, recursive=False)\n |      Add a file to be downloaded with this Spark job on every node.\n |      The `path` passed can be either a local file, a file in HDFS\n |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n |      FTP URI.\n |      \n |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n |      filename to find its download location.\n |      \n |      A directory can be given if the recursive option is set to True.\n |      Currently directories are only supported for Hadoop-supported filesystems.\n |      \n |      .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n |      \n |      &gt;&gt;&gt; from pyspark import SparkFiles\n |      &gt;&gt;&gt; path = os.path.join(tempdir, &#34;test.txt&#34;)\n |      &gt;&gt;&gt; with open(path, &#34;w&#34;) as testFile:\n |      ...    _ = testFile.write(&#34;100&#34;)\n |      &gt;&gt;&gt; sc.addFile(path)\n |      &gt;&gt;&gt; def func(iterator):\n |      ...    with open(SparkFiles.get(&#34;test.txt&#34;)) as testFile:\n |      ...        fileVal = int(testFile.readline())\n |      ...        return [x * fileVal for x in iterator]\n |      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n |      [100, 200, 300, 400]\n |  \n |  addIsolatedLibraryPath(self, libname)\n |      For a .egg, .zip, or .jar library, add the path to sys.path, and add it to\n |      _python_includes for passing it to executor.\n |  \n |  addPyFile(self, path)\n |      Add a .py or .zip dependency for all tasks to be executed on this\n |      SparkContext in the future.  The `path` passed can be either a local\n |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n |      HTTP, HTTPS or FTP URI.\n |      \n |      .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n |  \n |  binaryFiles(self, path, minPartitions=None)\n |      Read a directory of binary files from HDFS, a local file system\n |      (available on all nodes), or any Hadoop-supported file system URI\n |      as a byte array. Each file is read as a single record and returned\n |      in a key-value pair, where the key is the path of each file, the\n |      value is the content of each file.\n |      \n |      .. note:: Small files are preferred, large file is also allowable, but\n |          may cause bad performance.\n |  \n |  binaryRecords(self, path, recordLength)\n |      Load data from a flat binary file, assuming each record is a set of numbers\n |      with the specified numerical format (see ByteBuffer), and the number of\n |      bytes per record is constant.\n |      \n |      :param path: Directory to the input data files\n |      :param recordLength: The length at which to split the records\n |  \n |  broadcast(self, value)\n |      Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\n |      object for reading it in distributed functions. The variable will\n |      be sent to each cluster only once.\n |  \n |  cancelAllJobs(self)\n |      Cancel all jobs that have been scheduled or are running.\n |  \n |  cancelJobGroup(self, groupId)\n |      Cancel active jobs for the specified group. See :meth:`SparkContext.setJobGroup`.\n |      for more information.\n |  \n |  dump_profiles(self, path)\n |      Dump the profile stats into directory `path`\n |  \n |  emptyRDD(self)\n |      Create an RDD that has no partitions or elements.\n |  \n |  getConf(self)\n |  \n |  getLocalProperty(self, key)\n |      Get a local property set in this thread, or null if it is missing. See\n |      :meth:`setLocalProperty`.\n |  \n |  hadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n |      Read an &#39;old&#39; Hadoop InputFormat with arbitrary key and value class from HDFS,\n |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n |      The mechanism is the same as for sc.sequenceFile.\n |      \n |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n |      Configuration in Java.\n |      \n |      :param path: path to Hadoop file\n |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n |             (e.g. &#34;org.apache.hadoop.mapred.TextInputFormat&#34;)\n |      :param keyClass: fully qualified classname of key Writable class\n |             (e.g. &#34;org.apache.hadoop.io.Text&#34;)\n |      :param valueClass: fully qualified classname of value Writable class\n |             (e.g. &#34;org.apache.hadoop.io.LongWritable&#34;)\n |      :param keyConverter: (None by default)\n |      :param valueConverter: (None by default)\n |      :param conf: Hadoop configuration, passed in as a dict\n |             (None by default)\n |      :param batchSize: The number of Python objects represented as a single\n |             Java object. (default 0, choose batchSize automatically)\n |  \n |  hadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n |      Read an &#39;old&#39; Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n |      Hadoop configuration, which is passed in as a Python dict.\n |      This will be converted into a Configuration in Java.\n |      The mechanism is the same as for sc.sequenceFile.\n |      \n |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n |             (e.g. &#34;org.apache.hadoop.mapred.TextInputFormat&#34;)\n |      :param keyClass: fully qualified classname of key Writable class\n |             (e.g. &#34;org.apache.hadoop.io.Text&#34;)\n |      :param valueClass: fully qualified classname of value Writable class\n |             (e.g. &#34;org.apache.hadoop.io.LongWritable&#34;)\n |      :param keyConverter: (None by default)\n |      :param valueConverter: (None by default)\n |      :param conf: Hadoop configuration, passed in as a dict\n |             (None by default)\n |      :param batchSize: The number of Python objects represented as a single\n |             Java object. (default 0, choose batchSize automatically)\n |  \n |  init_batched_serializer(self, data, numSlices=None)\n |      Init the batched serializer with the data to parallelize into an RDD and numSlices for the\n |      batch size.\n |  \n |  newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n |      Read a &#39;new API&#39; Hadoop InputFormat with arbitrary key and value class from HDFS,\n |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n |      The mechanism is the same as for sc.sequenceFile.\n |      \n |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n |      Configuration in Java\n |      \n |      :param path: path to Hadoop file\n |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n |             (e.g. &#34;org.apache.hadoop.mapreduce.lib.input.TextInputFormat&#34;)\n |      :param keyClass: fully qualified classname of key Writable class\n |             (e.g. &#34;org.apache.hadoop.io.Text&#34;)\n |      :param valueClass: fully qualified classname of value Writable class\n |             (e.g. &#34;org.apache.hadoop.io.LongWritable&#34;)\n |      :param keyConverter: (None by default)\n |      :param valueConverter: (None by default)\n |      :param conf: Hadoop configuration, passed in as a dict\n |             (None by default)\n |      :param batchSize: The number of Python objects represented as a single\n |             Java object. (default 0, choose batchSize automatically)\n |  \n |  newAPIHadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n |      Read a &#39;new API&#39; Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n |      Hadoop configuration, which is passed in as a Python dict.\n |      This will be converted into a Configuration in Java.\n |      The mechanism is the same as for sc.sequenceFile.\n |      \n |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n |             (e.g. &#34;org.apache.hadoop.mapreduce.lib.input.TextInputFormat&#34;)\n |      :param keyClass: fully qualified classname of key Writable class\n |             (e.g. &#34;org.apache.hadoop.io.Text&#34;)\n |      :param valueClass: fully qualified classname of value Writable class\n |             (e.g. &#34;org.apache.hadoop.io.LongWritable&#34;)\n |      :param keyConverter: (None by default)\n |      :param valueConverter: (None by default)\n |      :param conf: Hadoop configuration, passed in as a dict\n |             (None by default)\n |      :param batchSize: The number of Python objects represented as a single\n |             Java object. (default 0, choose batchSize automatically)\n |  \n |  parallelize(self, c, numSlices=None)\n |      Distribute a local Python collection to form an RDD. Using xrange\n |      is recommended if the input represents a range for performance.\n |      \n |      &gt;&gt;&gt; sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n |      [[0], [2], [3], [4], [6]]\n |      &gt;&gt;&gt; sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n |      [[], [0], [], [2], [4]]\n |  \n |  pickleFile(self, name, minPartitions=None)\n |      Load an RDD previously saved using :meth:`RDD.saveAsPickleFile` method.\n |      \n |      &gt;&gt;&gt; tmpFile = NamedTemporaryFile(delete=True)\n |      &gt;&gt;&gt; tmpFile.close()\n |      &gt;&gt;&gt; sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\n |      &gt;&gt;&gt; sorted(sc.pickleFile(tmpFile.name, 3).collect())\n |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n |  \n |  range(self, start, end=None, step=1, numSlices=None)\n |      Create a new RDD of int containing elements from `start` to `end`\n |      (exclusive), increased by `step` every element. Can be called the same\n |      way as python&#39;s built-in range() function. If called with a single argument,\n |      the argument is interpreted as `end`, and `start` is set to 0.\n |      \n |      :param start: the start value\n |      :param end: the end value (exclusive)\n |      :param step: the incremental step (default: 1)\n |      :param numSlices: the number of partitions of the new RDD\n |      :return: An RDD of int\n |      \n |      &gt;&gt;&gt; sc.range(5).collect()\n |      [0, 1, 2, 3, 4]\n |      &gt;&gt;&gt; sc.range(2, 4).collect()\n |      [2, 3]\n |      &gt;&gt;&gt; sc.range(1, 7, 2).collect()\n |      [1, 3, 5]\n |  \n |  runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False)\n |      Executes the given partitionFunc on the specified set of partitions,\n |      returning the result as an array of elements.\n |      \n |      If &#39;partitions&#39; is not specified, this will run over all partitions.\n |      \n |      &gt;&gt;&gt; myRDD = sc.parallelize(range(6), 3)\n |      &gt;&gt;&gt; sc.runJob(myRDD, lambda part: [x * x for x in part])\n |      [0, 1, 4, 9, 16, 25]\n |      \n |      &gt;&gt;&gt; myRDD = sc.parallelize(range(6), 3)\n |      &gt;&gt;&gt; sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n |      [0, 1, 16, 25]\n |  \n |  sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)\n |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n |      The mechanism is as follows:\n |      \n |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n |             and value Writable classes\n |          2. Serialization is attempted via Pyrolite pickling\n |          3. If this fails, the fallback is to call &#39;toString&#39; on each key and value\n |          4. :class:`PickleSerializer` is used to deserialize pickled objects on the Python side\n |      \n |      :param path: path to sequncefile\n |      :param keyClass: fully qualified classname of key Writable class\n |             (e.g. &#34;org.apache.hadoop.io.Text&#34;)\n |      :param valueClass: fully qualified classname of value Writable class\n |             (e.g. &#34;org.apache.hadoop.io.LongWritable&#34;)\n |      :param keyConverter:\n |      :param valueConverter:\n |      :param minSplits: minimum splits in dataset\n |             (default min(2, sc.defaultParallelism))\n |      :param batchSize: The number of Python objects represented as a single\n |             Java object. (default 0, choose batchSize automatically)\n |  \n |  setCheckpointDir(self, dirName)\n |      Set the directory under which RDDs are going to be checkpointed. The\n |      directory must be an HDFS path if running on a cluster.\n |  \n |  setJobDescription(self, value)\n |      Set a human readable description of the current job.\n |      \n |      .. note:: Currently, setting a job description (set to local properties) with multiple\n |          threads does not properly work. Internally threads on PVM and JVM are not synced,\n |          and JVM thread can be reused for multiple threads on PVM, which fails to isolate\n |          local properties for each thread on PVM. To work around this, You can use\n |          :meth:`RDD.collectWithJobGroup` for now.\n |  \n |  setJobGroup(self, groupId, description, interruptOnCancel=False)\n |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n |      different value or cleared.\n |      \n |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n |      Application programmers can use this method to group all those jobs together and give a\n |      group description. Once set, the Spark web UI will associate such jobs with this group.\n |      \n |      The application can use :meth:`SparkContext.cancelJobGroup` to cancel all\n |      running jobs in this group.\n |      \n |      &gt;&gt;&gt; import threading\n |      &gt;&gt;&gt; from time import sleep\n |      &gt;&gt;&gt; result = &#34;Not Set&#34;\n |      &gt;&gt;&gt; lock = threading.Lock()\n |      &gt;&gt;&gt; def map_func(x):\n |      ...     sleep(100)\n |      ...     raise Exception(&#34;Task should have been cancelled&#34;)\n |      &gt;&gt;&gt; def start_job(x):\n |      ...     global result\n |      ...     try:\n |      ...         sc.setJobGroup(&#34;job_to_cancel&#34;, &#34;some description&#34;)\n |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n |      ...     except Exception as e:\n |      ...         result = &#34;Cancelled&#34;\n |      ...     lock.release()\n |      &gt;&gt;&gt; def stop_job():\n |      ...     sleep(5)\n |      ...     sc.cancelJobGroup(&#34;job_to_cancel&#34;)\n |      &gt;&gt;&gt; suppress = lock.acquire()\n |      &gt;&gt;&gt; suppress = threading.Thread(target=start_job, args=(10,)).start()\n |      &gt;&gt;&gt; suppress = threading.Thread(target=stop_job).start()\n |      &gt;&gt;&gt; suppress = lock.acquire()\n |      &gt;&gt;&gt; print(result)\n |      Cancelled\n |      \n |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n |      in Thread.interrupt() being called on the job&#39;s executor threads. This is useful to help\n |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n |      \n |      .. note:: Currently, setting a group ID (set to local properties) with multiple threads\n |          does not properly work. Internally threads on PVM and JVM are not synced, and JVM\n |          thread can be reused for multiple threads on PVM, which fails to isolate local\n |          properties for each thread on PVM. To work around this, You can use\n |          :meth:`RDD.collectWithJobGroup` for now.\n |  \n |  setLocalProperty(self, key, value)\n |      Set a local property that affects jobs submitted from this thread, such as the\n |      Spark fair scheduler pool.\n |      \n |      .. note:: Currently, setting a local property with multiple threads does not properly work.\n |          Internally threads on PVM and JVM are not synced, and JVM thread\n |          can be reused for multiple threads on PVM, which fails to isolate local properties\n |          for each thread on PVM. To work around this, You can use\n |          :meth:`RDD.collectWithJobGroup`.\n |  \n |  setLogLevel(self, logLevel)\n |      Control our logLevel. This overrides any user-defined log settings.\n |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n |  \n |  show_profiles(self)\n |      Print the profile stats to stdout\n |  \n |  sparkUser(self)\n |      Get SPARK_USER for user who is running SparkContext.\n |  \n |  statusTracker(self)\n |      Return :class:`StatusTracker` object\n |  \n |  stop(self)\n |      Shut down the SparkContext.\n |  \n |  textFile(self, name, minPartitions=None, use_unicode=True)\n |      Read a text file from HDFS, a local file system (available on all\n |      nodes), or any Hadoop-supported file system URI, and return it as an\n |      RDD of Strings.\n |      The text files must be encoded as UTF-8.\n |      \n |      If use_unicode is False, the strings will be kept as `str` (encoding\n |      as `utf-8`), which is faster and smaller than unicode. (Added in\n |      Spark 1.2)\n |      \n |      &gt;&gt;&gt; path = os.path.join(tempdir, &#34;sample-text.txt&#34;)\n |      &gt;&gt;&gt; with open(path, &#34;w&#34;) as testFile:\n |      ...    _ = testFile.write(&#34;Hello world!&#34;)\n |      &gt;&gt;&gt; textFile = sc.textFile(path)\n |      &gt;&gt;&gt; textFile.collect()\n |      [&#39;Hello world!&#39;]\n |  \n |  union(self, rdds)\n |      Build the union of a list of RDDs.\n |      \n |      This supports unions() of RDDs with different serialized formats,\n |      although this forces them to be reserialized using the default\n |      serializer:\n |      \n |      &gt;&gt;&gt; path = os.path.join(tempdir, &#34;union-text.txt&#34;)\n |      &gt;&gt;&gt; with open(path, &#34;w&#34;) as testFile:\n |      ...    _ = testFile.write(&#34;Hello&#34;)\n |      &gt;&gt;&gt; textFile = sc.textFile(path)\n |      &gt;&gt;&gt; textFile.collect()\n |      [&#39;Hello&#39;]\n |      &gt;&gt;&gt; parallelized = sc.parallelize([&#34;World!&#34;])\n |      &gt;&gt;&gt; sorted(sc.union([textFile, parallelized]).collect())\n |      [&#39;Hello&#39;, &#39;World!&#39;]\n |  \n |  wholeTextFiles(self, path, minPartitions=None, use_unicode=True)\n |      Read a directory of text files from HDFS, a local file system\n |      (available on all nodes), or any  Hadoop-supported file system\n |      URI. Each file is read as a single record and returned in a\n |      key-value pair, where the key is the path of each file, the\n |      value is the content of each file.\n |      The text files must be encoded as UTF-8.\n |      \n |      If use_unicode is False, the strings will be kept as `str` (encoding\n |      as `utf-8`), which is faster and smaller than unicode. (Added in\n |      Spark 1.2)\n |      \n |      For example, if you have the following files:\n |      \n |      .. code-block:: text\n |      \n |          hdfs://a-hdfs-path/part-00000\n |          hdfs://a-hdfs-path/part-00001\n |          ...\n |          hdfs://a-hdfs-path/part-nnnnn\n |      \n |      Do ``rdd = sparkContext.wholeTextFiles(&#34;hdfs://a-hdfs-path&#34;)``,\n |      then ``rdd`` contains:\n |      \n |      .. code-block:: text\n |      \n |          (a-hdfs-path/part-00000, its content)\n |          (a-hdfs-path/part-00001, its content)\n |          ...\n |          (a-hdfs-path/part-nnnnn, its content)\n |      \n |      .. note:: Small files are preferred, as each file will be loaded\n |          fully in memory.\n |      \n |      &gt;&gt;&gt; dirPath = os.path.join(tempdir, &#34;files&#34;)\n |      &gt;&gt;&gt; os.mkdir(dirPath)\n |      &gt;&gt;&gt; with open(os.path.join(dirPath, &#34;1.txt&#34;), &#34;w&#34;) as file1:\n |      ...    _ = file1.write(&#34;1&#34;)\n |      &gt;&gt;&gt; with open(os.path.join(dirPath, &#34;2.txt&#34;), &#34;w&#34;) as file2:\n |      ...    _ = file2.write(&#34;2&#34;)\n |      &gt;&gt;&gt; textFiles = sc.wholeTextFiles(dirPath)\n |      &gt;&gt;&gt; sorted(textFiles.collect())\n |      [(&#39;.../1.txt&#39;, &#39;1&#39;), (&#39;.../2.txt&#39;, &#39;2&#39;)]\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.context.SparkContext:\n |  \n |  getOrCreate(conf=None) from builtins.type\n |      Get or instantiate a SparkContext and register it as a singleton object.\n |      \n |      :param conf: SparkConf (optional)\n |  \n |  setSystemProperty(key, value) from builtins.type\n |      Set a Java system property, such as spark.executor.memory. This must\n |      must be invoked before instantiating SparkContext.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.context.SparkContext:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  applicationId\n |      A unique identifier for the Spark application.\n |      Its format depends on the scheduler implementation.\n |      \n |      * in case of local spark app something like &#39;local-1433865536131&#39;\n |      * in case of YARN something like &#39;application_1433865536131_34483&#39;\n |      \n |      &gt;&gt;&gt; sc.applicationId  # doctest: +ELLIPSIS\n |      &#39;local-...&#39;\n |  \n |  defaultMinPartitions\n |      Default min number of partitions for Hadoop RDDs when not given by user\n |  \n |  defaultParallelism\n |      Default level of parallelism to use when not given by user (e.g. for\n |      reduce tasks)\n |  \n |  resources\n |  \n |  startTime\n |      Return the epoch time when the Spark Context was started.\n |  \n |  uiWebUrl\n |      Return the URL of the SparkUI instance started by this SparkContext\n |  \n |  version\n |      The version of Spark on which this application is running.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.context.SparkContext:\n |  \n |  PACKAGE_EXTENSIONS = (&#39;.zip&#39;, &#39;.egg&#39;, &#39;.jar&#39;)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on RemoteContext in module PythonShellImpl object:\n\nclass RemoteContext(pyspark.context.SparkContext)\n  RemoteContext(master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=&lt;class &#39;pyspark.profiler.BasicProfiler&#39;&gt;)\n  \n  Main entry point for Spark functionality. A SparkContext represents the\n  connection to a Spark cluster, and can be used to create :class:`RDD` and\n  broadcast variables on that cluster.\n  \n  .. note:: Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n      the active :class:`SparkContext` before creating a new one.\n  \n  .. note:: :class:`SparkContext` instance is not supported to share across multiple\n      processes out of the box, and PySpark does not guarantee multi-processing execution.\n      Use threads instead for concurrent processing purpose.\n  \n  Method resolution order:\n      RemoteContext\n      pyspark.context.SparkContext\n      builtins.object\n  \n  Methods inherited from pyspark.context.SparkContext:\n  \n  __enter__(self)\n      Enable &#39;with SparkContext(...) as sc: app(sc)&#39; syntax.\n  \n  __exit__(self, type, value, trace)\n      Enable &#39;with SparkContext(...) as sc: app&#39; syntax.\n      \n      Specifically stop the context on exit of the with block.\n  \n  __getnewargs__(self)\n  \n  __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=&lt;class &#39;pyspark.profiler.BasicProfiler&#39;&gt;)\n      Create a new SparkContext. At least the master and app name should be set,\n      either through the named parameters here or through `conf`.\n      \n      :param master: Cluster URL to connect to\n             (e.g. mesos://host:port, spark://host:port, local[4]).\n      :param appName: A name for your job, to display on the cluster web UI.\n      :param sparkHome: Location where Spark is installed on cluster nodes.\n      :param pyFiles: Collection of .zip or .py files to send to the cluster\n             and add to PYTHONPATH.  These can be paths on the local file\n             system or HDFS, HTTP, HTTPS, or FTP URLs.\n      :param environment: A dictionary of environment variables to set on\n             worker nodes.\n      :param batchSize: The number of Python objects represented as a single\n             Java object. Set 1 to disable batching, 0 to automatically choose\n             the batch size based on object sizes, or -1 to use an unlimited\n             batch size\n      :param serializer: The serializer for RDDs.\n      :param conf: A :class:`SparkConf` object setting Spark properties.\n      :param gateway: Use an existing gateway and JVM, otherwise a new JVM\n             will be instantiated.\n      :param jsc: The JavaSparkContext instance (optional).\n      :param profiler_cls: A class of custom Profiler used to do profiling\n             (default is pyspark.profiler.BasicProfiler).\n      \n      \n      &gt;&gt;&gt; from pyspark.context import SparkContext\n      &gt;&gt;&gt; sc = SparkContext(&#39;local&#39;, &#39;test&#39;)\n      \n      &gt;&gt;&gt; sc2 = SparkContext(&#39;local&#39;, &#39;test2&#39;) # doctest: +IGNORE_EXCEPTION_DETAIL\n      Traceback (most recent call last):\n          ...\n      ValueError:...\n  \n  __repr__(self)\n      Return repr(self).\n  \n  accumulator(self, value, accum_param=None)\n      Create an :class:`Accumulator` with the given initial value, using a given\n      :class:`AccumulatorParam` helper object to define how to add values of the\n      data type if provided. Default AccumulatorParams are used for integers\n      and floating-point numbers if you do not provide one. For other types,\n      a custom AccumulatorParam can be used.\n  \n  addClusterWideLibraryToPath(self, libname)\n      # add cluster wide library to the system path. This is called by libraries installed\n      # through addPyFile, library installation UI or command line (databricks libraries install)\n  \n  addFile(self, path, recursive=False)\n      Add a file to be downloaded with this Spark job on every node.\n      The `path` passed can be either a local file, a file in HDFS\n      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n      FTP URI.\n      \n      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n      filename to find its download location.\n      \n      A directory can be given if the recursive option is set to True.\n      Currently directories are only supported for Hadoop-supported filesystems.\n      \n      .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n      \n      &gt;&gt;&gt; from pyspark import SparkFiles\n      &gt;&gt;&gt; path = os.path.join(tempdir, &#34;test.txt&#34;)\n      &gt;&gt;&gt; with open(path, &#34;w&#34;) as testFile:\n      ...    _ = testFile.write(&#34;100&#34;)\n      &gt;&gt;&gt; sc.addFile(path)\n      &gt;&gt;&gt; def func(iterator):\n      ...    with open(SparkFiles.get(&#34;test.txt&#34;)) as testFile:\n      ...        fileVal = int(testFile.readline())\n      ...        return [x * fileVal for x in iterator]\n      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n      [100, 200, 300, 400]\n  \n  addIsolatedLibraryPath(self, libname)\n      For a .egg, .zip, or .jar library, add the path to sys.path, and add it to\n      _python_includes for passing it to executor.\n  \n  addPyFile(self, path)\n      Add a .py or .zip dependency for all tasks to be executed on this\n      SparkContext in the future.  The `path` passed can be either a local\n      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n      HTTP, HTTPS or FTP URI.\n      \n      .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n  \n  binaryFiles(self, path, minPartitions=None)\n      Read a directory of binary files from HDFS, a local file system\n      (available on all nodes), or any Hadoop-supported file system URI\n      as a byte array. Each file is read as a single record and returned\n      in a key-value pair, where the key is the path of each file, the\n      value is the content of each file.\n      \n      .. note:: Small files are preferred, large file is also allowable, but\n          may cause bad performance.\n  \n  binaryRecords(self, path, recordLength)\n      Load data from a flat binary file, assuming each record is a set of numbers\n      with the specified numerical format (see ByteBuffer), and the number of\n      bytes per record is constant.\n      \n      :param path: Directory to the input data files\n      :param recordLength: The length at which to split the records\n  \n  broadcast(self, value)\n      Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\n      object for reading it in distributed functions. The variable will\n      be sent to each cluster only once.\n  \n  cancelAllJobs(self)\n      Cancel all jobs that have been scheduled or are running.\n  \n  cancelJobGroup(self, groupId)\n      Cancel active jobs for the specified group. See :meth:`SparkContext.setJobGroup`.\n      for more information.\n  \n  dump_profiles(self, path)\n      Dump the profile stats into directory `path`\n  \n  emptyRDD(self)\n      Create an RDD that has no partitions or elements.\n  \n  getConf(self)\n  \n  getLocalProperty(self, key)\n      Get a local property set in this thread, or null if it is missing. See\n      :meth:`setLocalProperty`.\n  \n  hadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n      Read an &#39;old&#39; Hadoop InputFormat with arbitrary key and value class from HDFS,\n      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n      The mechanism is the same as for sc.sequenceFile.\n      \n      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n      Configuration in Java.\n      \n      :param path: path to Hadoop file\n      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n             (e.g. &#34;org.apache.hadoop.mapred.TextInputFormat&#34;)\n      :param keyClass: fully qualified classname of key Writable class\n             (e.g. &#34;org.apache.hadoop.io.Text&#34;)\n      :param valueClass: fully qualified classname of value Writable class\n             (e.g. &#34;org.apache.hadoop.io.LongWritable&#34;)\n      :param keyConverter: (None by default)\n      :param valueConverter: (None by default)\n      :param conf: Hadoop configuration, passed in as a dict\n             (None by default)\n      :param batchSize: The number of Python objects represented as a single\n             Java object. (default 0, choose batchSize automatically)\n  \n  hadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n      Read an &#39;old&#39; Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n      Hadoop configuration, which is passed in as a Python dict.\n      This will be converted into a Configuration in Java.\n      The mechanism is the same as for sc.sequenceFile.\n      \n      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n             (e.g. &#34;org.apache.hadoop.mapred.TextInputFormat&#34;)\n      :param keyClass: fully qualified classname of key Writable class\n             (e.g. &#34;org.apache.hadoop.io.Text&#34;)\n      :param valueClass: fully qualified classname of value Writable class\n             (e.g. &#34;org.apache.hadoop.io.LongWritable&#34;)\n      :param keyConverter: (None by default)\n      :param valueConverter: (None by default)\n      :param conf: Hadoop configuration, passed in as a dict\n             (None by default)\n      :param batchSize: The number of Python objects represented as a single\n             Java object. (default 0, choose batchSize automatically)\n  \n  init_batched_serializer(self, data, numSlices=None)\n      Init the batched serializer with the data to parallelize into an RDD and numSlices for the\n      batch size.\n  \n  newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n      Read a &#39;new API&#39; Hadoop InputFormat with arbitrary key and value class from HDFS,\n      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n      The mechanism is the same as for sc.sequenceFile.\n      \n      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n      Configuration in Java\n      \n      :param path: path to Hadoop file\n      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n             (e.g. &#34;org.apache.hadoop.mapreduce.lib.input.TextInputFormat&#34;)\n      :param keyClass: fully qualified classname of key Writable class\n             (e.g. &#34;org.apache.hadoop.io.Text&#34;)\n      :param valueClass: fully qualified classname of value Writable class\n             (e.g. &#34;org.apache.hadoop.io.LongWritable&#34;)\n      :param keyConverter: (None by default)\n      :param valueConverter: (None by default)\n      :param conf: Hadoop configuration, passed in as a dict\n             (None by default)\n      :param batchSize: The number of Python objects represented as a single\n             Java object. (default 0, choose batchSize automatically)\n  \n  newAPIHadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n      Read a &#39;new API&#39; Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n      Hadoop configuration, which is passed in as a Python dict.\n      This will be converted into a Configuration in Java.\n      The mechanism is the same as for sc.sequenceFile.\n      \n      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n             (e.g. &#34;org.apache.hadoop.mapreduce.lib.input.TextInputFormat&#34;)\n      :param keyClass: fully qualified classname of key Writable class\n             (e.g. &#34;org.apache.hadoop.io.Text&#34;)\n      :param valueClass: fully qualified classname of value Writable class\n             (e.g. &#34;org.apache.hadoop.io.LongWritable&#34;)\n      :param keyConverter: (None by default)\n      :param valueConverter: (None by default)\n      :param conf: Hadoop configuration, passed in as a dict\n             (None by default)\n      :param batchSize: The number of Python objects represented as a single\n             Java object. (default 0, choose batchSize automatically)\n  \n  parallelize(self, c, numSlices=None)\n      Distribute a local Python collection to form an RDD. Using xrange\n      is recommended if the input represents a range for performance.\n      \n      &gt;&gt;&gt; sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n      [[0], [2], [3], [4], [6]]\n      &gt;&gt;&gt; sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n      [[], [0], [], [2], [4]]\n  \n  pickleFile(self, name, minPartitions=None)\n      Load an RDD previously saved using :meth:`RDD.saveAsPickleFile` method.\n      \n      &gt;&gt;&gt; tmpFile = NamedTemporaryFile(delete=True)\n      &gt;&gt;&gt; tmpFile.close()\n      &gt;&gt;&gt; sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\n      &gt;&gt;&gt; sorted(sc.pickleFile(tmpFile.name, 3).collect())\n      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n  \n  range(self, start, end=None, step=1, numSlices=None)\n      Create a new RDD of int containing elements from `start` to `end`\n      (exclusive), increased by `step` every element. Can be called the same\n      way as python&#39;s built-in range() function. If called with a single argument,\n      the argument is interpreted as `end`, and `start` is set to 0.\n      \n      :param start: the start value\n      :param end: the end value (exclusive)\n      :param step: the incremental step (default: 1)\n      :param numSlices: the number of partitions of the new RDD\n      :return: An RDD of int\n      \n      &gt;&gt;&gt; sc.range(5).collect()\n      [0, 1, 2, 3, 4]\n      &gt;&gt;&gt; sc.range(2, 4).collect()\n      [2, 3]\n      &gt;&gt;&gt; sc.range(1, 7, 2).collect()\n      [1, 3, 5]\n  \n  runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False)\n      Executes the given partitionFunc on the specified set of partitions,\n      returning the result as an array of elements.\n      \n      If &#39;partitions&#39; is not specified, this will run over all partitions.\n      \n      &gt;&gt;&gt; myRDD = sc.parallelize(range(6), 3)\n      &gt;&gt;&gt; sc.runJob(myRDD, lambda part: [x * x for x in part])\n      [0, 1, 4, 9, 16, 25]\n      \n      &gt;&gt;&gt; myRDD = sc.parallelize(range(6), 3)\n      &gt;&gt;&gt; sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n      [0, 1, 16, 25]\n  \n  sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)\n      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n      The mechanism is as follows:\n      \n          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n             and value Writable classes\n          2. Serialization is attempted via Pyrolite pickling\n          3. If this fails, the fallback is to call &#39;toString&#39; on each key and value\n          4. :class:`PickleSerializer` is used to deserialize pickled objects on the Python side\n      \n      :param path: path to sequncefile\n      :param keyClass: fully qualified classname of key Writable class\n             (e.g. &#34;org.apache.hadoop.io.Text&#34;)\n      :param valueClass: fully qualified classname of value Writable class\n             (e.g. &#34;org.apache.hadoop.io.LongWritable&#34;)\n      :param keyConverter:\n      :param valueConverter:\n      :param minSplits: minimum splits in dataset\n             (default min(2, sc.defaultParallelism))\n      :param batchSize: The number of Python objects represented as a single\n             Java object. (default 0, choose batchSize automatically)\n  \n  setCheckpointDir(self, dirName)\n      Set the directory under which RDDs are going to be checkpointed. The\n      directory must be an HDFS path if running on a cluster.\n  \n  setJobDescription(self, value)\n      Set a human readable description of the current job.\n      \n      .. note:: Currently, setting a job description (set to local properties) with multiple\n          threads does not properly work. Internally threads on PVM and JVM are not synced,\n          and JVM thread can be reused for multiple threads on PVM, which fails to isolate\n          local properties for each thread on PVM. To work around this, You can use\n          :meth:`RDD.collectWithJobGroup` for now.\n  \n  setJobGroup(self, groupId, description, interruptOnCancel=False)\n      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n      different value or cleared.\n      \n      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n      Application programmers can use this method to group all those jobs together and give a\n      group description. Once set, the Spark web UI will associate such jobs with this group.\n      \n      The application can use :meth:`SparkContext.cancelJobGroup` to cancel all\n      running jobs in this group.\n      \n      &gt;&gt;&gt; import threading\n      &gt;&gt;&gt; from time import sleep\n      &gt;&gt;&gt; result = &#34;Not Set&#34;\n      &gt;&gt;&gt; lock = threading.Lock()\n      &gt;&gt;&gt; def map_func(x):\n      ...     sleep(100)\n      ...     raise Exception(&#34;Task should have been cancelled&#34;)\n      &gt;&gt;&gt; def start_job(x):\n      ...     global result\n      ...     try:\n      ...         sc.setJobGroup(&#34;job_to_cancel&#34;, &#34;some description&#34;)\n      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n      ...     except Exception as e:\n      ...         result = &#34;Cancelled&#34;\n      ...     lock.release()\n      &gt;&gt;&gt; def stop_job():\n      ...     sleep(5)\n      ...     sc.cancelJobGroup(&#34;job_to_cancel&#34;)\n      &gt;&gt;&gt; suppress = lock.acquire()\n      &gt;&gt;&gt; suppress = threading.Thread(target=start_job, args=(10,)).start()\n      &gt;&gt;&gt; suppress = threading.Thread(target=stop_job).start()\n      &gt;&gt;&gt; suppress = lock.acquire()\n      &gt;&gt;&gt; print(result)\n      Cancelled\n      \n      If interruptOnCancel is set to true for the job group, then job cancellation will result\n      in Thread.interrupt() being called on the job&#39;s executor threads. This is useful to help\n      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n      \n      .. note:: Currently, setting a group ID (set to local properties) with multiple threads\n          does not properly work. Internally threads on PVM and JVM are not synced, and JVM\n          thread can be reused for multiple threads on PVM, which fails to isolate local\n          properties for each thread on PVM. To work around this, You can use\n          :meth:`RDD.collectWithJobGroup` for now.\n  \n  setLocalProperty(self, key, value)\n      Set a local property that affects jobs submitted from this thread, such as the\n      Spark fair scheduler pool.\n      \n      .. note:: Currently, setting a local property with multiple threads does not properly work.\n          Internally threads on PVM and JVM are not synced, and JVM thread\n          can be reused for multiple threads on PVM, which fails to isolate local properties\n          for each thread on PVM. To work around this, You can use\n          :meth:`RDD.collectWithJobGroup`.\n  \n  setLogLevel(self, logLevel)\n      Control our logLevel. This overrides any user-defined log settings.\n      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n  \n  show_profiles(self)\n      Print the profile stats to stdout\n  \n  sparkUser(self)\n      Get SPARK_USER for user who is running SparkContext.\n  \n  statusTracker(self)\n      Return :class:`StatusTracker` object\n  \n  stop(self)\n      Shut down the SparkContext.\n  \n  textFile(self, name, minPartitions=None, use_unicode=True)\n      Read a text file from HDFS, a local file system (available on all\n      nodes), or any Hadoop-supported file system URI, and return it as an\n      RDD of Strings.\n      The text files must be encoded as UTF-8.\n      \n      If use_unicode is False, the strings will be kept as `str` (encoding\n      as `utf-8`), which is faster and smaller than unicode. (Added in\n      Spark 1.2)\n      \n      &gt;&gt;&gt; path = os.path.join(tempdir, &#34;sample-text.txt&#34;)\n      &gt;&gt;&gt; with open(path, &#34;w&#34;) as testFile:\n      ...    _ = testFile.write(&#34;Hello world!&#34;)\n      &gt;&gt;&gt; textFile = sc.textFile(path)\n      &gt;&gt;&gt; textFile.collect()\n      [&#39;Hello world!&#39;]\n  \n  union(self, rdds)\n      Build the union of a list of RDDs.\n      \n      This supports unions() of RDDs with different serialized formats,\n      although this forces them to be reserialized using the default\n      serializer:\n      \n      &gt;&gt;&gt; path = os.path.join(tempdir, &#34;union-text.txt&#34;)\n      &gt;&gt;&gt; with open(path, &#34;w&#34;) as testFile:\n      ...    _ = testFile.write(&#34;Hello&#34;)\n      &gt;&gt;&gt; textFile = sc.textFile(path)\n      &gt;&gt;&gt; textFile.collect()\n      [&#39;Hello&#39;]\n      &gt;&gt;&gt; parallelized = sc.parallelize([&#34;World!&#34;])\n      &gt;&gt;&gt; sorted(sc.union([textFile, parallelized]).collect())\n      [&#39;Hello&#39;, &#39;World!&#39;]\n  \n  wholeTextFiles(self, path, minPartitions=None, use_unicode=True)\n      Read a directory of text files from HDFS, a local file system\n      (available on all nodes), or any  Hadoop-supported file system\n      URI. Each file is read as a single record and returned in a\n      key-value pair, where the key is the path of each file, the\n      value is the content of each file.\n      The text files must be encoded as UTF-8.\n      \n      If use_unicode is False, the strings will be kept as `str` (encoding\n      as `utf-8`), which is faster and smaller than unicode. (Added in\n      Spark 1.2)\n      \n      For example, if you have the following files:\n      \n      .. code-block:: text\n      \n          hdfs://a-hdfs-path/part-00000\n          hdfs://a-hdfs-path/part-00001\n          ...\n          hdfs://a-hdfs-path/part-nnnnn\n      \n      Do ``rdd = sparkContext.wholeTextFiles(&#34;hdfs://a-hdfs-path&#34;)``,\n      then ``rdd`` contains:\n      \n      .. code-block:: text\n      \n          (a-hdfs-path/part-00000, its content)\n          (a-hdfs-path/part-00001, its content)\n          ...\n          (a-hdfs-path/part-nnnnn, its content)\n      \n      .. note:: Small files are preferred, as each file will be loaded\n          fully in memory.\n      \n      &gt;&gt;&gt; dirPath = os.path.join(tempdir, &#34;files&#34;)\n      &gt;&gt;&gt; os.mkdir(dirPath)\n      &gt;&gt;&gt; with open(os.path.join(dirPath, &#34;1.txt&#34;), &#34;w&#34;) as file1:\n      ...    _ = file1.write(&#34;1&#34;)\n      &gt;&gt;&gt; with open(os.path.join(dirPath, &#34;2.txt&#34;), &#34;w&#34;) as file2:\n      ...    _ = file2.write(&#34;2&#34;)\n      &gt;&gt;&gt; textFiles = sc.wholeTextFiles(dirPath)\n      &gt;&gt;&gt; sorted(textFiles.collect())\n      [(&#39;.../1.txt&#39;, &#39;1&#39;), (&#39;.../2.txt&#39;, &#39;2&#39;)]\n  \n  ----------------------------------------------------------------------\n  Class methods inherited from pyspark.context.SparkContext:\n  \n  getOrCreate(conf=None) from builtins.type\n      Get or instantiate a SparkContext and register it as a singleton object.\n      \n      :param conf: SparkConf (optional)\n  \n  setSystemProperty(key, value) from builtins.type\n      Set a Java system property, such as spark.executor.memory. This must\n      must be invoked before instantiating SparkContext.\n  \n  ----------------------------------------------------------------------\n  Data descriptors inherited from pyspark.context.SparkContext:\n  \n  __dict__\n      dictionary for instance variables (if defined)\n  \n  __weakref__\n      list of weak references to the object (if defined)\n  \n  applicationId\n      A unique identifier for the Spark application.\n      Its format depends on the scheduler implementation.\n      \n      * in case of local spark app something like &#39;local-1433865536131&#39;\n      * in case of YARN something like &#39;application_1433865536131_34483&#39;\n      \n      &gt;&gt;&gt; sc.applicationId  # doctest: +ELLIPSIS\n      &#39;local-...&#39;\n  \n  defaultMinPartitions\n      Default min number of partitions for Hadoop RDDs when not given by user\n  \n  defaultParallelism\n      Default level of parallelism to use when not given by user (e.g. for\n      reduce tasks)\n  \n  resources\n  \n  startTime\n      Return the epoch time when the Spark Context was started.\n  \n  uiWebUrl\n      Return the URL of the SparkUI instance started by this SparkContext\n  \n  version\n      The version of Spark on which this application is running.\n  \n  ----------------------------------------------------------------------\n  Data and other attributes inherited from pyspark.context.SparkContext:\n  \n  PACKAGE_EXTENSIONS = (&#39;.zip&#39;, &#39;.egg&#39;, &#39;.jar&#39;)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Creating DataFrame using spark session (spark) `spark.createDataFrame()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20789812-b627-4e2a-90e3-7313d9f8b871"}}},{"cell_type":"code","source":["df = spark.createDataFrame([(1,'Raveendra'),(2,'Prasad'),(3,'Mahesh')], [\"ID\", \"Name\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e40a9d3-0b9d-4eb5-b21f-38f65b75e4be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Creating DataFrame using spark API... `spark.read.csv  or spark.read.format()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"590b2d8b-0bf8-4ae3-9650-4cacee5d1dfe"}}},{"cell_type":"code","source":["emp_csv_df = spark.read.csv(\"/FileStore/tables/emp.csv\",header=True,inferSchema=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86618e60-8a2a-4425-b001-db6968e45ca6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["emp_csv_df = spark.read.format(\"csv\")\\\n.option(\"header\",\"true\")\\\n.option(\"inferSchema\",\"true\")\\\n.option(\"sep\",\",\")\\\n.load(\"/FileStore/tables/emp.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"560b8228-daff-4a26-b3dd-9efac32ecf5c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["display(emp_csv_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2683f578-9c18-4744-89ff-8b0ffb71b9ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["7369","SMITH","CLERK","7902","17-12-80","800",null,"20"],["7499","ALLEN","SALESMAN","7698","20-02-81","1600","300","30"],["7521","WARD","SALESMAN","7698","22-02-81","1250","500","30"],["7566","JONES","MANAGER","7839","02-04-81","2975",null,"20"],["7654","MARTIN","SALESMAN","7698","28-09-81","1250","1400","30"],["7698","SGR","MANAGER","7839","01-05-81","2850",null,"30"],["7782","RAVI","MANAGER","7839","09-06-81","2450",null,"10"],["7788","SCOTT","ANALYST","7566","19-04-87","3000",null,"20"],["7839","KING","PRESIDENT",null,"17-11-81","5000",null,"10"],["7844","TURNER","SALESMAN","7698","08-09-81","1500","0","30"],["7876","ADAMS","CLERK","7788","23-05-87","1100",null,"20"],["7900","JAMES","CLERK","7698","03-12-81","950",null,"30"],["7902","FORD","ANALYST","7566","03-12-81","3000",null,"20"],["7934","MILLER","CLERK","7782","23-01-82","1300",null,"10"],["1234","SEKHAR","doctor","7777",null,"667","78","80"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"EMPNO","type":"\"string\"","metadata":"{}"},{"name":"ENAME","type":"\"string\"","metadata":"{}"},{"name":"JOB","type":"\"string\"","metadata":"{}"},{"name":"MGR","type":"\"string\"","metadata":"{}"},{"name":"HIREDATE","type":"\"string\"","metadata":"{}"},{"name":"SAL","type":"\"string\"","metadata":"{}"},{"name":"COMM","type":"\"string\"","metadata":"{}"},{"name":"DEPTNO","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EMPNO</th><th>ENAME</th><th>JOB</th><th>MGR</th><th>HIREDATE</th><th>SAL</th><th>COMM</th><th>DEPTNO</th></tr></thead><tbody><tr><td>7369</td><td>SMITH</td><td>CLERK</td><td>7902</td><td>17-12-80</td><td>800</td><td>null</td><td>20</td></tr><tr><td>7499</td><td>ALLEN</td><td>SALESMAN</td><td>7698</td><td>20-02-81</td><td>1600</td><td>300</td><td>30</td></tr><tr><td>7521</td><td>WARD</td><td>SALESMAN</td><td>7698</td><td>22-02-81</td><td>1250</td><td>500</td><td>30</td></tr><tr><td>7566</td><td>JONES</td><td>MANAGER</td><td>7839</td><td>02-04-81</td><td>2975</td><td>null</td><td>20</td></tr><tr><td>7654</td><td>MARTIN</td><td>SALESMAN</td><td>7698</td><td>28-09-81</td><td>1250</td><td>1400</td><td>30</td></tr><tr><td>7698</td><td>SGR</td><td>MANAGER</td><td>7839</td><td>01-05-81</td><td>2850</td><td>null</td><td>30</td></tr><tr><td>7782</td><td>RAVI</td><td>MANAGER</td><td>7839</td><td>09-06-81</td><td>2450</td><td>null</td><td>10</td></tr><tr><td>7788</td><td>SCOTT</td><td>ANALYST</td><td>7566</td><td>19-04-87</td><td>3000</td><td>null</td><td>20</td></tr><tr><td>7839</td><td>KING</td><td>PRESIDENT</td><td>null</td><td>17-11-81</td><td>5000</td><td>null</td><td>10</td></tr><tr><td>7844</td><td>TURNER</td><td>SALESMAN</td><td>7698</td><td>08-09-81</td><td>1500</td><td>0</td><td>30</td></tr><tr><td>7876</td><td>ADAMS</td><td>CLERK</td><td>7788</td><td>23-05-87</td><td>1100</td><td>null</td><td>20</td></tr><tr><td>7900</td><td>JAMES</td><td>CLERK</td><td>7698</td><td>03-12-81</td><td>950</td><td>null</td><td>30</td></tr><tr><td>7902</td><td>FORD</td><td>ANALYST</td><td>7566</td><td>03-12-81</td><td>3000</td><td>null</td><td>20</td></tr><tr><td>7934</td><td>MILLER</td><td>CLERK</td><td>7782</td><td>23-01-82</td><td>1300</td><td>null</td><td>10</td></tr><tr><td>1234</td><td>SEKHAR</td><td>doctor</td><td>7777</td><td>null</td><td>667</td><td>78</td><td>80</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs ls /FileStore/tables/emp.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e5c0cc5-37b1-44bf-ab7e-bdb232965b89"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/FileStore/tables/emp.csv","emp.csv",684]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/emp.csv</td><td>emp.csv</td><td>684</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0429f736-ea6f-4494-874a-e5d92cb403e9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on DataFrame in module pyspark.sql.dataframe object:\n\nclass DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n |  DataFrame(jdf, sql_ctx)\n |  \n |  A distributed collection of data grouped into named columns.\n |  \n |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n |  and can be created using various functions in :class:`SparkSession`::\n |  \n |      people = spark.read.parquet(&#34;...&#34;)\n |  \n |  Once created, it can be manipulated using the various domain-specific-language\n |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n |  \n |  To select a column from the :class:`DataFrame`, use the apply method::\n |  \n |      ageCol = people.age\n |  \n |  A more concrete example::\n |  \n |      # To create DataFrame using SparkSession\n |      people = spark.read.parquet(&#34;...&#34;)\n |      department = spark.read.parquet(&#34;...&#34;)\n |  \n |      people.filter(people.age &gt; 30).join(department, people.deptId == department.id) \\\n |        .groupBy(department.name, &#34;gender&#34;).agg({&#34;salary&#34;: &#34;avg&#34;, &#34;age&#34;: &#34;max&#34;})\n |  \n |  .. versionadded:: 1.3\n |  \n |  Method resolution order:\n |      DataFrame\n |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n |      pyspark.sql.pandas.conversion.PandasConversionMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getattr__(self, name)\n |      Returns the :class:`Column` denoted by ``name``.\n |      \n |      &gt;&gt;&gt; df.select(df.age).collect()\n |      [Row(age=2), Row(age=5)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  __getitem__(self, item)\n |      Returns the column as a :class:`Column`.\n |      \n |      &gt;&gt;&gt; df.select(df[&#39;age&#39;]).collect()\n |      [Row(age=2), Row(age=5)]\n |      &gt;&gt;&gt; df[ [&#34;name&#34;, &#34;age&#34;]].collect()\n |      [Row(name=&#39;Alice&#39;, age=2), Row(name=&#39;Bob&#39;, age=5)]\n |      &gt;&gt;&gt; df[ df.age &gt; 3 ].collect()\n |      [Row(age=5, name=&#39;Bob&#39;)]\n |      &gt;&gt;&gt; df[df[0] &gt; 3].collect()\n |      [Row(age=5, name=&#39;Bob&#39;)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  __init__(self, jdf, sql_ctx)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  agg(self, *exprs)\n |      Aggregate on the entire :class:`DataFrame` without groups\n |      (shorthand for ``df.groupBy.agg()``).\n |      \n |      &gt;&gt;&gt; df.agg({&#34;age&#34;: &#34;max&#34;}).collect()\n |      [Row(max(age)=5)]\n |      &gt;&gt;&gt; from pyspark.sql import functions as F\n |      &gt;&gt;&gt; df.agg(F.min(df.age)).collect()\n |      [Row(min(age)=2)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  alias(self, alias)\n |      Returns a new :class:`DataFrame` with an alias set.\n |      \n |      :param alias: string, an alias name to be set for the :class:`DataFrame`.\n |      \n |      &gt;&gt;&gt; from pyspark.sql.functions import *\n |      &gt;&gt;&gt; df_as1 = df.alias(&#34;df_as1&#34;)\n |      &gt;&gt;&gt; df_as2 = df.alias(&#34;df_as2&#34;)\n |      &gt;&gt;&gt; joined_df = df_as1.join(df_as2, col(&#34;df_as1.name&#34;) == col(&#34;df_as2.name&#34;), &#39;inner&#39;)\n |      &gt;&gt;&gt; joined_df.select(&#34;df_as1.name&#34;, &#34;df_as2.name&#34;, &#34;df_as2.age&#34;)                 .sort(desc(&#34;df_as1.name&#34;)).collect()\n |      [Row(name=&#39;Bob&#39;, name=&#39;Bob&#39;, age=5), Row(name=&#39;Alice&#39;, name=&#39;Alice&#39;, age=2)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  approxQuantile(self, col, probabilities, relativeError)\n |      Calculates the approximate quantiles of numerical columns of a\n |      :class:`DataFrame`.\n |      \n |      The result of this algorithm has the following deterministic bound:\n |      If the :class:`DataFrame` has N elements and if we request the quantile at\n |      probability `p` up to error `err`, then the algorithm will return\n |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n |      close to (p * N). More precisely,\n |      \n |        floor((p - err) * N) &lt;= rank(x) &lt;= ceil((p + err) * N).\n |      \n |      This method implements a variation of the Greenwald-Khanna\n |      algorithm (with some speed optimizations). The algorithm was first\n |      present in [[https://doi.org/10.1145/375663.375670\n |      Space-efficient Online Computation of Quantile Summaries]]\n |      by Greenwald and Khanna.\n |      \n |      Note that null values will be ignored in numerical columns before calculation.\n |      For columns only containing null values, an empty list is returned.\n |      \n |      :param col: str, list.\n |        Can be a single column name, or a list of names for multiple columns.\n |      :param probabilities: a list of quantile probabilities\n |        Each number must belong to [0, 1].\n |        For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n |      :param relativeError:  The relative target precision to achieve\n |        (&gt;= 0). If set to zero, the exact quantiles are computed, which\n |        could be very expensive. Note that values greater than 1 are\n |        accepted but give the same result as 1.\n |      :return:  the approximate quantiles at the given probabilities. If\n |        the input `col` is a string, the output is a list of floats. If the\n |        input `col` is a list or tuple of strings, the output is also a\n |        list, but each element in it is a list of floats, i.e., the output\n |        is a list of list of floats.\n |      \n |      .. versionchanged:: 2.2\n |         Added support for multiple columns.\n |      \n |      .. versionadded:: 2.0\n |  \n |  cache(self)\n |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n |      \n |      .. note:: The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n |      \n |      .. versionadded:: 1.3\n |  \n |  checkpoint(self, eager=True)\n |      Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n |      logical plan of this :class:`DataFrame`, which is especially useful in iterative algorithms\n |      where the plan may grow exponentially. It will be saved to files inside the checkpoint\n |      directory set with :meth:`SparkContext.setCheckpointDir`.\n |      \n |      :param eager: Whether to checkpoint this :class:`DataFrame` immediately\n |      \n |      .. note:: Experimental\n |      \n |      .. versionadded:: 2.1\n |  \n |  coalesce(self, numPartitions)\n |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n |      \n |      :param numPartitions: int, to specify the target number of partitions\n |      \n |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n |      there will not be a shuffle, instead each of the 100 new partitions will\n |      claim 10 of the current partitions. If a larger number of partitions is requested,\n |      it will stay at the current number of partitions.\n |      \n |      However, if you&#39;re doing a drastic coalesce, e.g. to numPartitions = 1,\n |      this may result in your computation taking place on fewer nodes than\n |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n |      you can call repartition(). This will add a shuffle step, but means the\n |      current upstream partitions will be executed in parallel (per whatever\n |      the current partitioning is).\n |      \n |      &gt;&gt;&gt; df.coalesce(1).rdd.getNumPartitions()\n |      1\n |      \n |      .. versionadded:: 1.4\n |  \n |  colRegex(self, colName)\n |      Selects column based on the column name specified as a regex and returns it\n |      as :class:`Column`.\n |      \n |      :param colName: string, column name specified as a regex.\n |      \n |      &gt;&gt;&gt; df = spark.createDataFrame([(&#34;a&#34;, 1), (&#34;b&#34;, 2), (&#34;c&#34;,  3)], [&#34;Col1&#34;, &#34;Col2&#34;])\n |      &gt;&gt;&gt; df.select(df.colRegex(&#34;`(Col1)?+.+`&#34;)).show()\n |      +----+\n |      |Col2|\n |      +----+\n |      |   1|\n |      |   2|\n |      |   3|\n |      +----+\n |      \n |      .. versionadded:: 2.3\n |  \n |  collect(self)\n |      Returns all the records as a list of :class:`Row`.\n |      \n |      &gt;&gt;&gt; df.collect()\n |      [Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  corr(self, col1, col2, method=None)\n |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n |      Currently only supports the Pearson Correlation Coefficient.\n |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n |      \n |      :param col1: The name of the first column\n |      :param col2: The name of the second column\n |      :param method: The correlation method. Currently only supports &#34;pearson&#34;\n |      \n |      .. versionadded:: 1.4\n |  \n |  count(self)\n |      Returns the number of rows in this :class:`DataFrame`.\n |      \n |      &gt;&gt;&gt; df.count()\n |      2\n |      \n |      .. versionadded:: 1.3\n |  \n |  cov(self, col1, col2)\n |      Calculate the sample covariance for the given columns, specified by their names, as a\n |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n |      \n |      :param col1: The name of the first column\n |      :param col2: The name of the second column\n |      \n |      .. versionadded:: 1.4\n |  \n |  createGlobalTempView(self, name)\n |      Creates a global temporary view with this :class:`DataFrame`.\n |      \n |      The lifetime of this temporary view is tied to this Spark application.\n |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n |      catalog.\n |      \n |      &gt;&gt;&gt; df.createGlobalTempView(&#34;people&#34;)\n |      &gt;&gt;&gt; df2 = spark.sql(&#34;select * from global_temp.people&#34;)\n |      &gt;&gt;&gt; sorted(df.collect()) == sorted(df2.collect())\n |      True\n |      &gt;&gt;&gt; df.createGlobalTempView(&#34;people&#34;)  # doctest: +IGNORE_EXCEPTION_DETAIL\n |      Traceback (most recent call last):\n |      ...\n |      AnalysisException: u&#34;Temporary table &#39;people&#39; already exists;&#34;\n |      &gt;&gt;&gt; spark.catalog.dropGlobalTempView(&#34;people&#34;)\n |      \n |      .. versionadded:: 2.1\n |  \n |  createOrReplaceGlobalTempView(self, name)\n |      Creates or replaces a global temporary view using the given name.\n |      \n |      The lifetime of this temporary view is tied to this Spark application.\n |      \n |      &gt;&gt;&gt; df.createOrReplaceGlobalTempView(&#34;people&#34;)\n |      &gt;&gt;&gt; df2 = df.filter(df.age &gt; 3)\n |      &gt;&gt;&gt; df2.createOrReplaceGlobalTempView(&#34;people&#34;)\n |      &gt;&gt;&gt; df3 = spark.sql(&#34;select * from global_temp.people&#34;)\n |      &gt;&gt;&gt; sorted(df3.collect()) == sorted(df2.collect())\n |      True\n |      &gt;&gt;&gt; spark.catalog.dropGlobalTempView(&#34;people&#34;)\n |      \n |      .. versionadded:: 2.2\n |  \n |  createOrReplaceTempView(self, name)\n |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n |      \n |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n |      that was used to create this :class:`DataFrame`.\n |      \n |      &gt;&gt;&gt; df.createOrReplaceTempView(&#34;people&#34;)\n |      &gt;&gt;&gt; df2 = df.filter(df.age &gt; 3)\n |      &gt;&gt;&gt; df2.createOrReplaceTempView(&#34;people&#34;)\n |      &gt;&gt;&gt; df3 = spark.sql(&#34;select * from people&#34;)\n |      &gt;&gt;&gt; sorted(df3.collect()) == sorted(df2.collect())\n |      True\n |      &gt;&gt;&gt; spark.catalog.dropTempView(&#34;people&#34;)\n |      \n |      .. versionadded:: 2.0\n |  \n |  createTempView(self, name)\n |      Creates a local temporary view with this :class:`DataFrame`.\n |      \n |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n |      that was used to create this :class:`DataFrame`.\n |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n |      catalog.\n |      \n |      &gt;&gt;&gt; df.createTempView(&#34;people&#34;)\n |      &gt;&gt;&gt; df2 = spark.sql(&#34;select * from people&#34;)\n |      &gt;&gt;&gt; sorted(df.collect()) == sorted(df2.collect())\n |      True\n |      &gt;&gt;&gt; df.createTempView(&#34;people&#34;)  # doctest: +IGNORE_EXCEPTION_DETAIL\n |      Traceback (most recent call last):\n |      ...\n |      AnalysisException: u&#34;Temporary table &#39;people&#39; already exists;&#34;\n |      &gt;&gt;&gt; spark.catalog.dropTempView(&#34;people&#34;)\n |      \n |      .. versionadded:: 2.0\n |  \n |  crossJoin(self, other)\n |      Returns the cartesian product with another :class:`DataFrame`.\n |      \n |      :param other: Right side of the cartesian product.\n |      \n |      &gt;&gt;&gt; df.select(&#34;age&#34;, &#34;name&#34;).collect()\n |      [Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]\n |      &gt;&gt;&gt; df2.select(&#34;name&#34;, &#34;height&#34;).collect()\n |      [Row(name=&#39;Tom&#39;, height=80), Row(name=&#39;Bob&#39;, height=85)]\n |      &gt;&gt;&gt; df.crossJoin(df2.select(&#34;height&#34;)).select(&#34;age&#34;, &#34;name&#34;, &#34;height&#34;).collect()\n |      [Row(age=2, name=&#39;Alice&#39;, height=80), Row(age=2, name=&#39;Alice&#39;, height=85),\n |       Row(age=5, name=&#39;Bob&#39;, height=80), Row(age=5, name=&#39;Bob&#39;, height=85)]\n |      \n |      .. versionadded:: 2.1\n |  \n |  crosstab(self, col1, col2)\n |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n |      non-zero pair frequencies will be returned.\n |      The first column of each row will be the distinct values of `col1` and the column names\n |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n |      Pairs that have no occurrences will have zero as their counts.\n |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n |      \n |      :param col1: The name of the first column. Distinct items will make the first item of\n |          each row.\n |      :param col2: The name of the second column. Distinct items will make the column names\n |          of the :class:`DataFrame`.\n |      \n |      .. versionadded:: 1.4\n |  \n |  cube(self, *cols)\n |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n |      the specified columns, so we can run aggregations on them.\n |      \n |      &gt;&gt;&gt; df.cube(&#34;name&#34;, df.age).count().orderBy(&#34;name&#34;, &#34;age&#34;).show()\n |      +-----+----+-----+\n |      | name| age|count|\n |      +-----+----+-----+\n |      | null|null|    2|\n |      | null|   2|    1|\n |      | null|   5|    1|\n |      |Alice|null|    1|\n |      |Alice|   2|    1|\n |      |  Bob|null|    1|\n |      |  Bob|   5|    1|\n |      +-----+----+-----+\n |      \n |      .. versionadded:: 1.4\n |  \n |  describe(self, *cols)\n |      Computes basic statistics for numeric and string columns.\n |      \n |      This include count, mean, stddev, min, and max. If no columns are\n |      given, this function computes statistics for all numerical or string columns.\n |      \n |      .. note:: This function is meant for exploratory data analysis, as we make no\n |          guarantee about the backward compatibility of the schema of the resulting\n |          :class:`DataFrame`.\n |      \n |      &gt;&gt;&gt; df.describe([&#39;age&#39;]).show()\n |      +-------+------------------+\n |      |summary|               age|\n |      +-------+------------------+\n |      |  count|                 2|\n |      |   mean|               3.5|\n |      | stddev|2.1213203435596424|\n |      |    min|                 2|\n |      |    max|                 5|\n |      +-------+------------------+\n |      &gt;&gt;&gt; df.describe().show()\n |      +-------+------------------+-----+\n |      |summary|               age| name|\n |      +-------+------------------+-----+\n |      |  count|                 2|    2|\n |      |   mean|               3.5| null|\n |      | stddev|2.1213203435596424| null|\n |      |    min|                 2|Alice|\n |      |    max|                 5|  Bob|\n |      +-------+------------------+-----+\n |      \n |      Use summary for expanded statistics and control over which statistics to compute.\n |      \n |      .. versionadded:: 1.3.1\n |  \n |  display = df_display(df, *args, **kwargs)\n |      df.display() is an alias for display(df). Run help(display) for more information.\n |  \n |  distinct(self)\n |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n |      \n |      &gt;&gt;&gt; df.distinct().count()\n |      2\n |      \n |      .. versionadded:: 1.3\n |  \n |  drop(self, *cols)\n |      Returns a new :class:`DataFrame` that drops the specified column.\n |      This is a no-op if schema doesn&#39;t contain the given column name(s).\n |      \n |      :param cols: a string name of the column to drop, or a\n |          :class:`Column` to drop, or a list of string name of the columns to drop.\n |      \n |      &gt;&gt;&gt; df.drop(&#39;age&#39;).collect()\n |      [Row(name=&#39;Alice&#39;), Row(name=&#39;Bob&#39;)]\n |      \n |      &gt;&gt;&gt; df.drop(df.age).collect()\n |      [Row(name=&#39;Alice&#39;), Row(name=&#39;Bob&#39;)]\n |      \n |      &gt;&gt;&gt; df.join(df2, df.name == df2.name, &#39;inner&#39;).drop(df.name).collect()\n |      [Row(age=5, height=85, name=&#39;Bob&#39;)]\n |      \n |      &gt;&gt;&gt; df.join(df2, df.name == df2.name, &#39;inner&#39;).drop(df2.name).collect()\n |      [Row(age=5, name=&#39;Bob&#39;, height=85)]\n |      \n |      &gt;&gt;&gt; df.join(df2, &#39;name&#39;, &#39;inner&#39;).drop(&#39;age&#39;, &#39;height&#39;).collect()\n |      [Row(name=&#39;Bob&#39;)]\n |      \n |      .. versionadded:: 1.4\n |  \n |  dropDuplicates(self, subset=None)\n |      Return a new :class:`DataFrame` with duplicate rows removed,\n |      optionally only considering certain columns.\n |      \n |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n |      be and system will accordingly limit the state. In addition, too late data older than\n |      watermark will be dropped to avoid any possibility of duplicates.\n |      \n |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n |      \n |      &gt;&gt;&gt; from pyspark.sql import Row\n |      &gt;&gt;&gt; df = sc.parallelize([ \\\n |      ...     Row(name=&#39;Alice&#39;, age=5, height=80), \\\n |      ...     Row(name=&#39;Alice&#39;, age=5, height=80), \\\n |      ...     Row(name=&#39;Alice&#39;, age=10, height=80)]).toDF()\n |      &gt;&gt;&gt; df.dropDuplicates().show()\n |      +---+------+-----+\n |      |age|height| name|\n |      +---+------+-----+\n |      |  5|    80|Alice|\n |      | 10|    80|Alice|\n |      +---+------+-----+\n |      \n |      &gt;&gt;&gt; df.dropDuplicates([&#39;name&#39;, &#39;height&#39;]).show()\n |      +---+------+-----+\n |      |age|height| name|\n |      +---+------+-----+\n |      |  5|    80|Alice|\n |      +---+------+-----+\n |      \n |      .. versionadded:: 1.4\n |  \n |  drop_duplicates = dropDuplicates(self, subset=None)\n |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n |      \n |      .. versionadded:: 1.4\n |  \n |  dropna(self, how=&#39;any&#39;, thresh=None, subset=None)\n |      Returns a new :class:`DataFrame` omitting rows with null values.\n |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n |      \n |      :param how: &#39;any&#39; or &#39;all&#39;.\n |          If &#39;any&#39;, drop a row if it contains any nulls.\n |          If &#39;all&#39;, drop a row only if all its values are null.\n |      :param thresh: int, default None\n |          If specified, drop rows that have less than `thresh` non-null values.\n |          This overwrites the `how` parameter.\n |      :param subset: optional list of column names to consider.\n |      \n |      &gt;&gt;&gt; df4.na.drop().show()\n |      +---+------+-----+\n |      |age|height| name|\n |      +---+------+-----+\n |      | 10|    80|Alice|\n |      +---+------+-----+\n |      \n |      .. versionadded:: 1.3.1\n |  \n |  exceptAll(self, other)\n |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n |      not in another :class:`DataFrame` while preserving duplicates.\n |      \n |      This is equivalent to `EXCEPT ALL` in SQL.\n |      \n |      &gt;&gt;&gt; df1 = spark.createDataFrame(\n |      ...         [(&#34;a&#34;, 1), (&#34;a&#34;, 1), (&#34;a&#34;, 1), (&#34;a&#34;, 2), (&#34;b&#34;,  3), (&#34;c&#34;, 4)], [&#34;C1&#34;, &#34;C2&#34;])\n |      &gt;&gt;&gt; df2 = spark.createDataFrame([(&#34;a&#34;, 1), (&#34;b&#34;, 3)], [&#34;C1&#34;, &#34;C2&#34;])\n |      \n |      &gt;&gt;&gt; df1.exceptAll(df2).show()\n |      +---+---+\n |      | C1| C2|\n |      +---+---+\n |      |  a|  1|\n |      |  a|  1|\n |      |  a|  2|\n |      |  c|  4|\n |      +---+---+\n |      \n |      Also as standard in SQL, this function resolves columns by position (not by name).\n |      \n |      .. versionadded:: 2.4\n |  \n |  explain(self, extended=None, mode=None)\n |      Prints the (logical and physical) plans to the console for debugging purpose.\n |      \n |      :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.\n |          When this is a string without specifying the ``mode``, it works as the mode is\n |          specified.\n |      :param mode: specifies the expected output format of plans.\n |      \n |          * ``simple``: Print only a physical plan.\n |          * ``extended``: Print both logical and physical plans.\n |          * ``codegen``: Print a physical plan and generated codes if they are available.\n |          * ``cost``: Print a logical plan and statistics if they are available.\n |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n |      \n |      &gt;&gt;&gt; df.explain()\n |      == Physical Plan ==\n |      *(1) Scan ExistingRDD[age#0,name#1]\n |      \n |      &gt;&gt;&gt; df.explain(True)\n |      == Parsed Logical Plan ==\n |      ...\n |      == Analyzed Logical Plan ==\n |      ...\n |      == Optimized Logical Plan ==\n |      ...\n |      == Physical Plan ==\n |      ...\n |      \n |      &gt;&gt;&gt; df.explain(mode=&#34;formatted&#34;)\n |      == Physical Plan ==\n |      * Scan ExistingRDD (1)\n |      (1) Scan ExistingRDD [codegen id : 1]\n |      Output [2]: [age#0, name#1]\n |      ...\n |      \n |      &gt;&gt;&gt; df.explain(&#34;cost&#34;)\n |      == Optimized Logical Plan ==\n |      ...Statistics...\n |      ...\n |      \n |      .. versionchanged:: 3.0.0\n |         Added optional argument `mode` to specify the expected output format of plans.\n |      \n |      .. versionadded:: 1.3\n |  \n |  fillna(self, value, subset=None)\n |      Replace null values, alias for ``na.fill()``.\n |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n |      \n |      :param value: int, long, float, string, bool or dict.\n |          Value to replace null values with.\n |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n |          from column name (string) to replacement value. The replacement value must be\n |          an int, long, float, boolean, or string.\n |      :param subset: optional list of column names to consider.\n |          Columns specified in subset that do not have matching data type are ignored.\n |          For example, if `value` is a string, and subset contains a non-string column,\n |          then the non-string column is simply ignored.\n |      \n |      &gt;&gt;&gt; df4.na.fill(50).show()\n |      +---+------+-----+\n |      |age|height| name|\n |      +---+------+-----+\n |      | 10|    80|Alice|\n |      |  5|    50|  Bob|\n |      | 50|    50|  Tom|\n |      | 50|    50| null|\n |      +---+------+-----+\n |      \n |      &gt;&gt;&gt; df5.na.fill(False).show()\n |      +----+-------+-----+\n |      | age|   name|  spy|\n |      +----+-------+-----+\n |      |  10|  Alice|false|\n |      |   5|    Bob|false|\n |      |null|Mallory| true|\n |      +----+-------+-----+\n |      \n |      &gt;&gt;&gt; df4.na.fill({&#39;age&#39;: 50, &#39;name&#39;: &#39;unknown&#39;}).show()\n |      +---+------+-------+\n |      |age|height|   name|\n |      +---+------+-------+\n |      | 10|    80|  Alice|\n |      |  5|  null|    Bob|\n |      | 50|  null|    Tom|\n |      | 50|  null|unknown|\n |      +---+------+-------+\n |      \n |      .. versionadded:: 1.3.1\n |  \n |  filter(self, condition)\n |      Filters rows using the given condition.\n |      \n |      :func:`where` is an alias for :func:`filter`.\n |      \n |      :param condition: a :class:`Column` of :class:`types.BooleanType`\n |          or a string of SQL expression.\n |      \n |      &gt;&gt;&gt; df.filter(df.age &gt; 3).collect()\n |      [Row(age=5, name=&#39;Bob&#39;)]\n |      &gt;&gt;&gt; df.where(df.age == 2).collect()\n |      [Row(age=2, name=&#39;Alice&#39;)]\n |      \n |      &gt;&gt;&gt; df.filter(&#34;age &gt; 3&#34;).collect()\n |      [Row(age=5, name=&#39;Bob&#39;)]\n |      &gt;&gt;&gt; df.where(&#34;age = 2&#34;).collect()\n |      [Row(age=2, name=&#39;Alice&#39;)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  first(self)\n |      Returns the first row as a :class:`Row`.\n |      \n |      &gt;&gt;&gt; df.first()\n |      Row(age=2, name=&#39;Alice&#39;)\n |      \n |      .. versionadded:: 1.3\n |  \n |  foreach(self, f)\n |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n |      \n |      This is a shorthand for ``df.rdd.foreach()``.\n |      \n |      &gt;&gt;&gt; def f(person):\n |      ...     print(person.name)\n |      &gt;&gt;&gt; df.foreach(f)\n |      \n |      .. versionadded:: 1.3\n |  \n |  foreachPartition(self, f)\n |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n |      \n |      This a shorthand for ``df.rdd.foreachPartition()``.\n |      \n |      &gt;&gt;&gt; def f(people):\n |      ...     for person in people:\n |      ...         print(person.name)\n |      &gt;&gt;&gt; df.foreachPartition(f)\n\n*** WARNING: skipped 12445 bytes of output ***\n\n |      or strings. Value can have None. When replacing, the new value will be cast\n |      to the type of the existing column.\n |      For numeric replacements all values to be replaced should have unique\n |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n |      and arbitrary replacement will be used.\n |      \n |      :param to_replace: bool, int, long, float, string, list or dict.\n |          Value to be replaced.\n |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n |          must be a mapping between a value and a replacement.\n |      :param value: bool, int, long, float, string, list or None.\n |          The replacement value must be a bool, int, long, float, string or None. If `value` is a\n |          list, `value` should be of the same length and type as `to_replace`.\n |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n |          used as a replacement for each item in `to_replace`.\n |      :param subset: optional list of column names to consider.\n |          Columns specified in subset that do not have matching data type are ignored.\n |          For example, if `value` is a string, and subset contains a non-string column,\n |          then the non-string column is simply ignored.\n |      \n |      &gt;&gt;&gt; df4.na.replace(10, 20).show()\n |      +----+------+-----+\n |      | age|height| name|\n |      +----+------+-----+\n |      |  20|    80|Alice|\n |      |   5|  null|  Bob|\n |      |null|  null|  Tom|\n |      |null|  null| null|\n |      +----+------+-----+\n |      \n |      &gt;&gt;&gt; df4.na.replace(&#39;Alice&#39;, None).show()\n |      +----+------+----+\n |      | age|height|name|\n |      +----+------+----+\n |      |  10|    80|null|\n |      |   5|  null| Bob|\n |      |null|  null| Tom|\n |      |null|  null|null|\n |      +----+------+----+\n |      \n |      &gt;&gt;&gt; df4.na.replace({&#39;Alice&#39;: None}).show()\n |      +----+------+----+\n |      | age|height|name|\n |      +----+------+----+\n |      |  10|    80|null|\n |      |   5|  null| Bob|\n |      |null|  null| Tom|\n |      |null|  null|null|\n |      +----+------+----+\n |      \n |      &gt;&gt;&gt; df4.na.replace([&#39;Alice&#39;, &#39;Bob&#39;], [&#39;A&#39;, &#39;B&#39;], &#39;name&#39;).show()\n |      +----+------+----+\n |      | age|height|name|\n |      +----+------+----+\n |      |  10|    80|   A|\n |      |   5|  null|   B|\n |      |null|  null| Tom|\n |      |null|  null|null|\n |      +----+------+----+\n |      \n |      .. versionadded:: 1.4\n |  \n |  rollup(self, *cols)\n |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n |      the specified columns, so we can run aggregation on them.\n |      \n |      &gt;&gt;&gt; df.rollup(&#34;name&#34;, df.age).count().orderBy(&#34;name&#34;, &#34;age&#34;).show()\n |      +-----+----+-----+\n |      | name| age|count|\n |      +-----+----+-----+\n |      | null|null|    2|\n |      |Alice|null|    1|\n |      |Alice|   2|    1|\n |      |  Bob|null|    1|\n |      |  Bob|   5|    1|\n |      +-----+----+-----+\n |      \n |      .. versionadded:: 1.4\n |  \n |  sample(self, withReplacement=None, fraction=None, seed=None)\n |      Returns a sampled subset of this :class:`DataFrame`.\n |      \n |      :param withReplacement: Sample with replacement or not (default ``False``).\n |      :param fraction: Fraction of rows to generate, range [0.0, 1.0].\n |      :param seed: Seed for sampling (default a random seed).\n |      \n |      .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n |          count of the given :class:`DataFrame`.\n |      \n |      .. note:: `fraction` is required and, `withReplacement` and `seed` are optional.\n |      \n |      &gt;&gt;&gt; df = spark.range(10)\n |      &gt;&gt;&gt; df.sample(0.5, 3).count()\n |      7\n |      &gt;&gt;&gt; df.sample(fraction=0.5, seed=3).count()\n |      7\n |      &gt;&gt;&gt; df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n |      1\n |      &gt;&gt;&gt; df.sample(1.0).count()\n |      10\n |      &gt;&gt;&gt; df.sample(fraction=1.0).count()\n |      10\n |      &gt;&gt;&gt; df.sample(False, fraction=1.0).count()\n |      10\n |      \n |      .. versionadded:: 1.3\n |  \n |  sampleBy(self, col, fractions, seed=None)\n |      Returns a stratified sample without replacement based on the\n |      fraction given on each stratum.\n |      \n |      :param col: column that defines strata\n |      :param fractions:\n |          sampling fraction for each stratum. If a stratum is not\n |          specified, we treat its fraction as zero.\n |      :param seed: random seed\n |      :return: a new :class:`DataFrame` that represents the stratified sample\n |      \n |      &gt;&gt;&gt; from pyspark.sql.functions import col\n |      &gt;&gt;&gt; dataset = sqlContext.range(0, 100).select((col(&#34;id&#34;) % 3).alias(&#34;key&#34;))\n |      &gt;&gt;&gt; sampled = dataset.sampleBy(&#34;key&#34;, fractions={0: 0.1, 1: 0.2}, seed=0)\n |      &gt;&gt;&gt; sampled.groupBy(&#34;key&#34;).count().orderBy(&#34;key&#34;).show()\n |      +---+-----+\n |      |key|count|\n |      +---+-----+\n |      |  0|    3|\n |      |  1|    6|\n |      +---+-----+\n |      &gt;&gt;&gt; dataset.sampleBy(col(&#34;key&#34;), fractions={2: 1.0}, seed=0).count()\n |      33\n |      \n |      .. versionchanged:: 3.0\n |         Added sampling by a column of :class:`Column`\n |      \n |      .. versionadded:: 1.5\n |  \n |  select(self, *cols)\n |      Projects a set of expressions and returns a new :class:`DataFrame`.\n |      \n |      :param cols: list of column names (string) or expressions (:class:`Column`).\n |          If one of the column names is &#39;*&#39;, that column is expanded to include all columns\n |          in the current :class:`DataFrame`.\n |      \n |      &gt;&gt;&gt; df.select(&#39;*&#39;).collect()\n |      [Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]\n |      &gt;&gt;&gt; df.select(&#39;name&#39;, &#39;age&#39;).collect()\n |      [Row(name=&#39;Alice&#39;, age=2), Row(name=&#39;Bob&#39;, age=5)]\n |      &gt;&gt;&gt; df.select(df.name, (df.age + 10).alias(&#39;age&#39;)).collect()\n |      [Row(name=&#39;Alice&#39;, age=12), Row(name=&#39;Bob&#39;, age=15)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  selectExpr(self, *expr)\n |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n |      \n |      This is a variant of :func:`select` that accepts SQL expressions.\n |      \n |      &gt;&gt;&gt; df.selectExpr(&#34;age * 2&#34;, &#34;abs(age)&#34;).collect()\n |      [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  show(self, n=20, truncate=True, vertical=False)\n |      Prints the first ``n`` rows to the console.\n |      \n |      :param n: Number of rows to show.\n |      :param truncate: If set to ``True``, truncate strings longer than 20 chars by default.\n |          If set to a number greater than one, truncates long strings to length ``truncate``\n |          and align cells right.\n |      :param vertical: If set to ``True``, print output rows vertically (one line\n |          per column value).\n |      \n |      &gt;&gt;&gt; df\n |      DataFrame[age: int, name: string]\n |      &gt;&gt;&gt; df.show()\n |      +---+-----+\n |      |age| name|\n |      +---+-----+\n |      |  2|Alice|\n |      |  5|  Bob|\n |      +---+-----+\n |      &gt;&gt;&gt; df.show(truncate=3)\n |      +---+----+\n |      |age|name|\n |      +---+----+\n |      |  2| Ali|\n |      |  5| Bob|\n |      +---+----+\n |      &gt;&gt;&gt; df.show(vertical=True)\n |      -RECORD 0-----\n |       age  | 2\n |       name | Alice\n |      -RECORD 1-----\n |       age  | 5\n |       name | Bob\n |      \n |      .. versionadded:: 1.3\n |  \n |  sort(self, *cols, **kwargs)\n |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n |      \n |      :param cols: list of :class:`Column` or column names to sort by.\n |      :param ascending: boolean or list of boolean (default ``True``).\n |          Sort ascending vs. descending. Specify list for multiple sort orders.\n |          If a list is specified, length of the list must equal length of the `cols`.\n |      \n |      &gt;&gt;&gt; df.sort(df.age.desc()).collect()\n |      [Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]\n |      &gt;&gt;&gt; df.sort(&#34;age&#34;, ascending=False).collect()\n |      [Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]\n |      &gt;&gt;&gt; df.orderBy(df.age.desc()).collect()\n |      [Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]\n |      &gt;&gt;&gt; from pyspark.sql.functions import *\n |      &gt;&gt;&gt; df.sort(asc(&#34;age&#34;)).collect()\n |      [Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]\n |      &gt;&gt;&gt; df.orderBy(desc(&#34;age&#34;), &#34;name&#34;).collect()\n |      [Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]\n |      &gt;&gt;&gt; df.orderBy([&#34;age&#34;, &#34;name&#34;], ascending=[0, 1]).collect()\n |      [Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  sortWithinPartitions(self, *cols, **kwargs)\n |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n |      \n |      :param cols: list of :class:`Column` or column names to sort by.\n |      :param ascending: boolean or list of boolean (default ``True``).\n |          Sort ascending vs. descending. Specify list for multiple sort orders.\n |          If a list is specified, length of the list must equal length of the `cols`.\n |      \n |      &gt;&gt;&gt; df.sortWithinPartitions(&#34;age&#34;, ascending=False).show()\n |      +---+-----+\n |      |age| name|\n |      +---+-----+\n |      |  2|Alice|\n |      |  5|  Bob|\n |      +---+-----+\n |      \n |      .. versionadded:: 1.6\n |  \n |  subtract(self, other)\n |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n |      but not in another :class:`DataFrame`.\n |      \n |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n |      \n |      .. versionadded:: 1.3\n |  \n |  summary(self, *statistics)\n |      Computes specified statistics for numeric and string columns. Available statistics are:\n |      - count\n |      - mean\n |      - stddev\n |      - min\n |      - max\n |      - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n |      \n |      If no statistics are given, this function computes count, mean, stddev, min,\n |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n |      \n |      .. note:: This function is meant for exploratory data analysis, as we make no\n |          guarantee about the backward compatibility of the schema of the resulting\n |          :class:`DataFrame`.\n |      \n |      &gt;&gt;&gt; df.summary().show()\n |      +-------+------------------+-----+\n |      |summary|               age| name|\n |      +-------+------------------+-----+\n |      |  count|                 2|    2|\n |      |   mean|               3.5| null|\n |      | stddev|2.1213203435596424| null|\n |      |    min|                 2|Alice|\n |      |    25%|                 2| null|\n |      |    50%|                 2| null|\n |      |    75%|                 5| null|\n |      |    max|                 5|  Bob|\n |      +-------+------------------+-----+\n |      \n |      &gt;&gt;&gt; df.summary(&#34;count&#34;, &#34;min&#34;, &#34;25%&#34;, &#34;75%&#34;, &#34;max&#34;).show()\n |      +-------+---+-----+\n |      |summary|age| name|\n |      +-------+---+-----+\n |      |  count|  2|    2|\n |      |    min|  2|Alice|\n |      |    25%|  2| null|\n |      |    75%|  5| null|\n |      |    max|  5|  Bob|\n |      +-------+---+-----+\n |      \n |      To do a summary for specific columns first select them:\n |      \n |      &gt;&gt;&gt; df.select(&#34;age&#34;, &#34;name&#34;).summary(&#34;count&#34;).show()\n |      +-------+---+----+\n |      |summary|age|name|\n |      +-------+---+----+\n |      |  count|  2|   2|\n |      +-------+---+----+\n |      \n |      See also describe for basic statistics.\n |      \n |      .. versionadded:: 2.3.0\n |  \n |  tail(self, num)\n |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n |      \n |      Running tail requires moving data into the application&#39;s driver process, and doing so with\n |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n |      \n |      &gt;&gt;&gt; df.tail(1)\n |      [Row(age=5, name=&#39;Bob&#39;)]\n |      \n |      .. versionadded:: 3.0\n |  \n |  take(self, num)\n |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n |      \n |      &gt;&gt;&gt; df.take(2)\n |      [Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  toDF(self, *cols)\n |      Returns a new :class:`DataFrame` that with new specified column names\n |      \n |      :param cols: list of new column names (string)\n |      \n |      &gt;&gt;&gt; df.toDF(&#39;f1&#39;, &#39;f2&#39;).collect()\n |      [Row(f1=2, f2=&#39;Alice&#39;), Row(f1=5, f2=&#39;Bob&#39;)]\n |  \n |  toJSON(self, use_unicode=True)\n |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n |      \n |      Each row is turned into a JSON document as one element in the returned RDD.\n |      \n |      &gt;&gt;&gt; df.toJSON().first()\n |      &#39;{&#34;age&#34;:2,&#34;name&#34;:&#34;Alice&#34;}&#39;\n |      \n |      .. versionadded:: 1.3\n |  \n |  toLocalIterator(self, prefetchPartitions=False)\n |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n |      The iterator will consume as much memory as the largest partition in this\n |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n |      partitions.\n |      \n |      :param prefetchPartitions: If Spark should pre-fetch the next partition\n |                                 before it is needed.\n |      \n |      &gt;&gt;&gt; list(df.toLocalIterator())\n |      [Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]\n |      \n |      .. versionadded:: 2.0\n |  \n |  transform(self, func)\n |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n |      \n |      :param func: a function that takes and returns a :class:`DataFrame`.\n |      \n |      &gt;&gt;&gt; from pyspark.sql.functions import col\n |      &gt;&gt;&gt; df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [&#34;int&#34;, &#34;float&#34;])\n |      &gt;&gt;&gt; def cast_all_to_int(input_df):\n |      ...     return input_df.select([col(col_name).cast(&#34;int&#34;) for col_name in input_df.columns])\n |      &gt;&gt;&gt; def sort_columns_asc(input_df):\n |      ...     return input_df.select(*sorted(input_df.columns))\n |      &gt;&gt;&gt; df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n |      +-----+---+\n |      |float|int|\n |      +-----+---+\n |      |    1|  1|\n |      |    2|  2|\n |      +-----+---+\n |      \n |      .. versionadded:: 3.0\n |  \n |  union(self, other)\n |      Return a new :class:`DataFrame` containing union of rows in this and another\n |      :class:`DataFrame`.\n |      \n |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n |      \n |      Also as standard in SQL, this function resolves columns by position (not by name).\n |      \n |      .. versionadded:: 2.0\n |  \n |  unionAll(self, other)\n |      Return a new :class:`DataFrame` containing union of rows in this and another\n |      :class:`DataFrame`.\n |      \n |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n |      \n |      Also as standard in SQL, this function resolves columns by position (not by name).\n |      \n |      .. versionadded:: 1.3\n |  \n |  unionByName(self, other)\n |      Returns a new :class:`DataFrame` containing union of rows in this and another\n |      :class:`DataFrame`.\n |      \n |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n |      \n |      The difference between this function and :func:`union` is that this function\n |      resolves columns by name (not by position):\n |      \n |      &gt;&gt;&gt; df1 = spark.createDataFrame([[1, 2, 3]], [&#34;col0&#34;, &#34;col1&#34;, &#34;col2&#34;])\n |      &gt;&gt;&gt; df2 = spark.createDataFrame([[4, 5, 6]], [&#34;col1&#34;, &#34;col2&#34;, &#34;col0&#34;])\n |      &gt;&gt;&gt; df1.unionByName(df2).show()\n |      +----+----+----+\n |      |col0|col1|col2|\n |      +----+----+----+\n |      |   1|   2|   3|\n |      |   6|   4|   5|\n |      +----+----+----+\n |      \n |      .. versionadded:: 2.3\n |  \n |  unpersist(self, blocking=False)\n |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n |      memory and disk.\n |      \n |      .. note:: `blocking` default has changed to ``False`` to match Scala in 2.0.\n |      \n |      .. versionadded:: 1.3\n |  \n |  where = filter(self, condition)\n |      :func:`where` is an alias for :func:`filter`.\n |      \n |      .. versionadded:: 1.3\n |  \n |  withColumn(self, colName, col)\n |      Returns a new :class:`DataFrame` by adding a column or replacing the\n |      existing column that has the same name.\n |      \n |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n |      a column from some other :class:`DataFrame` will raise an error.\n |      \n |      :param colName: string, name of the new column.\n |      :param col: a :class:`Column` expression for the new column.\n |      \n |      .. note:: This method introduces a projection internally. Therefore, calling it multiple\n |          times, for instance, via loops in order to add multiple columns can generate big\n |          plans which can cause performance issues and even `StackOverflowException`.\n |          To avoid this, use :func:`select` with the multiple columns at once.\n |      \n |      &gt;&gt;&gt; df.withColumn(&#39;age2&#39;, df.age + 2).collect()\n |      [Row(age=2, name=&#39;Alice&#39;, age2=4), Row(age=5, name=&#39;Bob&#39;, age2=7)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  withColumnRenamed(self, existing, new)\n |      Returns a new :class:`DataFrame` by renaming an existing column.\n |      This is a no-op if schema doesn&#39;t contain the given column name.\n |      \n |      :param existing: string, name of the existing column to rename.\n |      :param new: string, new name of the column.\n |      \n |      &gt;&gt;&gt; df.withColumnRenamed(&#39;age&#39;, &#39;age2&#39;).collect()\n |      [Row(age2=2, name=&#39;Alice&#39;), Row(age2=5, name=&#39;Bob&#39;)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  withWatermark(self, eventTime, delayThreshold)\n |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n |      in time before which we assume no more late data is going to arrive.\n |      \n |      Spark will use this watermark for several purposes:\n |        - To know when a given time window aggregation can be finalized and thus can be emitted\n |          when using output modes that do not allow updates.\n |      \n |        - To minimize the amount of state that we need to keep for on-going aggregations.\n |      \n |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n |      process records that arrive more than `delayThreshold` late.\n |      \n |      :param eventTime: the name of the column that contains the event time of the row.\n |      :param delayThreshold: the minimum delay to wait to data to arrive late, relative to the\n |          latest record that has been processed in the form of an interval\n |          (e.g. &#34;1 minute&#34; or &#34;5 hours&#34;).\n |      \n |      .. note:: Evolving\n |      \n |      &gt;&gt;&gt; sdf.select(&#39;name&#39;, sdf.time.cast(&#39;timestamp&#39;)).withWatermark(&#39;time&#39;, &#39;10 minutes&#39;)\n |      DataFrame[name: string, time: timestamp]\n |      \n |      .. versionadded:: 2.1\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  columns\n |      Returns all column names as a list.\n |      \n |      &gt;&gt;&gt; df.columns\n |      [&#39;age&#39;, &#39;name&#39;]\n |      \n |      .. versionadded:: 1.3\n |  \n |  dtypes\n |      Returns all column names and their data types as a list.\n |      \n |      &gt;&gt;&gt; df.dtypes\n |      [(&#39;age&#39;, &#39;int&#39;), (&#39;name&#39;, &#39;string&#39;)]\n |      \n |      .. versionadded:: 1.3\n |  \n |  isStreaming\n |      Returns ``True`` if this :class:`Dataset` contains one or more sources that continuously\n |      return data as it arrives. A :class:`Dataset` that reads data from a streaming source\n |      must be executed as a :class:`StreamingQuery` using the :func:`start` method in\n |      :class:`DataStreamWriter`.  Methods that return a single answer, (e.g., :func:`count` or\n |      :func:`collect`) will throw an :class:`AnalysisException` when there is a streaming\n |      source present.\n |      \n |      .. note:: Evolving\n |      \n |      .. versionadded:: 2.0\n |  \n |  na\n |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n |      \n |      .. versionadded:: 1.3.1\n |  \n |  rdd\n |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n |      \n |      .. versionadded:: 1.3\n |  \n |  schema\n |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n |      \n |      &gt;&gt;&gt; df.schema\n |      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n |      \n |      .. versionadded:: 1.3\n |  \n |  stat\n |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n |      \n |      .. versionadded:: 1.4\n |  \n |  storageLevel\n |      Get the :class:`DataFrame`&#39;s current storage level.\n |      \n |      &gt;&gt;&gt; df.storageLevel\n |      StorageLevel(False, False, False, False, 1)\n |      &gt;&gt;&gt; df.cache().storageLevel\n |      StorageLevel(True, True, False, True, 1)\n |      &gt;&gt;&gt; df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n |      StorageLevel(True, False, False, False, 2)\n |      \n |      .. versionadded:: 2.1\n |  \n |  write\n |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n |      storage.\n |      \n |      :return: :class:`DataFrameWriter`\n |      \n |      .. versionadded:: 1.4\n |  \n |  writeStream\n |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n |      storage.\n |      \n |      .. note:: Evolving.\n |      \n |      :return: :class:`DataStreamWriter`\n |      \n |      .. versionadded:: 2.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n |  \n |  mapInPandas(self, func, schema)\n |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n |      function that takes and outputs a pandas DataFrame, and returns the result as a\n |      :class:`DataFrame`.\n |      \n |      The function should take an iterator of `pandas.DataFrame`\\s and return\n |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n |      Each `pandas.DataFrame` size can be controlled by\n |      `spark.sql.execution.arrow.maxRecordsPerBatch`.\n |      \n |      :param func: a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n |          outputs an iterator of `pandas.DataFrame`\\s.\n |      :param schema: the return type of the `func` in PySpark. The value can be either a\n |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n |      \n |      &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf\n |      &gt;&gt;&gt; df = spark.createDataFrame([(1, 21), (2, 30)], (&#34;id&#34;, &#34;age&#34;))\n |      &gt;&gt;&gt; def filter_func(iterator):\n |      ...     for pdf in iterator:\n |      ...         yield pdf[pdf.id == 1]\n |      &gt;&gt;&gt; df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n |      +---+---+\n |      | id|age|\n |      +---+---+\n |      |  1| 21|\n |      +---+---+\n |      \n |      .. seealso:: :meth:`pyspark.sql.functions.pandas_udf`\n |      \n |      .. note:: Experimental\n |      \n |      .. versionadded:: 3.0\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n |  \n |  toPandas(self)\n |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n |      \n |      This is only available if Pandas is installed and available.\n |      \n |      .. note:: This method should only be used if the resulting Pandas&#39;s :class:`DataFrame` is\n |          expected to be small, as all the data is loaded into the driver&#39;s memory.\n |      \n |      .. note:: Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n |      \n |      &gt;&gt;&gt; df.toPandas()  # doctest: +SKIP\n |         age   name\n |      0    2  Alice\n |      1    5    Bob\n |      \n |      .. versionadded:: 1.3\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on DataFrame in module pyspark.sql.dataframe object:\n\nclass DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n  DataFrame(jdf, sql_ctx)\n  \n  A distributed collection of data grouped into named columns.\n  \n  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n  and can be created using various functions in :class:`SparkSession`::\n  \n      people = spark.read.parquet(&#34;...&#34;)\n  \n  Once created, it can be manipulated using the various domain-specific-language\n  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n  \n  To select a column from the :class:`DataFrame`, use the apply method::\n  \n      ageCol = people.age\n  \n  A more concrete example::\n  \n      # To create DataFrame using SparkSession\n      people = spark.read.parquet(&#34;...&#34;)\n      department = spark.read.parquet(&#34;...&#34;)\n  \n      people.filter(people.age &gt; 30).join(department, people.deptId == department.id) \\\n        .groupBy(department.name, &#34;gender&#34;).agg({&#34;salary&#34;: &#34;avg&#34;, &#34;age&#34;: &#34;max&#34;})\n  \n  .. versionadded:: 1.3\n  \n  Method resolution order:\n      DataFrame\n      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n      pyspark.sql.pandas.conversion.PandasConversionMixin\n      builtins.object\n  \n  Methods defined here:\n  \n  __getattr__(self, name)\n      Returns the :class:`Column` denoted by ``name``.\n      \n      &gt;&gt;&gt; df.select(df.age).collect()\n      [Row(age=2), Row(age=5)]\n      \n      .. versionadded:: 1.3\n  \n  __getitem__(self, item)\n      Returns the column as a :class:`Column`.\n      \n      &gt;&gt;&gt; df.select(df[&#39;age&#39;]).collect()\n      [Row(age=2), Row(age=5)]\n      &gt;&gt;&gt; df[ [&#34;name&#34;, &#34;age&#34;]].collect()\n      [Row(name=&#39;Alice&#39;, age=2), Row(name=&#39;Bob&#39;, age=5)]\n      &gt;&gt;&gt; df[ df.age &gt; 3 ].collect()\n      [Row(age=5, name=&#39;Bob&#39;)]\n      &gt;&gt;&gt; df[df[0] &gt; 3].collect()\n      [Row(age=5, name=&#39;Bob&#39;)]\n      \n      .. versionadded:: 1.3\n  \n  __init__(self, jdf, sql_ctx)\n      Initialize self.  See help(type(self)) for accurate signature.\n  \n  __repr__(self)\n      Return repr(self).\n  \n  agg(self, *exprs)\n      Aggregate on the entire :class:`DataFrame` without groups\n      (shorthand for ``df.groupBy.agg()``).\n      \n      &gt;&gt;&gt; df.agg({&#34;age&#34;: &#34;max&#34;}).collect()\n      [Row(max(age)=5)]\n      &gt;&gt;&gt; from pyspark.sql import functions as F\n      &gt;&gt;&gt; df.agg(F.min(df.age)).collect()\n      [Row(min(age)=2)]\n      \n      .. versionadded:: 1.3\n  \n  alias(self, alias)\n      Returns a new :class:`DataFrame` with an alias set.\n      \n      :param alias: string, an alias name to be set for the :class:`DataFrame`.\n      \n      &gt;&gt;&gt; from pyspark.sql.functions import *\n      &gt;&gt;&gt; df_as1 = df.alias(&#34;df_as1&#34;)\n      &gt;&gt;&gt; df_as2 = df.alias(&#34;df_as2&#34;)\n      &gt;&gt;&gt; joined_df = df_as1.join(df_as2, col(&#34;df_as1.name&#34;) == col(&#34;df_as2.name&#34;), &#39;inner&#39;)\n      &gt;&gt;&gt; joined_df.select(&#34;df_as1.name&#34;, &#34;df_as2.name&#34;, &#34;df_as2.age&#34;)                 .sort(desc(&#34;df_as1.name&#34;)).collect()\n      [Row(name=&#39;Bob&#39;, name=&#39;Bob&#39;, age=5), Row(name=&#39;Alice&#39;, name=&#39;Alice&#39;, age=2)]\n      \n      .. versionadded:: 1.3\n  \n  approxQuantile(self, col, probabilities, relativeError)\n      Calculates the approximate quantiles of numerical columns of a\n      :class:`DataFrame`.\n      \n      The result of this algorithm has the following deterministic bound:\n      If the :class:`DataFrame` has N elements and if we request the quantile at\n      probability `p` up to error `err`, then the algorithm will return\n      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n      close to (p * N). More precisely,\n      \n        floor((p - err) * N) &lt;= rank(x) &lt;= ceil((p + err) * N).\n      \n      This method implements a variation of the Greenwald-Khanna\n      algorithm (with some speed optimizations). The algorithm was first\n      present in [[https://doi.org/10.1145/375663.375670\n      Space-efficient Online Computation of Quantile Summaries]]\n      by Greenwald and Khanna.\n      \n      Note that null values will be ignored in numerical columns before calculation.\n      For columns only containing null values, an empty list is returned.\n      \n      :param col: str, list.\n        Can be a single column name, or a list of names for multiple columns.\n      :param probabilities: a list of quantile probabilities\n        Each number must belong to [0, 1].\n        For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n      :param relativeError:  The relative target precision to achieve\n        (&gt;= 0). If set to zero, the exact quantiles are computed, which\n        could be very expensive. Note that values greater than 1 are\n        accepted but give the same result as 1.\n      :return:  the approximate quantiles at the given probabilities. If\n        the input `col` is a string, the output is a list of floats. If the\n        input `col` is a list or tuple of strings, the output is also a\n        list, but each element in it is a list of floats, i.e., the output\n        is a list of list of floats.\n      \n      .. versionchanged:: 2.2\n         Added support for multiple columns.\n      \n      .. versionadded:: 2.0\n  \n  cache(self)\n      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n      \n      .. note:: The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n      \n      .. versionadded:: 1.3\n  \n  checkpoint(self, eager=True)\n      Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n      logical plan of this :class:`DataFrame`, which is especially useful in iterative algorithms\n      where the plan may grow exponentially. It will be saved to files inside the checkpoint\n      directory set with :meth:`SparkContext.setCheckpointDir`.\n      \n      :param eager: Whether to checkpoint this :class:`DataFrame` immediately\n      \n      .. note:: Experimental\n      \n      .. versionadded:: 2.1\n  \n  coalesce(self, numPartitions)\n      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n      \n      :param numPartitions: int, to specify the target number of partitions\n      \n      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n      there will not be a shuffle, instead each of the 100 new partitions will\n      claim 10 of the current partitions. If a larger number of partitions is requested,\n      it will stay at the current number of partitions.\n      \n      However, if you&#39;re doing a drastic coalesce, e.g. to numPartitions = 1,\n      this may result in your computation taking place on fewer nodes than\n      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n      you can call repartition(). This will add a shuffle step, but means the\n      current upstream partitions will be executed in parallel (per whatever\n      the current partitioning is).\n      \n      &gt;&gt;&gt; df.coalesce(1).rdd.getNumPartitions()\n      1\n      \n      .. versionadded:: 1.4\n  \n  colRegex(self, colName)\n      Selects column based on the column name specified as a regex and returns it\n      as :class:`Column`.\n      \n      :param colName: string, column name specified as a regex.\n      \n      &gt;&gt;&gt; df = spark.createDataFrame([(&#34;a&#34;, 1), (&#34;b&#34;, 2), (&#34;c&#34;,  3)], [&#34;Col1&#34;, &#34;Col2&#34;])\n      &gt;&gt;&gt; df.select(df.colRegex(&#34;`(Col1)?+.+`&#34;)).show()\n      +----+\n      |Col2|\n      +----+\n      |   1|\n      |   2|\n      |   3|\n      +----+\n      \n      .. versionadded:: 2.3\n  \n  collect(self)\n      Returns all the records as a list of :class:`Row`.\n      \n      &gt;&gt;&gt; df.collect()\n      [Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]\n      \n      .. versionadded:: 1.3\n  \n  corr(self, col1, col2, method=None)\n      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n      Currently only supports the Pearson Correlation Coefficient.\n      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n      \n      :param col1: The name of the first column\n      :param col2: The name of the second column\n      :param method: The correlation method. Currently only supports &#34;pearson&#34;\n      \n      .. versionadded:: 1.4\n  \n  count(self)\n      Returns the number of rows in this :class:`DataFrame`.\n      \n      &gt;&gt;&gt; df.count()\n      2\n      \n      .. versionadded:: 1.3\n  \n  cov(self, col1, col2)\n      Calculate the sample covariance for the given columns, specified by their names, as a\n      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n      \n      :param col1: The name of the first column\n      :param col2: The name of the second column\n      \n      .. versionadded:: 1.4\n  \n  createGlobalTempView(self, name)\n      Creates a global temporary view with this :class:`DataFrame`.\n      \n      The lifetime of this temporary view is tied to this Spark application.\n      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n      catalog.\n      \n      &gt;&gt;&gt; df.createGlobalTempView(&#34;people&#34;)\n      &gt;&gt;&gt; df2 = spark.sql(&#34;select * from global_temp.people&#34;)\n      &gt;&gt;&gt; sorted(df.collect()) == sorted(df2.collect())\n      True\n      &gt;&gt;&gt; df.createGlobalTempView(&#34;people&#34;)  # doctest: +IGNORE_EXCEPTION_DETAIL\n      Traceback (most recent call last):\n      ...\n      AnalysisException: u&#34;Temporary table &#39;people&#39; already exists;&#34;\n      &gt;&gt;&gt; spark.catalog.dropGlobalTempView(&#34;people&#34;)\n      \n      .. versionadded:: 2.1\n  \n  createOrReplaceGlobalTempView(self, name)\n      Creates or replaces a global temporary view using the given name.\n      \n      The lifetime of this temporary view is tied to this Spark application.\n      \n      &gt;&gt;&gt; df.createOrReplaceGlobalTempView(&#34;people&#34;)\n      &gt;&gt;&gt; df2 = df.filter(df.age &gt; 3)\n      &gt;&gt;&gt; df2.createOrReplaceGlobalTempView(&#34;people&#34;)\n      &gt;&gt;&gt; df3 = spark.sql(&#34;select * from global_temp.people&#34;)\n      &gt;&gt;&gt; sorted(df3.collect()) == sorted(df2.collect())\n      True\n      &gt;&gt;&gt; spark.catalog.dropGlobalTempView(&#34;people&#34;)\n      \n      .. versionadded:: 2.2\n  \n  createOrReplaceTempView(self, name)\n      Creates or replaces a local temporary view with this :class:`DataFrame`.\n      \n      The lifetime of this temporary table is tied to the :class:`SparkSession`\n      that was used to create this :class:`DataFrame`.\n      \n      &gt;&gt;&gt; df.createOrReplaceTempView(&#34;people&#34;)\n      &gt;&gt;&gt; df2 = df.filter(df.age &gt; 3)\n      &gt;&gt;&gt; df2.createOrReplaceTempView(&#34;people&#34;)\n      &gt;&gt;&gt; df3 = spark.sql(&#34;select * from people&#34;)\n      &gt;&gt;&gt; sorted(df3.collect()) == sorted(df2.collect())\n      True\n      &gt;&gt;&gt; spark.catalog.dropTempView(&#34;people&#34;)\n      \n      .. versionadded:: 2.0\n  \n  createTempView(self, name)\n      Creates a local temporary view with this :class:`DataFrame`.\n      \n      The lifetime of this temporary table is tied to the :class:`SparkSession`\n      that was used to create this :class:`DataFrame`.\n      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n      catalog.\n      \n      &gt;&gt;&gt; df.createTempView(&#34;people&#34;)\n      &gt;&gt;&gt; df2 = spark.sql(&#34;select * from people&#34;)\n      &gt;&gt;&gt; sorted(df.collect()) == sorted(df2.collect())\n      True\n      &gt;&gt;&gt; df.createTempView(&#34;people&#34;)  # doctest: +IGNORE_EXCEPTION_DETAIL\n      Traceback (most recent call last):\n      ...\n      AnalysisException: u&#34;Temporary table &#39;people&#39; already exists;&#34;\n      &gt;&gt;&gt; spark.catalog.dropTempView(&#34;people&#34;)\n      \n      .. versionadded:: 2.0\n  \n  crossJoin(self, other)\n      Returns the cartesian product with another :class:`DataFrame`.\n      \n      :param other: Right side of the cartesian product.\n      \n      &gt;&gt;&gt; df.select(&#34;age&#34;, &#34;name&#34;).collect()\n      [Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]\n      &gt;&gt;&gt; df2.select(&#34;name&#34;, &#34;height&#34;).collect()\n      [Row(name=&#39;Tom&#39;, height=80), Row(name=&#39;Bob&#39;, height=85)]\n      &gt;&gt;&gt; df.crossJoin(df2.select(&#34;height&#34;)).select(&#34;age&#34;, &#34;name&#34;, &#34;height&#34;).collect()\n      [Row(age=2, name=&#39;Alice&#39;, height=80), Row(age=2, name=&#39;Alice&#39;, height=85),\n       Row(age=5, name=&#39;Bob&#39;, height=80), Row(age=5, name=&#39;Bob&#39;, height=85)]\n      \n      .. versionadded:: 2.1\n  \n  crosstab(self, col1, col2)\n      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n      non-zero pair frequencies will be returned.\n      The first column of each row will be the distinct values of `col1` and the column names\n      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n      Pairs that have no occurrences will have zero as their counts.\n      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n      \n      :param col1: The name of the first column. Distinct items will make the first item of\n          each row.\n      :param col2: The name of the second column. Distinct items will make the column names\n          of the :class:`DataFrame`.\n      \n      .. versionadded:: 1.4\n  \n  cube(self, *cols)\n      Create a multi-dimensional cube for the current :class:`DataFrame` using\n      the specified columns, so we can run aggregations on them.\n      \n      &gt;&gt;&gt; df.cube(&#34;name&#34;, df.age).count().orderBy(&#34;name&#34;, &#34;age&#34;).show()\n      +-----+----+-----+\n      | name| age|count|\n      +-----+----+-----+\n      | null|null|    2|\n      | null|   2|    1|\n      | null|   5|    1|\n      |Alice|null|    1|\n      |Alice|   2|    1|\n      |  Bob|null|    1|\n      |  Bob|   5|    1|\n      +-----+----+-----+\n      \n      .. versionadded:: 1.4\n  \n  describe(self, *cols)\n      Computes basic statistics for numeric and string columns.\n      \n      This include count, mean, stddev, min, and max. If no columns are\n      given, this function computes statistics for all numerical or string columns.\n      \n      .. note:: This function is meant for exploratory data analysis, as we make no\n          guarantee about the backward compatibility of the schema of the resulting\n          :class:`DataFrame`.\n      \n      &gt;&gt;&gt; df.describe([&#39;age&#39;]).show()\n      +-------+------------------+\n      |summary|               age|\n      +-------+------------------+\n      |  count|                 2|\n      |   mean|               3.5|\n      | stddev|2.1213203435596424|\n      |    min|                 2|\n      |    max|                 5|\n      +-------+------------------+\n      &gt;&gt;&gt; df.describe().show()\n      +-------+------------------+-----+\n      |summary|               age| name|\n      +-------+------------------+-----+\n      |  count|                 2|    2|\n      |   mean|               3.5| null|\n      | stddev|2.1213203435596424| null|\n      |    min|                 2|Alice|\n      |    max|                 5|  Bob|\n      +-------+------------------+-----+\n      \n      Use summary for expanded statistics and control over which statistics to compute.\n      \n      .. versionadded:: 1.3.1\n  \n  display = df_display(df, *args, **kwargs)\n      df.display() is an alias for display(df). Run help(display) for more information.\n  \n  distinct(self)\n      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n      \n      &gt;&gt;&gt; df.distinct().count()\n      2\n      \n      .. versionadded:: 1.3\n  \n  drop(self, *cols)\n      Returns a new :class:`DataFrame` that drops the specified column.\n      This is a no-op if schema doesn&#39;t contain the given column name(s).\n      \n      :param cols: a string name of the column to drop, or a\n          :class:`Column` to drop, or a list of string name of the columns to drop.\n      \n      &gt;&gt;&gt; df.drop(&#39;age&#39;).collect()\n      [Row(name=&#39;Alice&#39;), Row(name=&#39;Bob&#39;)]\n      \n      &gt;&gt;&gt; df.drop(df.age).collect()\n      [Row(name=&#39;Alice&#39;), Row(name=&#39;Bob&#39;)]\n      \n      &gt;&gt;&gt; df.join(df2, df.name == df2.name, &#39;inner&#39;).drop(df.name).collect()\n      [Row(age=5, height=85, name=&#39;Bob&#39;)]\n      \n      &gt;&gt;&gt; df.join(df2, df.name == df2.name, &#39;inner&#39;).drop(df2.name).collect()\n      [Row(age=5, name=&#39;Bob&#39;, height=85)]\n      \n      &gt;&gt;&gt; df.join(df2, &#39;name&#39;, &#39;inner&#39;).drop(&#39;age&#39;, &#39;height&#39;).collect()\n      [Row(name=&#39;Bob&#39;)]\n      \n      .. versionadded:: 1.4\n  \n  dropDuplicates(self, subset=None)\n      Return a new :class:`DataFrame` with duplicate rows removed,\n      optionally only considering certain columns.\n      \n      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n      be and system will accordingly limit the state. In addition, too late data older than\n      watermark will be dropped to avoid any possibility of duplicates.\n      \n      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n      \n      &gt;&gt;&gt; from pyspark.sql import Row\n      &gt;&gt;&gt; df = sc.parallelize([ \\\n      ...     Row(name=&#39;Alice&#39;, age=5, height=80), \\\n      ...     Row(name=&#39;Alice&#39;, age=5, height=80), \\\n      ...     Row(name=&#39;Alice&#39;, age=10, height=80)]).toDF()\n      &gt;&gt;&gt; df.dropDuplicates().show()\n      +---+------+-----+\n      |age|height| name|\n      +---+------+-----+\n      |  5|    80|Alice|\n      | 10|    80|Alice|\n      +---+------+-----+\n      \n      &gt;&gt;&gt; df.dropDuplicates([&#39;name&#39;, &#39;height&#39;]).show()\n      +---+------+-----+\n      |age|height| name|\n      +---+------+-----+\n      |  5|    80|Alice|\n      +---+------+-----+\n      \n      .. versionadded:: 1.4\n  \n  drop_duplicates = dropDuplicates(self, subset=None)\n      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n      \n      .. versionadded:: 1.4\n  \n  dropna(self, how=&#39;any&#39;, thresh=None, subset=None)\n      Returns a new :class:`DataFrame` omitting rows with null values.\n      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n      \n      :param how: &#39;any&#39; or &#39;all&#39;.\n          If &#39;any&#39;, drop a row if it contains any nulls.\n          If &#39;all&#39;, drop a row only if all its values are null.\n      :param thresh: int, default None\n          If specified, drop rows that have less than `thresh` non-null values.\n          This overwrites the `how` parameter.\n      :param subset: optional list of column names to consider.\n      \n      &gt;&gt;&gt; df4.na.drop().show()\n      +---+------+-----+\n      |age|height| name|\n      +---+------+-----+\n      | 10|    80|Alice|\n      +---+------+-----+\n      \n      .. versionadded:: 1.3.1\n  \n  exceptAll(self, other)\n      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n      not in another :class:`DataFrame` while preserving duplicates.\n      \n      This is equivalent to `EXCEPT ALL` in SQL.\n      \n      &gt;&gt;&gt; df1 = spark.createDataFrame(\n      ...         [(&#34;a&#34;, 1), (&#34;a&#34;, 1), (&#34;a&#34;, 1), (&#34;a&#34;, 2), (&#34;b&#34;,  3), (&#34;c&#34;, 4)], [&#34;C1&#34;, &#34;C2&#34;])\n      &gt;&gt;&gt; df2 = spark.createDataFrame([(&#34;a&#34;, 1), (&#34;b&#34;, 3)], [&#34;C1&#34;, &#34;C2&#34;])\n      \n      &gt;&gt;&gt; df1.exceptAll(df2).show()\n      +---+---+\n      | C1| C2|\n      +---+---+\n      |  a|  1|\n      |  a|  1|\n      |  a|  2|\n      |  c|  4|\n      +---+---+\n      \n      Also as standard in SQL, this function resolves columns by position (not by name).\n      \n      .. versionadded:: 2.4\n  \n  explain(self, extended=None, mode=None)\n      Prints the (logical and physical) plans to the console for debugging purpose.\n      \n      :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.\n          When this is a string without specifying the ``mode``, it works as the mode is\n          specified.\n      :param mode: specifies the expected output format of plans.\n      \n          * ``simple``: Print only a physical plan.\n          * ``extended``: Print both logical and physical plans.\n          * ``codegen``: Print a physical plan and generated codes if they are available.\n          * ``cost``: Print a logical plan and statistics if they are available.\n          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n      \n      &gt;&gt;&gt; df.explain()\n      == Physical Plan ==\n      *(1) Scan ExistingRDD[age#0,name#1]\n      \n      &gt;&gt;&gt; df.explain(True)\n      == Parsed Logical Plan ==\n      ...\n      == Analyzed Logical Plan ==\n      ...\n      == Optimized Logical Plan ==\n      ...\n      == Physical Plan ==\n      ...\n      \n      &gt;&gt;&gt; df.explain(mode=&#34;formatted&#34;)\n      == Physical Plan ==\n      * Scan ExistingRDD (1)\n      (1) Scan ExistingRDD [codegen id : 1]\n      Output [2]: [age#0, name#1]\n      ...\n      \n      &gt;&gt;&gt; df.explain(&#34;cost&#34;)\n      == Optimized Logical Plan ==\n      ...Statistics...\n      ...\n      \n      .. versionchanged:: 3.0.0\n         Added optional argument `mode` to specify the expected output format of plans.\n      \n      .. versionadded:: 1.3\n  \n  fillna(self, value, subset=None)\n      Replace null values, alias for ``na.fill()``.\n      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n      \n      :param value: int, long, float, string, bool or dict.\n          Value to replace null values with.\n          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n          from column name (string) to replacement value. The replacement value must be\n          an int, long, float, boolean, or string.\n      :param subset: optional list of column names to consider.\n          Columns specified in subset that do not have matching data type are ignored.\n          For example, if `value` is a string, and subset contains a non-string column,\n          then the non-string column is simply ignored.\n      \n      &gt;&gt;&gt; df4.na.fill(50).show()\n      +---+------+-----+\n      |age|height| name|\n      +---+------+-----+\n      | 10|    80|Alice|\n      |  5|    50|  Bob|\n      | 50|    50|  Tom|\n      | 50|    50| null|\n      +---+------+-----+\n      \n      &gt;&gt;&gt; df5.na.fill(False).show()\n      +----+-------+-----+\n      | age|   name|  spy|\n      +----+-------+-----+\n      |  10|  Alice|false|\n      |   5|    Bob|false|\n      |null|Mallory| true|\n      +----+-------+-----+\n      \n      &gt;&gt;&gt; df4.na.fill({&#39;age&#39;: 50, &#39;name&#39;: &#39;unknown&#39;}).show()\n      +---+------+-------+\n      |age|height|   name|\n      +---+------+-------+\n      | 10|    80|  Alice|\n      |  5|  null|    Bob|\n      | 50|  null|    Tom|\n      | 50|  null|unknown|\n      +---+------+-------+\n      \n      .. versionadded:: 1.3.1\n  \n  filter(self, condition)\n      Filters rows using the given condition.\n      \n      :func:`where` is an alias for :func:`filter`.\n      \n      :param condition: a :class:`Column` of :class:`types.BooleanType`\n          or a string of SQL expression.\n      \n      &gt;&gt;&gt; df.filter(df.age &gt; 3).collect()\n      [Row(age=5, name=&#39;Bob&#39;)]\n      &gt;&gt;&gt; df.where(df.age == 2).collect()\n      [Row(age=2, name=&#39;Alice&#39;)]\n      \n      &gt;&gt;&gt; df.filter(&#34;age &gt; 3&#34;).collect()\n      [Row(age=5, name=&#39;Bob&#39;)]\n      &gt;&gt;&gt; df.where(&#34;age = 2&#34;).collect()\n      [Row(age=2, name=&#39;Alice&#39;)]\n      \n      .. versionadded:: 1.3\n  \n  first(self)\n      Returns the first row as a :class:`Row`.\n      \n      &gt;&gt;&gt; df.first()\n      Row(age=2, name=&#39;Alice&#39;)\n      \n      .. versionadded:: 1.3\n  \n  foreach(self, f)\n      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n      \n      This is a shorthand for ``df.rdd.foreach()``.\n      \n      &gt;&gt;&gt; def f(person):\n      ...     print(person.name)\n      &gt;&gt;&gt; df.foreach(f)\n      \n      .. versionadded:: 1.3\n  \n  foreachPartition(self, f)\n      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n      \n      This a shorthand for ``df.rdd.foreachPartition()``.\n      \n      &gt;&gt;&gt; def f(people):\n      ...     for person in people:\n      ...         print(person.name)\n      &gt;&gt;&gt; df.foreachPartition(f)\n\n*** WARNING: skipped 12445 bytes of output ***\n\n      or strings. Value can have None. When replacing, the new value will be cast\n      to the type of the existing column.\n      For numeric replacements all values to be replaced should have unique\n      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n      and arbitrary replacement will be used.\n      \n      :param to_replace: bool, int, long, float, string, list or dict.\n          Value to be replaced.\n          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n          must be a mapping between a value and a replacement.\n      :param value: bool, int, long, float, string, list or None.\n          The replacement value must be a bool, int, long, float, string or None. If `value` is a\n          list, `value` should be of the same length and type as `to_replace`.\n          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n          used as a replacement for each item in `to_replace`.\n      :param subset: optional list of column names to consider.\n          Columns specified in subset that do not have matching data type are ignored.\n          For example, if `value` is a string, and subset contains a non-string column,\n          then the non-string column is simply ignored.\n      \n      &gt;&gt;&gt; df4.na.replace(10, 20).show()\n      +----+------+-----+\n      | age|height| name|\n      +----+------+-----+\n      |  20|    80|Alice|\n      |   5|  null|  Bob|\n      |null|  null|  Tom|\n      |null|  null| null|\n      +----+------+-----+\n      \n      &gt;&gt;&gt; df4.na.replace(&#39;Alice&#39;, None).show()\n      +----+------+----+\n      | age|height|name|\n      +----+------+----+\n      |  10|    80|null|\n      |   5|  null| Bob|\n      |null|  null| Tom|\n      |null|  null|null|\n      +----+------+----+\n      \n      &gt;&gt;&gt; df4.na.replace({&#39;Alice&#39;: None}).show()\n      +----+------+----+\n      | age|height|name|\n      +----+------+----+\n      |  10|    80|null|\n      |   5|  null| Bob|\n      |null|  null| Tom|\n      |null|  null|null|\n      +----+------+----+\n      \n      &gt;&gt;&gt; df4.na.replace([&#39;Alice&#39;, &#39;Bob&#39;], [&#39;A&#39;, &#39;B&#39;], &#39;name&#39;).show()\n      +----+------+----+\n      | age|height|name|\n      +----+------+----+\n      |  10|    80|   A|\n      |   5|  null|   B|\n      |null|  null| Tom|\n      |null|  null|null|\n      +----+------+----+\n      \n      .. versionadded:: 1.4\n  \n  rollup(self, *cols)\n      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n      the specified columns, so we can run aggregation on them.\n      \n      &gt;&gt;&gt; df.rollup(&#34;name&#34;, df.age).count().orderBy(&#34;name&#34;, &#34;age&#34;).show()\n      +-----+----+-----+\n      | name| age|count|\n      +-----+----+-----+\n      | null|null|    2|\n      |Alice|null|    1|\n      |Alice|   2|    1|\n      |  Bob|null|    1|\n      |  Bob|   5|    1|\n      +-----+----+-----+\n      \n      .. versionadded:: 1.4\n  \n  sample(self, withReplacement=None, fraction=None, seed=None)\n      Returns a sampled subset of this :class:`DataFrame`.\n      \n      :param withReplacement: Sample with replacement or not (default ``False``).\n      :param fraction: Fraction of rows to generate, range [0.0, 1.0].\n      :param seed: Seed for sampling (default a random seed).\n      \n      .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n          count of the given :class:`DataFrame`.\n      \n      .. note:: `fraction` is required and, `withReplacement` and `seed` are optional.\n      \n      &gt;&gt;&gt; df = spark.range(10)\n      &gt;&gt;&gt; df.sample(0.5, 3).count()\n      7\n      &gt;&gt;&gt; df.sample(fraction=0.5, seed=3).count()\n      7\n      &gt;&gt;&gt; df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n      1\n      &gt;&gt;&gt; df.sample(1.0).count()\n      10\n      &gt;&gt;&gt; df.sample(fraction=1.0).count()\n      10\n      &gt;&gt;&gt; df.sample(False, fraction=1.0).count()\n      10\n      \n      .. versionadded:: 1.3\n  \n  sampleBy(self, col, fractions, seed=None)\n      Returns a stratified sample without replacement based on the\n      fraction given on each stratum.\n      \n      :param col: column that defines strata\n      :param fractions:\n          sampling fraction for each stratum. If a stratum is not\n          specified, we treat its fraction as zero.\n      :param seed: random seed\n      :return: a new :class:`DataFrame` that represents the stratified sample\n      \n      &gt;&gt;&gt; from pyspark.sql.functions import col\n      &gt;&gt;&gt; dataset = sqlContext.range(0, 100).select((col(&#34;id&#34;) % 3).alias(&#34;key&#34;))\n      &gt;&gt;&gt; sampled = dataset.sampleBy(&#34;key&#34;, fractions={0: 0.1, 1: 0.2}, seed=0)\n      &gt;&gt;&gt; sampled.groupBy(&#34;key&#34;).count().orderBy(&#34;key&#34;).show()\n      +---+-----+\n      |key|count|\n      +---+-----+\n      |  0|    3|\n      |  1|    6|\n      +---+-----+\n      &gt;&gt;&gt; dataset.sampleBy(col(&#34;key&#34;), fractions={2: 1.0}, seed=0).count()\n      33\n      \n      .. versionchanged:: 3.0\n         Added sampling by a column of :class:`Column`\n      \n      .. versionadded:: 1.5\n  \n  select(self, *cols)\n      Projects a set of expressions and returns a new :class:`DataFrame`.\n      \n      :param cols: list of column names (string) or expressions (:class:`Column`).\n          If one of the column names is &#39;*&#39;, that column is expanded to include all columns\n          in the current :class:`DataFrame`.\n      \n      &gt;&gt;&gt; df.select(&#39;*&#39;).collect()\n      [Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]\n      &gt;&gt;&gt; df.select(&#39;name&#39;, &#39;age&#39;).collect()\n      [Row(name=&#39;Alice&#39;, age=2), Row(name=&#39;Bob&#39;, age=5)]\n      &gt;&gt;&gt; df.select(df.name, (df.age + 10).alias(&#39;age&#39;)).collect()\n      [Row(name=&#39;Alice&#39;, age=12), Row(name=&#39;Bob&#39;, age=15)]\n      \n      .. versionadded:: 1.3\n  \n  selectExpr(self, *expr)\n      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n      \n      This is a variant of :func:`select` that accepts SQL expressions.\n      \n      &gt;&gt;&gt; df.selectExpr(&#34;age * 2&#34;, &#34;abs(age)&#34;).collect()\n      [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n      \n      .. versionadded:: 1.3\n  \n  show(self, n=20, truncate=True, vertical=False)\n      Prints the first ``n`` rows to the console.\n      \n      :param n: Number of rows to show.\n      :param truncate: If set to ``True``, truncate strings longer than 20 chars by default.\n          If set to a number greater than one, truncates long strings to length ``truncate``\n          and align cells right.\n      :param vertical: If set to ``True``, print output rows vertically (one line\n          per column value).\n      \n      &gt;&gt;&gt; df\n      DataFrame[age: int, name: string]\n      &gt;&gt;&gt; df.show()\n      +---+-----+\n      |age| name|\n      +---+-----+\n      |  2|Alice|\n      |  5|  Bob|\n      +---+-----+\n      &gt;&gt;&gt; df.show(truncate=3)\n      +---+----+\n      |age|name|\n      +---+----+\n      |  2| Ali|\n      |  5| Bob|\n      +---+----+\n      &gt;&gt;&gt; df.show(vertical=True)\n      -RECORD 0-----\n       age  | 2\n       name | Alice\n      -RECORD 1-----\n       age  | 5\n       name | Bob\n      \n      .. versionadded:: 1.3\n  \n  sort(self, *cols, **kwargs)\n      Returns a new :class:`DataFrame` sorted by the specified column(s).\n      \n      :param cols: list of :class:`Column` or column names to sort by.\n      :param ascending: boolean or list of boolean (default ``True``).\n          Sort ascending vs. descending. Specify list for multiple sort orders.\n          If a list is specified, length of the list must equal length of the `cols`.\n      \n      &gt;&gt;&gt; df.sort(df.age.desc()).collect()\n      [Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]\n      &gt;&gt;&gt; df.sort(&#34;age&#34;, ascending=False).collect()\n      [Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]\n      &gt;&gt;&gt; df.orderBy(df.age.desc()).collect()\n      [Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]\n      &gt;&gt;&gt; from pyspark.sql.functions import *\n      &gt;&gt;&gt; df.sort(asc(&#34;age&#34;)).collect()\n      [Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]\n      &gt;&gt;&gt; df.orderBy(desc(&#34;age&#34;), &#34;name&#34;).collect()\n      [Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]\n      &gt;&gt;&gt; df.orderBy([&#34;age&#34;, &#34;name&#34;], ascending=[0, 1]).collect()\n      [Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]\n      \n      .. versionadded:: 1.3\n  \n  sortWithinPartitions(self, *cols, **kwargs)\n      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n      \n      :param cols: list of :class:`Column` or column names to sort by.\n      :param ascending: boolean or list of boolean (default ``True``).\n          Sort ascending vs. descending. Specify list for multiple sort orders.\n          If a list is specified, length of the list must equal length of the `cols`.\n      \n      &gt;&gt;&gt; df.sortWithinPartitions(&#34;age&#34;, ascending=False).show()\n      +---+-----+\n      |age| name|\n      +---+-----+\n      |  2|Alice|\n      |  5|  Bob|\n      +---+-----+\n      \n      .. versionadded:: 1.6\n  \n  subtract(self, other)\n      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n      but not in another :class:`DataFrame`.\n      \n      This is equivalent to `EXCEPT DISTINCT` in SQL.\n      \n      .. versionadded:: 1.3\n  \n  summary(self, *statistics)\n      Computes specified statistics for numeric and string columns. Available statistics are:\n      - count\n      - mean\n      - stddev\n      - min\n      - max\n      - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n      \n      If no statistics are given, this function computes count, mean, stddev, min,\n      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n      \n      .. note:: This function is meant for exploratory data analysis, as we make no\n          guarantee about the backward compatibility of the schema of the resulting\n          :class:`DataFrame`.\n      \n      &gt;&gt;&gt; df.summary().show()\n      +-------+------------------+-----+\n      |summary|               age| name|\n      +-------+------------------+-----+\n      |  count|                 2|    2|\n      |   mean|               3.5| null|\n      | stddev|2.1213203435596424| null|\n      |    min|                 2|Alice|\n      |    25%|                 2| null|\n      |    50%|                 2| null|\n      |    75%|                 5| null|\n      |    max|                 5|  Bob|\n      +-------+------------------+-----+\n      \n      &gt;&gt;&gt; df.summary(&#34;count&#34;, &#34;min&#34;, &#34;25%&#34;, &#34;75%&#34;, &#34;max&#34;).show()\n      +-------+---+-----+\n      |summary|age| name|\n      +-------+---+-----+\n      |  count|  2|    2|\n      |    min|  2|Alice|\n      |    25%|  2| null|\n      |    75%|  5| null|\n      |    max|  5|  Bob|\n      +-------+---+-----+\n      \n      To do a summary for specific columns first select them:\n      \n      &gt;&gt;&gt; df.select(&#34;age&#34;, &#34;name&#34;).summary(&#34;count&#34;).show()\n      +-------+---+----+\n      |summary|age|name|\n      +-------+---+----+\n      |  count|  2|   2|\n      +-------+---+----+\n      \n      See also describe for basic statistics.\n      \n      .. versionadded:: 2.3.0\n  \n  tail(self, num)\n      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n      \n      Running tail requires moving data into the application&#39;s driver process, and doing so with\n      a very large ``num`` can crash the driver process with OutOfMemoryError.\n      \n      &gt;&gt;&gt; df.tail(1)\n      [Row(age=5, name=&#39;Bob&#39;)]\n      \n      .. versionadded:: 3.0\n  \n  take(self, num)\n      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n      \n      &gt;&gt;&gt; df.take(2)\n      [Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]\n      \n      .. versionadded:: 1.3\n  \n  toDF(self, *cols)\n      Returns a new :class:`DataFrame` that with new specified column names\n      \n      :param cols: list of new column names (string)\n      \n      &gt;&gt;&gt; df.toDF(&#39;f1&#39;, &#39;f2&#39;).collect()\n      [Row(f1=2, f2=&#39;Alice&#39;), Row(f1=5, f2=&#39;Bob&#39;)]\n  \n  toJSON(self, use_unicode=True)\n      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n      \n      Each row is turned into a JSON document as one element in the returned RDD.\n      \n      &gt;&gt;&gt; df.toJSON().first()\n      &#39;{&#34;age&#34;:2,&#34;name&#34;:&#34;Alice&#34;}&#39;\n      \n      .. versionadded:: 1.3\n  \n  toLocalIterator(self, prefetchPartitions=False)\n      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n      The iterator will consume as much memory as the largest partition in this\n      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n      partitions.\n      \n      :param prefetchPartitions: If Spark should pre-fetch the next partition\n                                 before it is needed.\n      \n      &gt;&gt;&gt; list(df.toLocalIterator())\n      [Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]\n      \n      .. versionadded:: 2.0\n  \n  transform(self, func)\n      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n      \n      :param func: a function that takes and returns a :class:`DataFrame`.\n      \n      &gt;&gt;&gt; from pyspark.sql.functions import col\n      &gt;&gt;&gt; df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [&#34;int&#34;, &#34;float&#34;])\n      &gt;&gt;&gt; def cast_all_to_int(input_df):\n      ...     return input_df.select([col(col_name).cast(&#34;int&#34;) for col_name in input_df.columns])\n      &gt;&gt;&gt; def sort_columns_asc(input_df):\n      ...     return input_df.select(*sorted(input_df.columns))\n      &gt;&gt;&gt; df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n      +-----+---+\n      |float|int|\n      +-----+---+\n      |    1|  1|\n      |    2|  2|\n      +-----+---+\n      \n      .. versionadded:: 3.0\n  \n  union(self, other)\n      Return a new :class:`DataFrame` containing union of rows in this and another\n      :class:`DataFrame`.\n      \n      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n      (that does deduplication of elements), use this function followed by :func:`distinct`.\n      \n      Also as standard in SQL, this function resolves columns by position (not by name).\n      \n      .. versionadded:: 2.0\n  \n  unionAll(self, other)\n      Return a new :class:`DataFrame` containing union of rows in this and another\n      :class:`DataFrame`.\n      \n      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n      (that does deduplication of elements), use this function followed by :func:`distinct`.\n      \n      Also as standard in SQL, this function resolves columns by position (not by name).\n      \n      .. versionadded:: 1.3\n  \n  unionByName(self, other)\n      Returns a new :class:`DataFrame` containing union of rows in this and another\n      :class:`DataFrame`.\n      \n      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n      \n      The difference between this function and :func:`union` is that this function\n      resolves columns by name (not by position):\n      \n      &gt;&gt;&gt; df1 = spark.createDataFrame([[1, 2, 3]], [&#34;col0&#34;, &#34;col1&#34;, &#34;col2&#34;])\n      &gt;&gt;&gt; df2 = spark.createDataFrame([[4, 5, 6]], [&#34;col1&#34;, &#34;col2&#34;, &#34;col0&#34;])\n      &gt;&gt;&gt; df1.unionByName(df2).show()\n      +----+----+----+\n      |col0|col1|col2|\n      +----+----+----+\n      |   1|   2|   3|\n      |   6|   4|   5|\n      +----+----+----+\n      \n      .. versionadded:: 2.3\n  \n  unpersist(self, blocking=False)\n      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n      memory and disk.\n      \n      .. note:: `blocking` default has changed to ``False`` to match Scala in 2.0.\n      \n      .. versionadded:: 1.3\n  \n  where = filter(self, condition)\n      :func:`where` is an alias for :func:`filter`.\n      \n      .. versionadded:: 1.3\n  \n  withColumn(self, colName, col)\n      Returns a new :class:`DataFrame` by adding a column or replacing the\n      existing column that has the same name.\n      \n      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n      a column from some other :class:`DataFrame` will raise an error.\n      \n      :param colName: string, name of the new column.\n      :param col: a :class:`Column` expression for the new column.\n      \n      .. note:: This method introduces a projection internally. Therefore, calling it multiple\n          times, for instance, via loops in order to add multiple columns can generate big\n          plans which can cause performance issues and even `StackOverflowException`.\n          To avoid this, use :func:`select` with the multiple columns at once.\n      \n      &gt;&gt;&gt; df.withColumn(&#39;age2&#39;, df.age + 2).collect()\n      [Row(age=2, name=&#39;Alice&#39;, age2=4), Row(age=5, name=&#39;Bob&#39;, age2=7)]\n      \n      .. versionadded:: 1.3\n  \n  withColumnRenamed(self, existing, new)\n      Returns a new :class:`DataFrame` by renaming an existing column.\n      This is a no-op if schema doesn&#39;t contain the given column name.\n      \n      :param existing: string, name of the existing column to rename.\n      :param new: string, new name of the column.\n      \n      &gt;&gt;&gt; df.withColumnRenamed(&#39;age&#39;, &#39;age2&#39;).collect()\n      [Row(age2=2, name=&#39;Alice&#39;), Row(age2=5, name=&#39;Bob&#39;)]\n      \n      .. versionadded:: 1.3\n  \n  withWatermark(self, eventTime, delayThreshold)\n      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n      in time before which we assume no more late data is going to arrive.\n      \n      Spark will use this watermark for several purposes:\n        - To know when a given time window aggregation can be finalized and thus can be emitted\n          when using output modes that do not allow updates.\n      \n        - To minimize the amount of state that we need to keep for on-going aggregations.\n      \n      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n      of coordinating this value across partitions, the actual watermark used is only guaranteed\n      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n      process records that arrive more than `delayThreshold` late.\n      \n      :param eventTime: the name of the column that contains the event time of the row.\n      :param delayThreshold: the minimum delay to wait to data to arrive late, relative to the\n          latest record that has been processed in the form of an interval\n          (e.g. &#34;1 minute&#34; or &#34;5 hours&#34;).\n      \n      .. note:: Evolving\n      \n      &gt;&gt;&gt; sdf.select(&#39;name&#39;, sdf.time.cast(&#39;timestamp&#39;)).withWatermark(&#39;time&#39;, &#39;10 minutes&#39;)\n      DataFrame[name: string, time: timestamp]\n      \n      .. versionadded:: 2.1\n  \n  ----------------------------------------------------------------------\n  Data descriptors defined here:\n  \n  columns\n      Returns all column names as a list.\n      \n      &gt;&gt;&gt; df.columns\n      [&#39;age&#39;, &#39;name&#39;]\n      \n      .. versionadded:: 1.3\n  \n  dtypes\n      Returns all column names and their data types as a list.\n      \n      &gt;&gt;&gt; df.dtypes\n      [(&#39;age&#39;, &#39;int&#39;), (&#39;name&#39;, &#39;string&#39;)]\n      \n      .. versionadded:: 1.3\n  \n  isStreaming\n      Returns ``True`` if this :class:`Dataset` contains one or more sources that continuously\n      return data as it arrives. A :class:`Dataset` that reads data from a streaming source\n      must be executed as a :class:`StreamingQuery` using the :func:`start` method in\n      :class:`DataStreamWriter`.  Methods that return a single answer, (e.g., :func:`count` or\n      :func:`collect`) will throw an :class:`AnalysisException` when there is a streaming\n      source present.\n      \n      .. note:: Evolving\n      \n      .. versionadded:: 2.0\n  \n  na\n      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n      \n      .. versionadded:: 1.3.1\n  \n  rdd\n      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n      \n      .. versionadded:: 1.3\n  \n  schema\n      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n      \n      &gt;&gt;&gt; df.schema\n      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n      \n      .. versionadded:: 1.3\n  \n  stat\n      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n      \n      .. versionadded:: 1.4\n  \n  storageLevel\n      Get the :class:`DataFrame`&#39;s current storage level.\n      \n      &gt;&gt;&gt; df.storageLevel\n      StorageLevel(False, False, False, False, 1)\n      &gt;&gt;&gt; df.cache().storageLevel\n      StorageLevel(True, True, False, True, 1)\n      &gt;&gt;&gt; df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n      StorageLevel(True, False, False, False, 2)\n      \n      .. versionadded:: 2.1\n  \n  write\n      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n      storage.\n      \n      :return: :class:`DataFrameWriter`\n      \n      .. versionadded:: 1.4\n  \n  writeStream\n      Interface for saving the content of the streaming :class:`DataFrame` out into external\n      storage.\n      \n      .. note:: Evolving.\n      \n      :return: :class:`DataStreamWriter`\n      \n      .. versionadded:: 2.0\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n  \n  mapInPandas(self, func, schema)\n      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n      function that takes and outputs a pandas DataFrame, and returns the result as a\n      :class:`DataFrame`.\n      \n      The function should take an iterator of `pandas.DataFrame`\\s and return\n      another iterator of `pandas.DataFrame`\\s. All columns are passed\n      together as an iterator of `pandas.DataFrame`\\s to the function and the\n      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n      Each `pandas.DataFrame` size can be controlled by\n      `spark.sql.execution.arrow.maxRecordsPerBatch`.\n      \n      :param func: a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n          outputs an iterator of `pandas.DataFrame`\\s.\n      :param schema: the return type of the `func` in PySpark. The value can be either a\n          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n      \n      &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf\n      &gt;&gt;&gt; df = spark.createDataFrame([(1, 21), (2, 30)], (&#34;id&#34;, &#34;age&#34;))\n      &gt;&gt;&gt; def filter_func(iterator):\n      ...     for pdf in iterator:\n      ...         yield pdf[pdf.id == 1]\n      &gt;&gt;&gt; df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n      +---+---+\n      | id|age|\n      +---+---+\n      |  1| 21|\n      +---+---+\n      \n      .. seealso:: :meth:`pyspark.sql.functions.pandas_udf`\n      \n      .. note:: Experimental\n      \n      .. versionadded:: 3.0\n  \n  ----------------------------------------------------------------------\n  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n  \n  __dict__\n      dictionary for instance variables (if defined)\n  \n  __weakref__\n      list of weak references to the object (if defined)\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n  \n  toPandas(self)\n      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n      \n      This is only available if Pandas is installed and available.\n      \n      .. note:: This method should only be used if the resulting Pandas&#39;s :class:`DataFrame` is\n          expected to be small, as all the data is loaded into the driver&#39;s memory.\n      \n      .. note:: Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n      \n      &gt;&gt;&gt; df.toPandas()  # doctest: +SKIP\n         age   name\n      0    2  Alice\n      1    5    Bob\n      \n      .. versionadded:: 1.3\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd968b68-acd7-4d03-9107-058ca49e6d28"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- _1: long (nullable = true)\n |-- _2: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _1: long (nullable = true)\n-- _2: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a68d398-bb9c-47d1-a417-a4585b82b15b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["my_rdd = spark.sparkContext.parallelize(['Ravi','Raj','Mahesh','Prasad','Sridhar'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c72718a9-a175-456b-8934-627ed7cc4410"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["x = sc.parallelize([\"one two three\", \"four five\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27ff0e53-47e8-498f-8d70-e48ccb689288"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Split each value based space using space function\n# and reading each using map transformation...\n\ny = x.map(lambda x: x.split(' '))\ny.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85208086-54b8-4b47-b163-bbc1e0d788ca"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Spark_Configuration","dashboards":[],"language":"python","widgets":{},"notebookOrigID":759994179829454}},"nbformat":4,"nbformat_minor":0}
